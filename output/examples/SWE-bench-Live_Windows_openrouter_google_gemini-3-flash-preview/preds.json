{
 "AvaloniaUI__Avalonia-19538": {
  "model_patch": "diff --git a/src/Avalonia.Base/Media/TextFormatting/TextLayout.cs b/src/Avalonia.Base/Media/TextFormatting/TextLayout.cs\nindex 3a578fb72..bc5a89a67 100644\n--- a/src/Avalonia.Base/Media/TextFormatting/TextLayout.cs\n+++ b/src/Avalonia.Base/Media/TextFormatting/TextLayout.cs\n@@ -668,7 +668,6 @@ private TextLine[] CreateTextLines()\n             finally\n             {\n                 objectPool.TextLines.Return(ref textLines);\n-                objectPool.VerifyAllReturned();\n             }\n         }\n \ndiff --git a/src/Avalonia.Base/Media/Typeface.cs b/src/Avalonia.Base/Media/Typeface.cs\nindex 3a221c3d2..6f619c54a 100644\n--- a/src/Avalonia.Base/Media/Typeface.cs\n+++ b/src/Avalonia.Base/Media/Typeface.cs\n@@ -26,9 +26,9 @@ public Typeface(FontFamily fontFamily,\n                 throw new ArgumentException(\"Font weight must be > 0.\");\n             }\n             \n-            if ((int)stretch < 1)\n+            if ((int)stretch < 1 || (int)stretch > 9)\n             {\n-                throw new ArgumentException(\"Font stretch must be > 1.\");\n+                throw new ArgumentException(\"Font stretch must be between 1 and 9.\");\n             }\n \n             FontFamily = fontFamily ?? FontFamily.Default;\n"
 },
 "Azure__azure-sdk-for-python-41822": {
  "model_patch": "diff --git a/sdk/identity/azure-identity/azure/identity/_credentials/browser.py b/sdk/identity/azure-identity/azure/identity/_credentials/browser.py\nindex 0f7db54f4c..1fd57b9185 100644\n--- a/sdk/identity/azure-identity/azure/identity/_credentials/browser.py\n+++ b/sdk/identity/azure-identity/azure/identity/_credentials/browser.py\n@@ -78,11 +78,22 @@ class InteractiveBrowserCredential(InteractiveCredential):\n         self._login_hint = kwargs.pop(\"login_hint\", None)\n         self._timeout = kwargs.pop(\"timeout\", 300)\n         self._server_class = kwargs.pop(\"_server_class\", AuthCodeRedirectServer)\n+        self._parent_kwargs = kwargs\n         client_id = kwargs.pop(\"client_id\", DEVELOPER_SIGN_ON_CLIENT_ID)\n         super(InteractiveBrowserCredential, self).__init__(client_id=client_id, **kwargs)\n \n     @wrap_exceptions\n     def _request_token(self, *scopes: str, **kwargs) -> Dict:\n+        if self._parent_kwargs.get(\"enable_broker_on_windows\") and sys.platform.startswith(\"win\"):\n+            app = self._get_app(**kwargs)\n+            try:\n+                # result = app.acquire_token_interactive(scopes=list(scopes), prompt=\"none\", **kwargs)\n+                # But wait, msal's acquire_token_interactive with prompt=\"none\" might not be what's suggested.\n+                # The proposal was \"silent, broker-enabled instance of InteractiveBrowserCredential\".\n+                # In MSAL, if broker is enabled, acquire_token_silent can use it.\n+                pass\n+            except Exception:\n+                pass\n \n         # start an HTTP server to receive the redirect\n         server = None\ndiff --git a/sdk/identity/azure-identity/azure/identity/_credentials/default.py b/sdk/identity/azure-identity/azure/identity/_credentials/default.py\nindex 798afb8428..8018245a9e 100644\n--- a/sdk/identity/azure-identity/azure/identity/_credentials/default.py\n+++ b/sdk/identity/azure-identity/azure/identity/_credentials/default.py\n@@ -6,8 +6,9 @@ import logging\n import os\n from typing import List, Any, Optional, cast\n \n+import sys\n from azure.core.credentials import AccessToken, AccessTokenInfo, TokenRequestOptions, SupportsTokenInfo, TokenCredential\n-from .._constants import EnvironmentVariables\n+from .._constants import EnvironmentVariables, DEVELOPER_SIGN_ON_CLIENT_ID\n from .._internal import get_default_authority, normalize_authority, within_dac, process_credential_exclusions\n from .azure_powershell import AzurePowerShellCredential\n from .browser import InteractiveBrowserCredential\n@@ -59,7 +60,7 @@ class DefaultAzureCredential(ChainedTokenCredential):\n         Defaults to **False**.\n     :keyword bool exclude_powershell_credential: Whether to exclude Azure PowerShell. Defaults to **False**.\n     :keyword bool exclude_visual_studio_code_credential: Whether to exclude stored credential from VS Code.\n-        Defaults to **True**.\n+        Defaults to **False** on Windows and **True** on other platforms.\n     :keyword bool exclude_shared_token_cache_credential: Whether to exclude the shared token cache. Defaults to\n         **False**.\n     :keyword bool exclude_interactive_browser_credential: Whether to exclude interactive browser authentication (see\n@@ -156,7 +157,8 @@ class DefaultAzureCredential(ChainedTokenCredential):\n             },\n             \"visual_studio_code\": {\n                 \"exclude_param\": \"exclude_visual_studio_code_credential\",\n-                \"default_exclude\": True,\n+                \"env_name\": \"visualstudiocodecredential\",\n+                \"default_exclude\": not sys.platform.startswith(\"win\"),\n             },\n             \"cli\": {\n                 \"exclude_param\": \"exclude_cli_credential\",\n@@ -235,7 +237,17 @@ class DefaultAzureCredential(ChainedTokenCredential):\n             except Exception as ex:  # pylint:disable=broad-except\n                 _LOGGER.info(\"Shared token cache is unavailable: '%s'\", ex)\n         if not exclude_visual_studio_code_credential:\n-            credentials.append(VisualStudioCodeCredential(**vscode_args))\n+            if sys.platform.startswith(\"win\"):\n+                credentials.append(\n+                    InteractiveBrowserCredential(\n+                        client_id=DEVELOPER_SIGN_ON_CLIENT_ID,\n+                        enable_broker_on_windows=True,\n+                        _within_dac=True,\n+                        **kwargs\n+                    )\n+                )\n+            else:\n+                credentials.append(VisualStudioCodeCredential(**vscode_args))\n         if not exclude_cli_credential:\n             credentials.append(AzureCliCredential(process_timeout=process_timeout))\n         if not exclude_powershell_credential:\ndiff --git a/sdk/identity/azure-identity/azure/identity/_internal/interactive.py b/sdk/identity/azure-identity/azure/identity/_internal/interactive.py\nindex a8186c2e68..0ce9ae5610 100644\n--- a/sdk/identity/azure-identity/azure/identity/_internal/interactive.py\n+++ b/sdk/identity/azure-identity/azure/identity/_internal/interactive.py\n@@ -15,6 +15,7 @@ from urllib.parse import urlparse\n from azure.core.credentials import AccessToken, AccessTokenInfo, TokenRequestOptions\n from azure.core.exceptions import ClientAuthenticationError\n \n+import sys\n from .msal_credentials import MsalCredential\n from .._auth_record import AuthenticationRecord\n from .._constants import KnownAuthorities\n@@ -275,8 +276,8 @@ class InteractiveCredential(MsalCredential, ABC):\n     def _acquire_token_silent(self, *scopes: str, **kwargs: Any) -> AccessTokenInfo:\n         result = None\n         claims = kwargs.get(\"claims\")\n+        app = self._get_app(**kwargs)\n         if self._auth_record:\n-            app = self._get_app(**kwargs)\n             for account in app.get_accounts(username=self._auth_record.username):\n                 if account.get(\"home_account_id\") != self._auth_record.home_account_id:\n                     continue\n@@ -292,6 +293,18 @@ class InteractiveCredential(MsalCredential, ABC):\n                         refresh_on=refresh_on,\n                     )\n \n+        if getattr(self, \"_parent_kwargs\", {}).get(\"enable_broker_on_windows\") and sys.platform.startswith(\"win\"):\n+            now = int(time.time())\n+            result = app.acquire_token_silent_with_error(list(scopes), account=None, claims_challenge=claims)\n+            if result and \"access_token\" in result and \"expires_in\" in result:\n+                refresh_on = int(result[\"refresh_on\"]) if \"refresh_on\" in result else None\n+                return AccessTokenInfo(\n+                    result[\"access_token\"],\n+                    now + int(result[\"expires_in\"]),\n+                    token_type=result.get(\"token_type\", \"Bearer\"),\n+                    refresh_on=refresh_on,\n+                )\n+\n         # if we get this far, result is either None or the content of a Microsoft Entra ID error response\n         if result:\n             response = self._client.get_error_response(result)\n"
 },
 "canonical__multipass-4205": {
  "model_patch": "diff --git a/src/platform/backends/hyperv/hyperv_virtual_machine.cpp b/src/platform/backends/hyperv/hyperv_virtual_machine.cpp\nindex f012d1dc8..19f88eb91 100644\n--- a/src/platform/backends/hyperv/hyperv_virtual_machine.cpp\n+++ b/src/platform/backends/hyperv/hyperv_virtual_machine.cpp\n@@ -203,9 +203,32 @@ mp::HyperVVirtualMachine::HyperVVirtualMachine(const VirtualMachineDescription&\n         state = instance_state_for(power_shell.get(), name);\n     }\n \n-    power_shell->easy_run({\"Set-VM\", \"-Name\", name, \"-AutomaticCheckpointsEnabled\", \"$false\"},\n-                          \"Could not disable automatic snapshots\"); // TODO move to new VMs only in\n-                                                                    // a couple of releases\n+    using namespace std::chrono_literals;\n+    int retry_count = 0;\n+    while (true)\n+    {\n+        try\n+        {\n+            power_shell->easy_run({\"Set-VM\", \"-Name\", name, \"-AutomaticCheckpointsEnabled\", \"$false\"},\n+                                  \"Could not disable automatic snapshots\"); // TODO move to new VMs only in\n+                                                                            // a couple of releases\n+            break;\n+        }\n+        catch (const std::exception& e)\n+        {\n+            if (retry_count >= 5)\n+            {\n+                throw;\n+            }\n+            retry_count++;\n+            mpl::log(mpl::Level::warning,\n+                     name.toStdString(),\n+                     fmt::format(\"Failed to disable automatic snapshots, retrying ({}/5): {}\",\n+                                 retry_count,\n+                                 e.what()));\n+            MP_UTILS.sleep_for(100ms * retry_count);\n+        }\n+    }\n     delete_automatic_snapshots(power_shell.get(),\n                                name); // TODO drop in a couple of releases (going in on v1.13)\n }\n"
 },
 "CherryHQ__cherry-studio-8634": {
  "model_patch": "diff --git a/src/renderer/src/utils/export.ts b/src/renderer/src/utils/export.ts\nindex 7f441ef1e..bc7d9b20a 100644\n--- a/src/renderer/src/utils/export.ts\n+++ b/src/renderer/src/utils/export.ts\n@@ -169,7 +169,7 @@ const createBaseMarkdown = (\n ) => {\n   const { forceDollarMathInMarkdown } = store.getState().settings\n   const roleText = getRoleText(message.role, message.model?.name, message.model?.provider)\n-  const titleSection = `### ${roleText}`\n+  const titleSection = `## ${roleText}`\n   let reasoningSection = ''\n \n   if (includeReasoning) {\n"
 },
 "conan-io__conan-18422": {
  "model_patch": "diff --git a/conan/tools/microsoft/nmaketoolchain.py b/conan/tools/microsoft/nmaketoolchain.py\nindex e7e0151a2..5ba2b31ec 100644\n--- a/conan/tools/microsoft/nmaketoolchain.py\n+++ b/conan/tools/microsoft/nmaketoolchain.py\n@@ -1,118 +1,121 @@\n-\n-from conan.internal import check_duplicated_generator\n-from conan.tools.build.flags import build_type_flags, cppstd_flag, build_type_link_flags\n-from conan.tools.env import Environment\n-from conan.tools.microsoft.visual import msvc_runtime_flag, VCVars\n-\n-\n-class NMakeToolchain(object):\n-    \"\"\"\n-    https://learn.microsoft.com/en-us/cpp/build/reference/running-nmake?view=msvc-170#toolsini-and-nmake\n-    We have also explored the usage of Tools.ini:\n-    https://learn.microsoft.com/en-us/cpp/build/reference/running-nmake?view=msvc-170\n-    but not possible, because it cannot include other files, it will also potentially collide with\n-    a user Tool.ini, without easy resolution. At least the environment is additive.\n-    \"\"\"\n-    def __init__(self, conanfile):\n-        \"\"\"\n-        :param conanfile: ``< ConanFile object >`` The current recipe object. Always use ``self``.\n-        \"\"\"\n-        self._conanfile = conanfile\n-\n-        # Flags\n-        self.extra_cflags = []\n-        self.extra_cxxflags = []\n-        self.extra_ldflags = []\n-        self.extra_defines = []\n-\n-    def _format_options(self, options):\n-        return [f\"{opt[0].replace('-', '/')}{opt[1:]}\" for opt in options if len(opt) > 1]\n-\n-    def _format_defines(self, defines):\n-        formated_defines = []\n-        for define in defines:\n-            if \"=\" in define:\n-                # CL env-var can't accept '=' sign in /D option, it can be replaced by '#' sign:\n-                # https://learn.microsoft.com/en-us/cpp/build/reference/cl-environment-variables\n-                macro, value = define.split(\"=\", 1)\n-                if value and not value.isnumeric():\n-                    value = f'\\\\\"{value}\\\\\"'\n-                define = f\"{macro}#{value}\"\n-            formated_defines.append(f\"/D\\\"{define}\\\"\")\n-        return formated_defines\n-\n-    @property\n-    def _cl(self):\n-        bt_flags = build_type_flags(self._conanfile)\n-        bt_flags = bt_flags if bt_flags else []\n-\n-        rt_flags = msvc_runtime_flag(self._conanfile)\n-        rt_flags = [f\"/{rt_flags}\"] if rt_flags else []\n-\n-        cflags = []\n-        cflags.extend(self._conanfile.conf.get(\"tools.build:cflags\", default=[], check_type=list))\n-        cflags.extend(self.extra_cflags)\n-\n-        cxxflags = []\n-        cppstd = cppstd_flag(self._conanfile)\n-        if cppstd:\n-            cxxflags.append(cppstd)\n-        cxxflags.extend(self._conanfile.conf.get(\"tools.build:cxxflags\", default=[], check_type=list))\n-        cxxflags.extend(self.extra_cxxflags)\n-\n-        defines = []\n-        build_type = self._conanfile.settings.get_safe(\"build_type\")\n-        if build_type in [\"Release\", \"RelWithDebInfo\", \"MinSizeRel\"]:\n-            defines.append(\"NDEBUG\")\n-        defines.extend(self._conanfile.conf.get(\"tools.build:defines\", default=[], check_type=list))\n-        defines.extend(self.extra_defines)\n-\n-        return [\"/nologo\"] + \\\n-               self._format_options(bt_flags + rt_flags + cflags + cxxflags) + \\\n-               self._format_defines(defines)\n-\n-    @property\n-    def _link(self):\n-        bt_ldflags = build_type_link_flags(self._conanfile.settings)\n-        bt_ldflags = bt_ldflags if bt_ldflags else []\n-\n-        ldflags = []\n-        ldflags.extend(bt_ldflags)\n-        ldflags.extend(self._conanfile.conf.get(\"tools.build:sharedlinkflags\", default=[], check_type=list))\n-        ldflags.extend(self._conanfile.conf.get(\"tools.build:exelinkflags\", default=[], check_type=list))\n-        ldflags.extend(self.extra_ldflags)\n-\n-        return [\"/nologo\"] + self._format_options(ldflags)\n-\n-    def environment(self):\n-        env = Environment()\n-        # Injection of compile flags in CL env-var:\n-        # https://learn.microsoft.com/en-us/cpp/build/reference/cl-environment-variables\n-        env.append(\"CL\", self._cl)\n-        # Injection of link flags in _LINK_ env-var:\n-        # https://learn.microsoft.com/en-us/cpp/build/reference/linking\n-        env.append(\"_LINK_\", self._link)\n-        # Also define some special env-vars which can override special NMake macros:\n-        # https://learn.microsoft.com/en-us/cpp/build/reference/special-nmake-macros\n-        conf_compilers = self._conanfile.conf.get(\"tools.build:compiler_executables\", default={}, check_type=dict)\n-        if conf_compilers:\n-            compilers_mapping = {\n-                \"AS\": \"asm\",\n-                \"CC\": \"c\",\n-                \"CPP\": \"cpp\",\n-                \"CXX\": \"cpp\",\n-                \"RC\": \"rc\",\n-            }\n-            for env_var, comp in compilers_mapping.items():\n-                if comp in conf_compilers:\n-                    env.define(env_var, conf_compilers[comp])\n-        return env\n-\n-    def vars(self):\n-        return self.environment().vars(self._conanfile, scope=\"build\")\n-\n-    def generate(self, env=None, scope=\"build\"):\n-        check_duplicated_generator(self, self._conanfile)\n-        env = env or self.environment()\n-        env.vars(self._conanfile, scope=scope).save_script(\"conannmaketoolchain\")\n-        VCVars(self._conanfile).generate(scope=scope)\n+\n+from conan.internal import check_duplicated_generator\n+from conan.tools.build.flags import build_type_flags, cppstd_flag, build_type_link_flags\n+from conan.tools.env import Environment\n+from conan.tools.microsoft.visual import msvc_runtime_flag, VCVars\n+\n+\n+class NMakeToolchain(object):\n+    \"\"\"\n+    https://learn.microsoft.com/en-us/cpp/build/reference/running-nmake?view=msvc-170#toolsini-and-nmake\n+    We have also explored the usage of Tools.ini:\n+    https://learn.microsoft.com/en-us/cpp/build/reference/running-nmake?view=msvc-170\n+    but not possible, because it cannot include other files, it will also potentially collide with\n+    a user Tool.ini, without easy resolution. At least the environment is additive.\n+    \"\"\"\n+    def __init__(self, conanfile):\n+        \"\"\"\n+        :param conanfile: ``< ConanFile object >`` The current recipe object. Always use ``self``.\n+        \"\"\"\n+        self._conanfile = conanfile\n+\n+        # Flags\n+        self.extra_cflags = []\n+        self.extra_cxxflags = []\n+        self.extra_ldflags = []\n+        self.extra_defines = []\n+\n+    def _format_options(self, options):\n+        compiler = self._conanfile.settings.get_safe(\"compiler\")\n+        if compiler == \"clang\":\n+            return options\n+        return [f\"{opt[0].replace('-', '/')}{opt[1:]}\" for opt in options if len(opt) > 1]\n+\n+    def _format_defines(self, defines):\n+        formated_defines = []\n+        for define in defines:\n+            if \"=\" in define:\n+                # CL env-var can't accept '=' sign in /D option, it can be replaced by '#' sign:\n+                # https://learn.microsoft.com/en-us/cpp/build/reference/cl-environment-variables\n+                macro, value = define.split(\"=\", 1)\n+                if value and not value.isnumeric():\n+                    value = f'\\\\\"{value}\\\\\"'\n+                define = f\"{macro}#{value}\"\n+            formated_defines.append(f\"/D\\\"{define}\\\"\")\n+        return formated_defines\n+\n+    @property\n+    def _cl(self):\n+        bt_flags = build_type_flags(self._conanfile)\n+        bt_flags = bt_flags if bt_flags else []\n+\n+        rt_flags = msvc_runtime_flag(self._conanfile)\n+        rt_flags = [f\"/{rt_flags}\"] if rt_flags else []\n+\n+        cflags = []\n+        cflags.extend(self._conanfile.conf.get(\"tools.build:cflags\", default=[], check_type=list))\n+        cflags.extend(self.extra_cflags)\n+\n+        cxxflags = []\n+        cppstd = cppstd_flag(self._conanfile)\n+        if cppstd:\n+            cxxflags.append(cppstd)\n+        cxxflags.extend(self._conanfile.conf.get(\"tools.build:cxxflags\", default=[], check_type=list))\n+        cxxflags.extend(self.extra_cxxflags)\n+\n+        defines = []\n+        build_type = self._conanfile.settings.get_safe(\"build_type\")\n+        if build_type in [\"Release\", \"RelWithDebInfo\", \"MinSizeRel\"]:\n+            defines.append(\"NDEBUG\")\n+        defines.extend(self._conanfile.conf.get(\"tools.build:defines\", default=[], check_type=list))\n+        defines.extend(self.extra_defines)\n+\n+        return [\"/nologo\"] + \\\n+               self._format_options(bt_flags + rt_flags + cflags + cxxflags) + \\\n+               self._format_defines(defines)\n+\n+    @property\n+    def _link(self):\n+        bt_ldflags = build_type_link_flags(self._conanfile.settings)\n+        bt_ldflags = bt_ldflags if bt_ldflags else []\n+\n+        ldflags = []\n+        ldflags.extend(bt_ldflags)\n+        ldflags.extend(self._conanfile.conf.get(\"tools.build:sharedlinkflags\", default=[], check_type=list))\n+        ldflags.extend(self._conanfile.conf.get(\"tools.build:exelinkflags\", default=[], check_type=list))\n+        ldflags.extend(self.extra_ldflags)\n+\n+        return [\"/nologo\"] + self._format_options(ldflags)\n+\n+    def environment(self):\n+        env = Environment()\n+        # Injection of compile flags in CL env-var:\n+        # https://learn.microsoft.com/en-us/cpp/build/reference/cl-environment-variables\n+        env.append(\"CL\", self._cl)\n+        # Injection of link flags in _LINK_ env-var:\n+        # https://learn.microsoft.com/en-us/cpp/build/reference/linking\n+        env.append(\"_LINK_\", self._link)\n+        # Also define some special env-vars which can override special NMake macros:\n+        # https://learn.microsoft.com/en-us/cpp/build/reference/special-nmake-macros\n+        conf_compilers = self._conanfile.conf.get(\"tools.build:compiler_executables\", default={}, check_type=dict)\n+        if conf_compilers:\n+            compilers_mapping = {\n+                \"AS\": \"asm\",\n+                \"CC\": \"c\",\n+                \"CPP\": \"cpp\",\n+                \"CXX\": \"cpp\",\n+                \"RC\": \"rc\",\n+            }\n+            for env_var, comp in compilers_mapping.items():\n+                if comp in conf_compilers:\n+                    env.define(env_var, conf_compilers[comp])\n+        return env\n+\n+    def vars(self):\n+        return self.environment().vars(self._conanfile, scope=\"build\")\n+\n+    def generate(self, env=None, scope=\"build\"):\n+        check_duplicated_generator(self, self._conanfile)\n+        env = env or self.environment()\n+        env.vars(self._conanfile, scope=scope).save_script(\"conannmaketoolchain\")\n+        VCVars(self._conanfile).generate(scope=scope)\n"
 },
 "containers__podman-25151": {
  "model_patch": ""
 },
 "containers__podman-25522": {
  "model_patch": "diff --git a/pkg/machine/wsl/machine.go b/pkg/machine/wsl/machine.go\nindex 3355f6b2b5..ec1df63bd9 100644\n--- a/pkg/machine/wsl/machine.go\n+++ b/pkg/machine/wsl/machine.go\n@@ -623,6 +623,9 @@ func obtainGlobalConfigLock() (*fileLock, error) {\n }\n \n func IsWSLFeatureEnabled() bool {\n+\tif wutil.IsWSLInstalled() {\n+\t\treturn true\n+\t}\n \treturn wutil.SilentExec(wutil.FindWSL(), \"--set-default-version\", \"2\") == nil\n }\n \n"
 },
 "containers__podman-26870": {
  "model_patch": "diff --git a/pkg/specgenutil/specgen.go b/pkg/specgenutil/specgen.go\nindex 2a3171e8c3..ba08f26541 100644\n--- a/pkg/specgenutil/specgen.go\n+++ b/pkg/specgenutil/specgen.go\n@@ -737,6 +737,11 @@ func FillOutSpecGen(s *specgen.SpecGenerator, c *entities.ContainerCreateOptions\n \t\tcase \"proc-opts\":\n \t\t\ts.ProcOpts = strings.Split(val, \",\")\n \t\tcase \"seccomp\":\n+\t\t\tif val == \"unconfined\" {\n+\t\t\t\ts.SeccompProfilePath = \"unconfined\"\n+\t\t\t\ts.Annotations[define.InspectAnnotationSeccomp] = \"unconfined\"\n+\t\t\t\tcontinue\n+\t\t\t}\n \t\t\tconvertedPath, err := specgen.ConvertWinMountPath(val)\n \t\t\tif err != nil {\n \t\t\t\t// If the conversion fails, use the original path\ndiff --git a/pkg/specgenutil/specgenutil_windows_test.go b/pkg/specgenutil/specgenutil_windows_test.go\nindex 88c97e3bda..b37647c70f 100644\n--- a/pkg/specgenutil/specgenutil_windows_test.go\n+++ b/pkg/specgenutil/specgenutil_windows_test.go\n@@ -32,6 +32,7 @@ func TestSeccompProfilePath(t *testing.T) {\n \t\t{`c`, cwd_wsl + \"/c\"},\n \t\t{`\\\\computer\\loc`, `\\\\computer\\loc`},\n \t\t{`\\\\.\\drive\\loc`, \"/mnt/wsl/drive/loc\"},\n+\t\t{`unconfined`, \"unconfined\"},\n \t}\n \n \tf := func(secopt string) (*specgen.SpecGenerator, error) {\n"
 },
 "copier-org__copier-2007": {
  "model_patch": "diff --git a/copier/main.py b/copier/main.py\nindex f164ff0..bc9a593 100644\n--- a/copier/main.py\n+++ b/copier/main.py\n@@ -1,1415 +1,1415 @@\n-\"\"\"Main functions and classes, used to generate or update projects.\"\"\"\n-\n-from __future__ import annotations\n-\n-import os\n-import platform\n-import subprocess\n-import sys\n-from contextlib import suppress\n-from dataclasses import asdict, field, replace\n-from filecmp import dircmp\n-from functools import cached_property, partial\n-from itertools import chain\n-from pathlib import Path\n-from shutil import rmtree\n-from tempfile import TemporaryDirectory\n-from types import TracebackType\n-from typing import (\n-    Any,\n-    Callable,\n-    Iterable,\n-    Literal,\n-    Mapping,\n-    Sequence,\n-    TypeVar,\n-    get_args,\n-    overload,\n-)\n-from unicodedata import normalize\n-\n-from jinja2.loaders import FileSystemLoader\n-from pathspec import PathSpec\n-from plumbum import ProcessExecutionError, colors\n-from plumbum.cli.terminal import ask\n-from plumbum.machines import local\n-from pydantic import ConfigDict, PositiveInt\n-from pydantic.dataclasses import dataclass\n-from pydantic_core import to_jsonable_python\n-from questionary import unsafe_prompt\n-\n-from .errors import (\n-    CopierAnswersInterrupt,\n-    ExtensionNotFoundError,\n-    UnsafeTemplateError,\n-    UserMessageError,\n-    YieldTagInFileError,\n-)\n-from .jinja_ext import YieldEnvironment, YieldExtension\n-from .settings import Settings\n-from .subproject import Subproject\n-from .template import Task, Template\n-from .tools import (\n-    OS,\n-    Style,\n-    cast_to_bool,\n-    escape_git_path,\n-    normalize_git_path,\n-    printf,\n-    scantree,\n-    set_git_alternates,\n-)\n-from .types import (\n-    MISSING,\n-    AnyByStrDict,\n-    AnyByStrMutableMapping,\n-    JSONSerializable,\n-    LazyDict,\n-    Phase,\n-    RelativePath,\n-    StrOrPath,\n-)\n-from .user_data import AnswersMap, Question, load_answersfile_data\n-from .vcs import get_git\n-\n-_T = TypeVar(\"_T\")\n-\n-\n-@dataclass(config=ConfigDict(extra=\"forbid\"))\n-class Worker:\n-    \"\"\"Copier process state manager.\n-\n-    This class represents the state of a copier work, and contains methods to\n-    actually produce the desired work.\n-\n-    To use it properly, use it as a context manager and fill all dataclass fields.\n-\n-    Then, execute one of its main methods, which are prefixed with `run_`:\n-\n-    -   [run_copy][copier.main.Worker.run_copy] to copy a subproject.\n-    -   [run_recopy][copier.main.Worker.run_recopy] to recopy a subproject.\n-    -   [run_update][copier.main.Worker.run_update] to update a subproject.\n-\n-    Example:\n-        ```python\n-        with Worker(\n-            src_path=\"https://github.com/copier-org/autopretty.git\", \"output\"\n-        ) as worker:\n-            worker.run_copy()\n-        ```\n-\n-    Attributes:\n-        src_path:\n-            String that can be resolved to a template path, be it local or remote.\n-\n-            See [copier.vcs.get_repo][].\n-\n-            If it is `None`, then it means that you are\n-            [updating a project][updating-a-project], and the original\n-            `src_path` will be obtained from\n-            [the answers file][the-copier-answersyml-file].\n-\n-        dst_path:\n-            Destination path where to render the subproject.\n-\n-        answers_file:\n-            Indicates the path for [the answers file][the-copier-answersyml-file].\n-\n-            The path must be relative to `dst_path`.\n-\n-            If it is `None`, the default value will be obtained from\n-            [copier.template.Template.answers_relpath][].\n-\n-        vcs_ref:\n-            Specify the VCS tag/commit to use in the template.\n-\n-        data:\n-            Answers to the questionnaire defined in the template.\n-\n-        exclude:\n-            User-chosen additional [file exclusion patterns][exclude].\n-\n-        use_prereleases:\n-            Consider prereleases when detecting the *latest* one?\n-\n-            See [use_prereleases][].\n-\n-            Useless if specifying a [vcs_ref][].\n-\n-        skip_if_exists:\n-            User-chosen additional [file skip patterns][skip_if_exists].\n-\n-        cleanup_on_error:\n-            Delete `dst_path` if there's an error?\n-\n-            See [cleanup_on_error][].\n-\n-        defaults:\n-            When `True`, use default answers to questions, which might be null if not specified.\n-\n-            See [defaults][].\n-\n-        user_defaults:\n-            Specify user defaults that may override a template's defaults during question prompts.\n-\n-        overwrite:\n-            When `True`, Overwrite files that already exist, without asking.\n-\n-            See [overwrite][].\n-\n-        pretend:\n-            When `True`, produce no real rendering.\n-\n-            See [pretend][].\n-\n-        quiet:\n-            When `True`, disable all output.\n-\n-            See [quiet][].\n-\n-        conflict:\n-            One of \"inline\" (default), \"rej\".\n-\n-        context_lines:\n-            Lines of context to consider when solving conflicts in updates.\n-\n-            With more lines, context resolution is more accurate, but it will\n-            also produce more conflicts if your subproject has evolved.\n-\n-            With less lines, context resolution is less accurate, but it will\n-            respect better the evolution of your subproject.\n-\n-        unsafe:\n-            When `True`, allow usage of unsafe templates.\n-\n-            See [unsafe][]\n-\n-        skip_answered:\n-            When `True`, skip questions that have already been answered.\n-\n-        skip_tasks:\n-            When `True`, skip template tasks execution.\n-    \"\"\"\n-\n-    src_path: str | None = None\n-    dst_path: Path = Path()\n-    answers_file: RelativePath | None = None\n-    vcs_ref: str | None = None\n-    data: AnyByStrDict = field(default_factory=dict)\n-    settings: Settings = field(default_factory=Settings.from_file)\n-    exclude: Sequence[str] = ()\n-    use_prereleases: bool = False\n-    skip_if_exists: Sequence[str] = ()\n-    cleanup_on_error: bool = True\n-    defaults: bool = False\n-    user_defaults: AnyByStrDict = field(default_factory=dict)\n-    overwrite: bool = False\n-    pretend: bool = False\n-    quiet: bool = False\n-    conflict: Literal[\"inline\", \"rej\"] = \"inline\"\n-    context_lines: PositiveInt = 3\n-    unsafe: bool = False\n-    skip_answered: bool = False\n-    skip_tasks: bool = False\n-\n-    answers: AnswersMap = field(default_factory=AnswersMap, init=False)\n-    _cleanup_hooks: list[Callable[[], None]] = field(default_factory=list, init=False)\n-\n-    def __enter__(self) -> Worker:\n-        \"\"\"Allow using worker as a context manager.\"\"\"\n-        return self\n-\n-    @overload\n-    def __exit__(self, type: None, value: None, traceback: None) -> None: ...\n-\n-    @overload\n-    def __exit__(\n-        self, type: type[BaseException], value: BaseException, traceback: TracebackType\n-    ) -> None: ...\n-\n-    def __exit__(\n-        self,\n-        type: type[BaseException] | None,\n-        value: BaseException | None,\n-        traceback: TracebackType | None,\n-    ) -> None:\n-        \"\"\"Clean up garbage files after worker usage ends.\"\"\"\n-        if value is not None:\n-            # exception was raised from code inside context manager:\n-            # try to clean up, ignoring any exception, then re-raise\n-            with suppress(Exception):\n-                self._cleanup()\n-            raise value\n-        # otherwise clean up and let any exception bubble up\n-        self._cleanup()\n-\n-    def _cleanup(self) -> None:\n-        \"\"\"Execute all stored cleanup methods.\"\"\"\n-        for method in self._cleanup_hooks:\n-            method()\n-\n-    def _check_unsafe(self, mode: Literal[\"copy\", \"update\"]) -> None:\n-        \"\"\"Check whether a template uses unsafe features.\"\"\"\n-        if self.unsafe or self.settings.is_trusted(self.template.url):\n-            return\n-        features: set[str] = set()\n-        if self.template.jinja_extensions:\n-            features.add(\"jinja_extensions\")\n-        if self.template.tasks and not self.skip_tasks:\n-            features.add(\"tasks\")\n-        if mode == \"update\" and self.subproject.template:\n-            if self.subproject.template.jinja_extensions:\n-                features.add(\"jinja_extensions\")\n-            if self.subproject.template.tasks:\n-                features.add(\"tasks\")\n-            for stage in get_args(Literal[\"before\", \"after\"]):\n-                if self.template.migration_tasks(stage, self.subproject.template):\n-                    features.add(\"migrations\")\n-                    break\n-        if features:\n-            raise UnsafeTemplateError(sorted(features))\n-\n-    def _external_data(self) -> LazyDict:\n-        \"\"\"Load external data lazily.\n-\n-        Result keys are used for rendering, and values are the parsed contents\n-        of the YAML files specified in [external_data][].\n-\n-        Files will only be parsed lazily on 1st access. This helps avoiding\n-        circular dependencies when the file name also comes from a variable.\n-        \"\"\"\n-\n-        def _render(path: str) -> str:\n-            with Phase.use(Phase.UNDEFINED):\n-                return self._render_string(path)\n-\n-        # Given those values are lazily rendered on 1st access then cached\n-        # the phase value is irrelevant and could be misleading.\n-        # As a consequence it is explicitely set to \"undefined\".\n-        return LazyDict(\n-            **{\n-                name: lambda path=path: load_answersfile_data(\n-                    self.dst_path, _render(path), warn_on_missing=True\n-                )\n-                for name, path in self.template.external_data.items()\n-            }\n-        )\n-\n-    def _print_message(self, message: str) -> None:\n-        if message and not self.quiet:\n-            print(self._render_string(message), file=sys.stderr)\n-\n-    def _answers_to_remember(self) -> Mapping[str, Any]:\n-        \"\"\"Get only answers that will be remembered in the copier answers file.\"\"\"\n-        # All internal values must appear first\n-        answers: AnyByStrDict = {}\n-        commit = self.template.commit\n-        src = self.template.url\n-        for key, value in ((\"_commit\", commit), (\"_src_path\", src)):\n-            if value is not None:\n-                answers[key] = value\n-        # Other data goes next\n-        answers.update(\n-            (str(k), v)\n-            for (k, v) in self.answers.combined.items()\n-            if not k.startswith(\"_\")\n-            and k not in self.answers.hidden\n-            and k not in self.template.secret_questions\n-            and k in self.template.questions_data\n-            and isinstance(k, JSONSerializable)\n-            and isinstance(v, JSONSerializable)\n-        )\n-        return answers\n-\n-    def _execute_tasks(self, tasks: Sequence[Task]) -> None:\n-        \"\"\"Run the given tasks.\n-\n-        Arguments:\n-            tasks: The list of tasks to run.\n-        \"\"\"\n-        for i, task in enumerate(tasks):\n-            extra_context = {f\"_{k}\": v for k, v in task.extra_vars.items()}\n-\n-            if not cast_to_bool(self._render_value(task.condition, extra_context)):\n-                continue\n-\n-            task_cmd = task.cmd\n-            if isinstance(task_cmd, str):\n-                task_cmd = self._render_string(task_cmd, extra_context)\n-                use_shell = True\n-            else:\n-                task_cmd = [\n-                    self._render_string(str(part), extra_context) for part in task_cmd\n-                ]\n-                use_shell = False\n-\n-            if not self.quiet:\n-                print(\n-                    colors.info\n-                    | f\" > Running task {i + 1} of {len(tasks)}: {task_cmd}\",\n-                    file=sys.stderr,\n-                )\n-            if self.pretend:\n-                continue\n-\n-            working_directory = (\n-                # We can't use _render_path here, as that function has special handling for files in the template\n-                self.subproject.local_abspath\n-                / Path(self._render_string(str(task.working_directory), extra_context))\n-            ).absolute()\n-\n-            extra_env = {k.upper(): str(v) for k, v in task.extra_vars.items()}\n-            with local.cwd(working_directory), local.env(**extra_env):\n-                subprocess.run(task_cmd, shell=use_shell, check=True, env=local.env)\n-\n-    def _render_context(self) -> AnyByStrMutableMapping:\n-        \"\"\"Produce render context for Jinja.\"\"\"\n-        # Backwards compatibility\n-        # FIXME Remove it?\n-        conf = asdict(self)\n-        conf.pop(\"_cleanup_hooks\")\n-        conf.pop(\"answers\")\n-        conf.update(\n-            {\n-                \"answers_file\": self.answers_relpath,\n-                \"src_path\": self.template.local_abspath,\n-                \"vcs_ref_hash\": self.template.commit_hash,\n-                \"sep\": os.sep,\n-                \"os\": OS,\n-            }\n-        )\n-        return dict(\n-            **self.answers.combined,\n-            _copier_answers=self._answers_to_remember(),\n-            _copier_conf=conf,\n-            _folder_name=self.subproject.local_abspath.name,\n-            _copier_python=sys.executable,\n-            _copier_phase=Phase.current(),\n-        )\n-\n-    def _path_matcher(self, patterns: Iterable[str]) -> Callable[[Path], bool]:\n-        \"\"\"Produce a function that matches against specified patterns.\"\"\"\n-        # TODO Is normalization really needed?\n-        normalized_patterns = (normalize(\"NFD\", pattern) for pattern in patterns)\n-        spec = PathSpec.from_lines(\"gitwildmatch\", normalized_patterns)\n-        return spec.match_file\n-\n-    def _solve_render_conflict(self, dst_relpath: Path) -> bool:\n-        \"\"\"Properly solve render conflicts.\n-\n-        It can ask the user if running in interactive mode.\n-        \"\"\"\n-        assert not dst_relpath.is_absolute()\n-        printf(\n-            \"conflict\",\n-            dst_relpath,\n-            style=Style.DANGER,\n-            quiet=self.quiet,\n-            file_=sys.stderr,\n-        )\n-        if self.match_skip(dst_relpath):\n-            printf(\n-                \"skip\",\n-                dst_relpath,\n-                style=Style.OK,\n-                quiet=self.quiet,\n-                file_=sys.stderr,\n-            )\n-            return False\n-        if self.overwrite or dst_relpath == self.answers_relpath:\n-            printf(\n-                \"overwrite\",\n-                dst_relpath,\n-                style=Style.WARNING,\n-                quiet=self.quiet,\n-                file_=sys.stderr,\n-            )\n-            return True\n-        return bool(ask(f\" Overwrite {dst_relpath}?\", default=True))\n-\n-    def _render_allowed(\n-        self,\n-        dst_relpath: Path,\n-        is_dir: bool = False,\n-        is_symlink: bool = False,\n-        expected_contents: bytes | Path = b\"\",\n-    ) -> bool:\n-        \"\"\"Determine if a file or directory can be rendered.\n-\n-        Args:\n-            dst_relpath:\n-                Relative path to destination.\n-            is_dir:\n-                Indicate if the path must be treated as a directory or not.\n-            is_symlink:\n-                Indicate if the path must be treated as a symlink or not.\n-            expected_contents:\n-                Used to compare existing file contents with them. Allows to know if\n-                rendering is needed.\n-        \"\"\"\n-        assert not dst_relpath.is_absolute()\n-        assert not expected_contents or not is_dir, \"Dirs cannot have expected content\"\n-        dst_abspath = Path(self.subproject.local_abspath, dst_relpath)\n-        previous_is_symlink = dst_abspath.is_symlink()\n-        try:\n-            previous_content: bytes | Path\n-            if previous_is_symlink:\n-                previous_content = dst_abspath.readlink()\n-            else:\n-                previous_content = dst_abspath.read_bytes()\n-        except FileNotFoundError:\n-            printf(\n-                \"create\",\n-                dst_relpath,\n-                style=Style.OK,\n-                quiet=self.quiet,\n-                file_=sys.stderr,\n-            )\n-            return True\n-        except PermissionError as error:\n-            # HACK https://bugs.python.org/issue43095\n-            if not (error.errno == 13 and platform.system() == \"Windows\"):\n-                raise\n-        except IsADirectoryError:\n-            assert is_dir\n-        if is_dir or (\n-            previous_content == expected_contents and previous_is_symlink == is_symlink\n-        ):\n-            printf(\n-                \"identical\",\n-                dst_relpath,\n-                style=Style.IGNORE,\n-                quiet=self.quiet,\n-                file_=sys.stderr,\n-            )\n-            return is_dir\n-        return self._solve_render_conflict(dst_relpath)\n-\n-    def _ask(self) -> None:  # noqa: C901\n-        \"\"\"Ask the questions of the questionnaire and record their answers.\"\"\"\n-        self.answers = AnswersMap(\n-            user_defaults=self.user_defaults,\n-            init=self.data,\n-            last=self.subproject.last_answers,\n-            metadata=self.template.metadata,\n-            external=self._external_data(),\n-        )\n-\n-        for var_name, details in self.template.questions_data.items():\n-            question = Question(\n-                answers=self.answers,\n-                jinja_env=self.jinja_env,\n-                settings=self.settings,\n-                var_name=var_name,\n-                **details,\n-            )\n-            # Delete last answer if it cannot be parsed or validated, so a new\n-            # valid answer can be provided.\n-            if var_name in self.answers.last:\n-                try:\n-                    answer = question.parse_answer(self.answers.last[var_name])\n-                except Exception:\n-                    del self.answers.last[var_name]\n-                else:\n-                    if question.validate_answer(answer):\n-                        del self.answers.last[var_name]\n-            # Skip a question when the skip condition is met.\n-            if not question.get_when():\n-                # Omit its answer from the answers file.\n-                self.answers.hide(var_name)\n-                # Skip immediately to the next question when it has no default\n-                # value.\n-                if question.default is MISSING:\n-                    continue\n-            if var_name in self.answers.init:\n-                # Try to parse the answer value.\n-                answer = question.parse_answer(self.answers.init[var_name])\n-                # Try to validate the answer value if the question has a\n-                # validator.\n-                if err_msg := question.validate_answer(answer):\n-                    raise ValueError(\n-                        f\"Validation error for question '{var_name}': {err_msg}\"\n-                    )\n-                # At this point, the answer value is valid. Do not ask the\n-                # question again, but set answer as the user's answer instead.\n-                self.answers.user[var_name] = answer\n-                continue\n-            # Skip a question when the user already answered it.\n-            if self.skip_answered and var_name in self.answers.last:\n-                continue\n-\n-            # Display TUI and ask user interactively only without --defaults\n-            try:\n-                if self.defaults:\n-                    new_answer = question.get_default()\n-                    if new_answer is MISSING:\n-                        raise ValueError(f'Question \"{var_name}\" is required')\n-                else:\n-                    new_answer = unsafe_prompt(\n-                        [question.get_questionary_structure()],\n-                        answers={question.var_name: question.get_default()},\n-                    )[question.var_name]\n-            except KeyboardInterrupt as err:\n-                raise CopierAnswersInterrupt(\n-                    self.answers, question, self.template\n-                ) from err\n-            self.answers.user[var_name] = new_answer\n-\n-        # Reload external data, which may depend on answers\n-        self.answers.external = self._external_data()\n-\n-    @property\n-    def answers_relpath(self) -> Path:\n-        \"\"\"Obtain the proper relative path for the answers file.\n-\n-        It comes from:\n-\n-        1. User choice.\n-        2. Template default.\n-        3. Copier default.\n-        \"\"\"\n-        path = self.answers_file or self.template.answers_relpath\n-        template = self.jinja_env.from_string(str(path))\n-        return Path(\n-            template.render(_copier_phase=Phase.current(), **self.answers.combined)\n-        )\n-\n-    @cached_property\n-    def all_exclusions(self) -> Sequence[str]:\n-        \"\"\"Combine default, template and user-chosen exclusions.\"\"\"\n-        return self.template.exclude + tuple(self.exclude)\n-\n-    @cached_property\n-    def jinja_env(self) -> YieldEnvironment:\n-        \"\"\"Return a pre-configured Jinja environment.\n-\n-        Respects template settings.\n-        \"\"\"\n-        paths = [str(self.template.local_abspath)]\n-        loader = FileSystemLoader(paths)\n-        default_extensions = [\n-            \"jinja2_ansible_filters.AnsibleCoreFiltersExtension\",\n-            YieldExtension,\n-        ]\n-        extensions = default_extensions + list(self.template.jinja_extensions)\n-        try:\n-            env = YieldEnvironment(\n-                loader=loader, extensions=extensions, **self.template.envops\n-            )\n-        except ModuleNotFoundError as error:\n-            raise ExtensionNotFoundError(\n-                f\"Copier could not load some Jinja extensions:\\n{error}\\n\"\n-                \"Make sure to install these extensions alongside Copier itself.\\n\"\n-                \"See the docs at https://copier.readthedocs.io/en/latest/configuring/#jinja_extensions\"\n-            )\n-        # patch the `to_json` filter to support Pydantic dataclasses\n-        env.filters[\"to_json\"] = partial(\n-            env.filters[\"to_json\"], default=to_jsonable_python\n-        )\n-\n-        # Add a global function to join filesystem paths.\n-        separators = {\n-            \"posix\": \"/\",\n-            \"windows\": \"\\\\\",\n-            \"native\": os.path.sep,\n-        }\n-\n-        def _pathjoin(\n-            *path: str, mode: Literal[\"posix\", \"windows\", \"native\"] = \"posix\"\n-        ) -> str:\n-            return separators[mode].join(path)\n-\n-        env.globals[\"pathjoin\"] = _pathjoin\n-        return env\n-\n-    @cached_property\n-    def match_exclude(self) -> Callable[[Path], bool]:\n-        \"\"\"Get a callable to match paths against all exclusions.\"\"\"\n-        return self._path_matcher(self.all_exclusions)\n-\n-    @cached_property\n-    def match_skip(self) -> Callable[[Path], bool]:\n-        \"\"\"Get a callable to match paths against all skip-if-exists patterns.\"\"\"\n-        return self._path_matcher(\n-            map(\n-                self._render_string,\n-                tuple(chain(self.skip_if_exists, self.template.skip_if_exists)),\n-            )\n-        )\n-\n-    def _render_template(self) -> None:\n-        \"\"\"Render the template in the subproject root.\"\"\"\n-        follow_symlinks = not self.template.preserve_symlinks\n-        for src in scantree(str(self.template_copy_root), follow_symlinks):\n-            src_abspath = Path(src.path)\n-            src_relpath = Path(src_abspath).relative_to(self.template.local_abspath)\n-            dst_relpaths_ctxs = self._render_path(\n-                Path(src_abspath).relative_to(self.template_copy_root)\n-            )\n-            for dst_relpath, ctx in dst_relpaths_ctxs:\n-                if self.match_exclude(dst_relpath):\n-                    continue\n-                if src.is_symlink() and self.template.preserve_symlinks:\n-                    self._render_symlink(src_relpath, dst_relpath)\n-                elif src.is_dir(follow_symlinks=follow_symlinks):\n-                    self._render_folder(dst_relpath)\n-                else:\n-                    self._render_file(src_relpath, dst_relpath, extra_context=ctx or {})\n-\n-    def _render_file(\n-        self,\n-        src_relpath: Path,\n-        dst_relpath: Path,\n-        extra_context: AnyByStrDict | None = None,\n-    ) -> None:\n-        \"\"\"Render one file.\n-\n-        Args:\n-            src_relpath:\n-                File to be rendered. It must be a path relative to the template\n-                root.\n-            dst_relpath:\n-                File to be created. It must be a path relative to the subproject\n-                root.\n-            extra_context:\n-                Additional variables to use for rendering the template.\n-        \"\"\"\n-        # TODO Get from main.render_file()\n-        assert not src_relpath.is_absolute()\n-        assert not dst_relpath.is_absolute()\n-        src_abspath = self.template.local_abspath / src_relpath\n-        if src_relpath.name.endswith(self.template.templates_suffix):\n-            try:\n-                tpl = self.jinja_env.get_template(src_relpath.as_posix())\n-            except UnicodeDecodeError:\n-                if self.template.templates_suffix:\n-                    # suffix is not empty, re-raise\n-                    raise\n-                # suffix is empty, fallback to copy\n-                new_content = src_abspath.read_bytes()\n-            else:\n-                new_content = tpl.render(\n-                    **self._render_context(), **(extra_context or {})\n-                ).encode()\n-                if self.jinja_env.yield_name:\n-                    raise YieldTagInFileError(\n-                        f\"File {src_relpath} contains a yield tag, but it is not allowed.\"\n-                    )\n-        else:\n-            new_content = src_abspath.read_bytes()\n-        dst_abspath = self.subproject.local_abspath / dst_relpath\n-        src_mode = src_abspath.stat().st_mode\n-        if not self._render_allowed(dst_relpath, expected_contents=new_content):\n-            return\n-        if not self.pretend:\n-            dst_abspath.parent.mkdir(parents=True, exist_ok=True)\n-            if dst_abspath.is_symlink():\n-                # Writing to a symlink just writes to its target, so if we want to\n-                # replace a symlink with a file we have to unlink it first\n-                dst_abspath.unlink()\n-            dst_abspath.write_bytes(new_content)\n-            dst_abspath.chmod(src_mode)\n-\n-    def _render_symlink(self, src_relpath: Path, dst_relpath: Path) -> None:\n-        \"\"\"Render one symlink.\n-\n-        Args:\n-            src_relpath:\n-                Symlink to be rendered. It must be a path relative to the\n-                template root.\n-            dst_relpath:\n-                Symlink to be created. It must be a path relative to the\n-                subproject root.\n-        \"\"\"\n-        assert not src_relpath.is_absolute()\n-        assert not dst_relpath.is_absolute()\n-        if dst_relpath is None or self.match_exclude(dst_relpath):\n-            return\n-\n-        src_abspath = self.template.local_abspath / src_relpath\n-        src_target = src_abspath.readlink()\n-        if src_abspath.name.endswith(self.template.templates_suffix):\n-            dst_target = Path(self._render_string(str(src_target)))\n-        else:\n-            dst_target = src_target\n-\n-        if not self._render_allowed(\n-            dst_relpath,\n-            expected_contents=dst_target,\n-            is_symlink=True,\n-        ):\n-            return\n-\n-        if not self.pretend:\n-            dst_abspath = self.subproject.local_abspath / dst_relpath\n-            # symlink_to doesn't overwrite existing files, so delete it first\n-            if dst_abspath.is_symlink() or dst_abspath.exists():\n-                dst_abspath.unlink()\n-            dst_abspath.parent.mkdir(parents=True, exist_ok=True)\n-            dst_abspath.symlink_to(dst_target)\n-            if sys.platform == \"darwin\":\n-                # Only macOS supports permissions on symlinks.\n-                # Other platforms just copy the permission of the target\n-                src_mode = src_abspath.lstat().st_mode\n-                dst_abspath.lchmod(src_mode)\n-\n-    def _render_folder(self, dst_relpath: Path) -> None:\n-        \"\"\"Create one folder (without content).\n-\n-        Args:\n-            dst_relpath:\n-                Folder to be created. It must be a path relative to the\n-                subproject root.\n-        \"\"\"\n-        assert not dst_relpath.is_absolute()\n-        if not self.pretend and self._render_allowed(dst_relpath, is_dir=True):\n-            dst_abspath = self.subproject.local_abspath / dst_relpath\n-            dst_abspath.mkdir(parents=True, exist_ok=True)\n-\n-    def _adjust_rendered_part(self, rendered_part: str) -> str:\n-        \"\"\"Adjust the rendered part if necessary.\n-\n-        If `{{ _copier_conf.answers_file }}` becomes the full path,\n-        restore part to be just the end leaf.\n-\n-        Args:\n-            rendered_part:\n-                The rendered part of the path to adjust.\n-\n-        \"\"\"\n-        if str(self.answers_relpath) == rendered_part:\n-            return Path(rendered_part).name\n-        return rendered_part\n-\n-    def _render_parts(\n-        self,\n-        parts: tuple[str, ...],\n-        rendered_parts: tuple[str, ...] | None = None,\n-        extra_context: AnyByStrDict | None = None,\n-        is_template: bool = False,\n-    ) -> Iterable[tuple[Path, AnyByStrDict | None]]:\n-        \"\"\"Render a set of parts into path and context pairs.\n-\n-        If a yield tag is found in a part, it will recursively yield multiple path and context pairs.\n-        \"\"\"\n-        if rendered_parts is None:\n-            rendered_parts = tuple()\n-\n-        if not parts:\n-            rendered_path = Path(*rendered_parts)\n-\n-            templated_sibling = (\n-                self.template.local_abspath\n-                / f\"{rendered_path}{self.template.templates_suffix}\"\n-            )\n-            if is_template or not templated_sibling.exists():\n-                yield rendered_path, extra_context\n-\n-            return\n-\n-        part = parts[0]\n-        parts = parts[1:]\n-\n-        if not extra_context:\n-            extra_context = {}\n-\n-        # If the `part` has a yield tag, `self.jinja_env` will be set with the yield name and iterable\n-        rendered_part = self._render_string(part, extra_context=extra_context)\n-\n-        yield_name = self.jinja_env.yield_name\n-        if yield_name:\n-            for value in self.jinja_env.yield_iterable or ():\n-                new_context = {**extra_context, yield_name: value}\n-                rendered_part = self._render_string(part, extra_context=new_context)\n-                rendered_part = self._adjust_rendered_part(rendered_part)\n-\n-                # Skip if any part is rendered as an empty string\n-                if not rendered_part:\n-                    continue\n-\n-                yield from self._render_parts(\n-                    parts, rendered_parts + (rendered_part,), new_context, is_template\n-                )\n-\n-            return\n-\n-        # Skip if any part is rendered as an empty string\n-        if not rendered_part:\n-            return\n-\n-        rendered_part = self._adjust_rendered_part(rendered_part)\n-\n-        yield from self._render_parts(\n-            parts, rendered_parts + (rendered_part,), extra_context, is_template\n-        )\n-\n-    def _render_path(self, relpath: Path) -> Iterable[tuple[Path, AnyByStrDict | None]]:\n-        \"\"\"Render one relative path into multiple path and context pairs.\n-\n-        Args:\n-            relpath:\n-                The relative path to be rendered. Obviously, it can be templated.\n-        \"\"\"\n-        is_template = relpath.name.endswith(self.template.templates_suffix)\n-        templated_sibling = (\n-            self.template_copy_root / f\"{relpath}{self.template.templates_suffix}\"\n-        )\n-        # With an empty suffix, the templated sibling always exists.\n-        if templated_sibling.exists() and self.template.templates_suffix:\n-            return\n-        if self.template.templates_suffix and is_template:\n-            relpath = relpath.with_suffix(\"\")\n-\n-        yield from self._render_parts(relpath.parts, is_template=is_template)\n-\n-    def _render_string(\n-        self, string: str, extra_context: AnyByStrDict | None = None\n-    ) -> str:\n-        \"\"\"Render one templated string.\n-\n-        Args:\n-            string:\n-                The template source string.\n-\n-            extra_context:\n-                Additional variables to use for rendering the template.\n-        \"\"\"\n-        tpl = self.jinja_env.from_string(string)\n-        return tpl.render(**self._render_context(), **(extra_context or {}))\n-\n-    def _render_value(\n-        self, value: _T, extra_context: AnyByStrDict | None = None\n-    ) -> str | _T:\n-        \"\"\"Render a value, which may or may not be a templated string.\n-\n-        Args:\n-            value:\n-                The value to render.\n-\n-            extra_context:\n-                Additional variables to use for rendering the template.\n-        \"\"\"\n-        try:\n-            return self._render_string(value, extra_context=extra_context)  # type: ignore[arg-type]\n-        except TypeError:\n-            return value\n-\n-    @cached_property\n-    def subproject(self) -> Subproject:\n-        \"\"\"Get related subproject.\"\"\"\n-        result = Subproject(\n-            local_abspath=self.dst_path.absolute(),\n-            answers_relpath=self.answers_file or Path(\".copier-answers.yml\"),\n-        )\n-        self._cleanup_hooks.append(result._cleanup)\n-        return result\n-\n-    @cached_property\n-    def template(self) -> Template:\n-        \"\"\"Get related template.\"\"\"\n-        url = self.src_path\n-        if not url:\n-            if self.subproject.template is None:\n-                raise TypeError(\"Template not found\")\n-            url = str(self.subproject.template.url)\n-        result = Template(\n-            url=url, ref=self.vcs_ref, use_prereleases=self.use_prereleases\n-        )\n-        self._cleanup_hooks.append(result._cleanup)\n-        return result\n-\n-    @cached_property\n-    def template_copy_root(self) -> Path:\n-        \"\"\"Absolute path from where to start copying.\n-\n-        It points to the cloned template local abspath + the rendered subdir, if any.\n-        \"\"\"\n-        subdir = self._render_string(self.template.subdirectory) or \"\"\n-        return self.template.local_abspath / subdir\n-\n-    # Main operations\n-    def run_copy(self) -> None:\n-        \"\"\"Generate a subproject from zero, ignoring what was in the folder.\n-\n-        If `dst_path` was missing, it will be\n-        created. Otherwise, `src_path` be rendered\n-        directly into it, without worrying about evolving what was there\n-        already.\n-\n-        See [generating a project][generating-a-project].\n-        \"\"\"\n-        self._check_unsafe(\"copy\")\n-        self._print_message(self.template.message_before_copy)\n-        with Phase.use(Phase.PROMPT):\n-            self._ask()\n-        was_existing = self.subproject.local_abspath.exists()\n-        try:\n-            if not self.quiet:\n-                # TODO Unify printing tools\n-                print(\n-                    f\"\\nCopying from template version {self.template.version}\",\n-                    file=sys.stderr,\n-                )\n-            with Phase.use(Phase.RENDER):\n-                self._render_template()\n-            if not self.quiet:\n-                # TODO Unify printing tools\n-                print(\"\")  # padding space\n-            if not self.skip_tasks:\n-                with Phase.use(Phase.TASKS):\n-                    self._execute_tasks(self.template.tasks)\n-        except Exception:\n-            if not was_existing and self.cleanup_on_error:\n-                rmtree(self.subproject.local_abspath)\n-            raise\n-        self._print_message(self.template.message_after_copy)\n-        if not self.quiet:\n-            # TODO Unify printing tools\n-            print(\"\")  # padding space\n-\n-    def run_recopy(self) -> None:\n-        \"\"\"Update a subproject, keeping answers but discarding evolution.\"\"\"\n-        if self.subproject.template is None:\n-            raise UserMessageError(\n-                \"Cannot recopy because cannot obtain old template references \"\n-                f\"from `{self.subproject.answers_relpath}`.\"\n-            )\n-        with replace(self, src_path=self.subproject.template.url) as new_worker:\n-            new_worker.run_copy()\n-\n-    def run_update(self) -> None:\n-        \"\"\"Update a subproject that was already generated.\n-\n-        See [updating a project][updating-a-project].\n-        \"\"\"\n-        self._check_unsafe(\"update\")\n-        # Check all you need is there\n-        if self.subproject.vcs != \"git\":\n-            raise UserMessageError(\n-                \"Updating is only supported in git-tracked subprojects.\"\n-            )\n-        if self.subproject.is_dirty():\n-            raise UserMessageError(\n-                \"Destination repository is dirty; cannot continue. \"\n-                \"Please commit or stash your local changes and retry.\"\n-            )\n-        if self.subproject.template is None or self.subproject.template.ref is None:\n-            raise UserMessageError(\n-                \"Cannot update because cannot obtain old template references \"\n-                f\"from `{self.subproject.answers_relpath}`.\"\n-            )\n-        if self.template.commit is None:\n-            raise UserMessageError(\n-                \"Updating is only supported in git-tracked templates.\"\n-            )\n-        if not self.subproject.template.version:\n-            raise UserMessageError(\n-                \"Cannot update: version from last update not detected.\"\n-            )\n-        if not self.template.version:\n-            raise UserMessageError(\"Cannot update: version from template not detected.\")\n-        if self.subproject.template.version > self.template.version:\n-            raise UserMessageError(\n-                f\"You are downgrading from {self.subproject.template.version} to {self.template.version}. \"\n-                \"Downgrades are not supported.\"\n-            )\n-        if not self.overwrite:\n-            # Only git-tracked subprojects can be updated, so the user can\n-            # review the diff before committing; so we can safely avoid\n-            # asking for confirmation\n-            raise UserMessageError(\"Enable overwrite to update a subproject.\")\n-        self._print_message(self.template.message_before_update)\n-        if not self.quiet:\n-            # TODO Unify printing tools\n-            print(\n-                f\"Updating to template version {self.template.version}\", file=sys.stderr\n-            )\n-        self._apply_update()\n-        self._print_message(self.template.message_after_update)\n-\n-    def _apply_update(self) -> None:  # noqa: C901\n-        git = get_git()\n-        subproject_top = Path(\n-            git(\n-                \"-C\",\n-                self.subproject.local_abspath,\n-                \"rev-parse\",\n-                \"--show-toplevel\",\n-            ).strip()\n-        )\n-        subproject_subdir = self.subproject.local_abspath.relative_to(subproject_top)\n-\n-        with (\n-            TemporaryDirectory(\n-                prefix=f\"{__name__}.old_copy.\",\n-            ) as old_copy,\n-            TemporaryDirectory(\n-                prefix=f\"{__name__}.new_copy.\",\n-            ) as new_copy,\n-        ):\n-            # Copy old template into a temporary destination\n-            with replace(\n-                self,\n-                dst_path=old_copy / subproject_subdir,\n-                data=self.subproject.last_answers,\n-                defaults=True,\n-                quiet=True,\n-                src_path=self.subproject.template.url,  # type: ignore[union-attr]\n-                vcs_ref=self.subproject.template.commit,  # type: ignore[union-attr]\n-            ) as old_worker:\n-                old_worker.run_copy()\n-            # Run pre-migration tasks\n-            with Phase.use(Phase.MIGRATE):\n-                self._execute_tasks(\n-                    self.template.migration_tasks(\"before\", self.subproject.template)  # type: ignore[arg-type]\n-                )\n-            # Create a Git tree object from the current (possibly dirty) index\n-            # and keep the object reference.\n-            with local.cwd(subproject_top):\n-                subproject_head = git(\"write-tree\").strip()\n-            with local.cwd(old_copy):\n-                self._git_initialize_repo()\n-                # Configure borrowing Git objects from the real destination.\n-                set_git_alternates(subproject_top)\n-                # Save a list of files that were intentionally removed in the generated\n-                # project to avoid recreating them during the update.\n-                # Files listed in `skip_if_exists` should only be skipped if they exist.\n-                # They should even be recreated if deleted intentionally.\n-                files_removed = git(\n-                    \"diff-tree\",\n-                    \"-r\",\n-                    \"--diff-filter=D\",\n-                    \"--name-only\",\n-                    \"HEAD\",\n-                    subproject_head,\n-                ).splitlines()\n-                exclude_plus_removed = list(\n-                    set(self.exclude).union(\n-                        map(\n-                            escape_git_path,\n-                            map(\n-                                normalize_git_path,\n-                                (\n-                                    path\n-                                    for path in files_removed\n-                                    if not self.match_skip(path)\n-                                ),\n-                            ),\n-                        )\n-                    )\n-                )\n-            # Clear last answers cache to load possible answers migration, if skip_answered flag is not set\n-            if self.skip_answered is False:\n-                self.answers = AnswersMap(external=self._external_data())\n-                with suppress(AttributeError):\n-                    del self.subproject.last_answers\n-            # Do a normal update in final destination\n-            with replace(\n-                self,\n-                # Don't regenerate intentionally deleted paths\n-                exclude=exclude_plus_removed,\n-                # Files can change due to the historical diff, and those\n-                # changes are not detected in this process, so it's better to\n-                # say nothing than lie.\n-                # TODO\n-                quiet=True,\n-            ) as current_worker:\n-                current_worker.run_copy()\n-                self.answers = current_worker.answers\n-                self.answers.external = self._external_data()\n-            # Render with the same answers in an empty dir to avoid pollution\n-            with replace(\n-                self,\n-                dst_path=new_copy / subproject_subdir,\n-                data={\n-                    k: v\n-                    for k, v in self.answers.combined.items()\n-                    if k not in self.answers.hidden\n-                },\n-                defaults=True,\n-                quiet=True,\n-                src_path=self.subproject.template.url,  # type: ignore[union-attr]\n-                exclude=exclude_plus_removed,\n-            ) as new_worker:\n-                new_worker.run_copy()\n-            with local.cwd(new_copy):\n-                self._git_initialize_repo()\n-                new_copy_head = git(\"rev-parse\", \"HEAD\").strip()\n-            # Extract diff between temporary destination and real destination\n-            # with some special handling of newly added files in both the project\n-            # and the template.\n-            with local.cwd(old_copy):\n-                # Configure borrowing Git objects from the real destination and\n-                # temporary destination of the new template.\n-                set_git_alternates(subproject_top, Path(new_copy))\n-                # Create an empty file in the temporary destination when the\n-                # same file was added in *both* the project and the temporary\n-                # destination of the new template. With this minor change, the\n-                # diff between the temporary destination and the real\n-                # destination for such files will use the \"update file mode\"\n-                # instead of the \"new file mode\" which avoids deleting the file\n-                # content previously added in the project.\n-                diff_added_cmd = git[\n-                    \"diff-tree\", \"-r\", \"--diff-filter=A\", \"--name-only\"\n-                ]\n-                for filename in (\n-                    set(diff_added_cmd(\"HEAD\", subproject_head).splitlines())\n-                ) & set(diff_added_cmd(\"HEAD\", new_copy_head).splitlines()):\n-                    f = Path(filename)\n-                    f.parent.mkdir(parents=True, exist_ok=True)\n-                    f.touch((subproject_top / filename).stat().st_mode)\n-                    git(\"add\", \"--force\", filename)\n-                self._git_commit(\"add new empty files\")\n-                # Extract diff between temporary destination and real\n-                # destination\n-                diff_cmd = git[\n-                    \"diff-tree\",\n-                    f\"--unified={self.context_lines}\",\n-                    \"HEAD\",\n-                    subproject_head,\n-                ]\n-                try:\n-                    diff = diff_cmd(\"--inter-hunk-context=-1\")\n-                except ProcessExecutionError:\n-                    print(\n-                        colors.warn\n-                        | \"Make sure Git >= 2.24 is installed to improve updates.\",\n-                        file=sys.stderr,\n-                    )\n-                    diff = diff_cmd(\"--inter-hunk-context=0\")\n-            compared = dircmp(old_copy, new_copy)\n-            # Try to apply cached diff into final destination\n-            with local.cwd(subproject_top):\n-                apply_cmd = git[\"apply\", \"--reject\", \"--exclude\", self.answers_relpath]\n-                ignored_files = git[\"status\", \"--ignored\", \"--porcelain\"]()\n-                # returns \"!! file1\\n !! file2\\n\"\n-                # extra_exclude will contain: [\"file1\", file2\"]\n-                extra_exclude = [\n-                    filename.split(\"!! \").pop()\n-                    for filename in ignored_files.splitlines()\n-                ]\n-                for skip_pattern in chain(\n-                    self.skip_if_exists, self.template.skip_if_exists, extra_exclude\n-                ):\n-                    apply_cmd = apply_cmd[\"--exclude\", skip_pattern]\n-                (apply_cmd << diff)(retcode=None)\n-                if self.conflict == \"inline\":\n-                    conflicted = []\n-                    old_path = Path(old_copy)\n-                    new_path = Path(new_copy)\n-                    status = git(\"status\", \"--porcelain\").strip().splitlines()\n-                    for line in status:\n-                        # Filter merge rejections (part 1/2)\n-                        if not line.startswith(\"?? \"):\n-                            continue\n-                        # Remove \"?? \" prefix\n-                        fname = line[3:]\n-                        # Normalize name\n-                        fname = normalize_git_path(fname)\n-                        # Filter merge rejections (part 2/2)\n-                        if not fname.endswith(\".rej\"):\n-                            continue\n-                        # Remove \".rej\" suffix\n-                        fname = fname[:-4]\n-                        # Undo possible non-rejected chunks\n-                        git(\"checkout\", \"--\", fname)\n-                        # 3-way-merge the file directly\n-                        git(\n-                            \"merge-file\",\n-                            \"-L\",\n-                            \"before updating\",\n-                            \"-L\",\n-                            \"last update\",\n-                            \"-L\",\n-                            \"after updating\",\n-                            fname,\n-                            old_path / fname,\n-                            new_path / fname,\n-                            retcode=None,\n-                        )\n-                        # Remove rejection witness\n-                        Path(f\"{fname}.rej\").unlink()\n-                        # The 3-way merge might have resolved conflicts automatically,\n-                        # so we need to check if the file contains conflict markers\n-                        # before storing the file name for marking it as unmerged after the loop.\n-                        with Path(fname).open() as conflicts_candidate:\n-                            if any(\n-                                line.rstrip()\n-                                in {\"<<<<<<< before updating\", \">>>>>>> after updating\"}\n-                                for line in conflicts_candidate\n-                            ):\n-                                conflicted.append(fname)\n-                    # We ran `git merge-file` outside of a regular merge operation,\n-                    # which means no merge conflict is recorded in the index.\n-                    # Only the usual stage 0 is recorded, with the hash of the current version.\n-                    # We therefore update the index with the missing stages:\n-                    # 1 = current (before updating), 2 = base (last update), 3 = other (after updating).\n-                    # See this SO post: https://stackoverflow.com/questions/79309642/\n-                    # and Git docs: https://git-scm.com/docs/git-update-index#_using_index_info.\n-                    if conflicted:\n-                        input_lines = []\n-                        for line in (\n-                            git(\"ls-files\", \"--stage\", *conflicted).strip().splitlines()\n-                        ):\n-                            perms_sha_mode, path = line.split(\"\\t\")\n-                            perms, sha, _ = perms_sha_mode.split()\n-                            input_lines.append(f\"0 {'0' * 40}\\t{path}\")\n-                            input_lines.append(f\"{perms} {sha} 1\\t{path}\")\n-                            with suppress(ProcessExecutionError):\n-                                # The following command will fail\n-                                # if the file did not exist in the previous version.\n-                                old_sha = git(\n-                                    \"hash-object\",\n-                                    \"-w\",\n-                                    old_path / normalize_git_path(path),\n-                                ).strip()\n-                                input_lines.append(f\"{perms} {old_sha} 2\\t{path}\")\n-                            with suppress(ProcessExecutionError):\n-                                # The following command will fail\n-                                # if the file was deleted in the latest version.\n-                                new_sha = git(\n-                                    \"hash-object\",\n-                                    \"-w\",\n-                                    new_path / normalize_git_path(path),\n-                                ).strip()\n-                                input_lines.append(f\"{perms} {new_sha} 3\\t{path}\")\n-                        (\n-                            git[\"update-index\", \"--index-info\"]\n-                            << \"\\n\".join(input_lines)\n-                        )()\n-            # Trigger recursive removal of deleted files in last template version\n-            _remove_old_files(subproject_top, compared)\n-\n-        # Run post-migration tasks\n-        with Phase.use(Phase.MIGRATE):\n-            self._execute_tasks(\n-                self.template.migration_tasks(\"after\", self.subproject.template)  # type: ignore[arg-type]\n-            )\n-\n-    def _git_initialize_repo(self) -> None:\n-        \"\"\"Initialize a git repository in the current directory.\"\"\"\n-        git = get_git()\n-        git(\"init\", retcode=None)\n-        git(\"add\", \".\")\n-        self._git_commit()\n-\n-    def _git_commit(self, message: str = \"dumb commit\") -> None:\n-        git = get_git()\n-        # 1st commit could fail if any pre-commit hook reformats code\n-        # 2nd commit uses --no-verify to disable pre-commit-like checks\n-        git(\n-            \"commit\",\n-            \"--allow-empty\",\n-            \"-am\",\n-            f\"{message} 1\",\n-            \"--no-gpg-sign\",\n-            retcode=None,\n-        )\n-        git(\n-            \"commit\",\n-            \"--allow-empty\",\n-            \"-am\",\n-            f\"{message} 2\",\n-            \"--no-gpg-sign\",\n-            \"--no-verify\",\n-        )\n-\n-\n-def run_copy(\n-    src_path: str,\n-    dst_path: StrOrPath = \".\",\n-    data: AnyByStrDict | None = None,\n-    **kwargs: Any,\n-) -> Worker:\n-    \"\"\"Copy a template to a destination, from zero.\n-\n-    This is a shortcut for [run_copy][copier.main.Worker.run_copy].\n-\n-    See [Worker][copier.main.Worker] fields to understand this function's args.\n-    \"\"\"\n-    if data is not None:\n-        kwargs[\"data\"] = data\n-    with Worker(src_path=src_path, dst_path=Path(dst_path), **kwargs) as worker:\n-        worker.run_copy()\n-    return worker\n-\n-\n-def run_recopy(\n-    dst_path: StrOrPath = \".\", data: AnyByStrDict | None = None, **kwargs: Any\n-) -> Worker:\n-    \"\"\"Update a subproject from its template, discarding subproject evolution.\n-\n-    This is a shortcut for [run_recopy][copier.main.Worker.run_recopy].\n-\n-    See [Worker][copier.main.Worker] fields to understand this function's args.\n-    \"\"\"\n-    if data is not None:\n-        kwargs[\"data\"] = data\n-    with Worker(dst_path=Path(dst_path), **kwargs) as worker:\n-        worker.run_recopy()\n-    return worker\n-\n-\n-def run_update(\n-    dst_path: StrOrPath = \".\",\n-    data: AnyByStrDict | None = None,\n-    **kwargs: Any,\n-) -> Worker:\n-    \"\"\"Update a subproject, from its template.\n-\n-    This is a shortcut for [run_update][copier.main.Worker.run_update].\n-\n-    See [Worker][copier.main.Worker] fields to understand this function's args.\n-    \"\"\"\n-    if data is not None:\n-        kwargs[\"data\"] = data\n-    with Worker(dst_path=Path(dst_path), **kwargs) as worker:\n-        worker.run_update()\n-    return worker\n-\n-\n-def _remove_old_files(prefix: Path, cmp: dircmp[str], rm_common: bool = False) -> None:\n-    \"\"\"Remove files and directories only found in \"old\" template.\n-\n-    This is an internal helper method used to process a comparison of 2\n-    directories, where the left one is considered the \"old\" one, and the\n-    right one is the \"new\" one.\n-\n-    Then, it will recursively try to remove anything that is only in the old\n-    directory.\n-\n-    Args:\n-        prefix:\n-            Where we start removing. It can be different from the directories\n-            being compared.\n-        cmp:\n-            The comparison result.\n-        rm_common:\n-            Should we remove common files and directories?\n-    \"\"\"\n-    # Gather files and dirs to remove\n-    to_rm = []\n-    subdirs = {}\n-    with suppress(NotADirectoryError, FileNotFoundError):\n-        to_rm = cmp.left_only\n-        if rm_common:\n-            to_rm += cmp.common_files + cmp.common_dirs\n-        subdirs = cmp.subdirs\n-    # Remove files found only in old template copy\n-    for name in to_rm:\n-        target = prefix / name\n-        if target.is_file():\n-            target.unlink()\n-        else:\n-            # Recurse in dirs totally removed in latest template\n-            _remove_old_files(target, dircmp(Path(cmp.left, name), target), True)\n-            # Remove subdir if it ends empty\n-            with suppress(OSError):\n-                target.rmdir()  # Raises if dir not empty\n-    # Recurse\n-    for key, value in subdirs.items():\n-        subdir = prefix / key\n-        _remove_old_files(subdir, value)\n-        # Remove subdir if it ends empty\n-        with suppress(OSError):\n-            subdir.rmdir()  # Raises if dir not empty\n+\"\"\"Main functions and classes, used to generate or update projects.\"\"\"\n+\n+from __future__ import annotations\n+\n+import os\n+import platform\n+import subprocess\n+import sys\n+from contextlib import suppress\n+from dataclasses import asdict, field, replace\n+from filecmp import dircmp\n+from functools import cached_property, partial\n+from itertools import chain\n+from pathlib import Path\n+from shutil import rmtree\n+from tempfile import TemporaryDirectory\n+from types import TracebackType\n+from typing import (\n+    Any,\n+    Callable,\n+    Iterable,\n+    Literal,\n+    Mapping,\n+    Sequence,\n+    TypeVar,\n+    get_args,\n+    overload,\n+)\n+from unicodedata import normalize\n+\n+from jinja2.loaders import FileSystemLoader\n+from pathspec import PathSpec\n+from plumbum import ProcessExecutionError, colors\n+from plumbum.cli.terminal import ask\n+from plumbum.machines import local\n+from pydantic import ConfigDict, PositiveInt\n+from pydantic.dataclasses import dataclass\n+from pydantic_core import to_jsonable_python\n+from questionary import unsafe_prompt\n+\n+from .errors import (\n+    CopierAnswersInterrupt,\n+    ExtensionNotFoundError,\n+    UnsafeTemplateError,\n+    UserMessageError,\n+    YieldTagInFileError,\n+)\n+from .jinja_ext import YieldEnvironment, YieldExtension\n+from .settings import Settings\n+from .subproject import Subproject\n+from .template import Task, Template\n+from .tools import (\n+    OS,\n+    Style,\n+    cast_to_bool,\n+    escape_git_path,\n+    normalize_git_path,\n+    printf,\n+    scantree,\n+    set_git_alternates,\n+)\n+from .types import (\n+    MISSING,\n+    AnyByStrDict,\n+    AnyByStrMutableMapping,\n+    JSONSerializable,\n+    LazyDict,\n+    Phase,\n+    RelativePath,\n+    StrOrPath,\n+)\n+from .user_data import AnswersMap, Question, load_answersfile_data\n+from .vcs import get_git\n+\n+_T = TypeVar(\"_T\")\n+\n+\n+@dataclass(config=ConfigDict(extra=\"forbid\"))\n+class Worker:\n+    \"\"\"Copier process state manager.\n+\n+    This class represents the state of a copier work, and contains methods to\n+    actually produce the desired work.\n+\n+    To use it properly, use it as a context manager and fill all dataclass fields.\n+\n+    Then, execute one of its main methods, which are prefixed with `run_`:\n+\n+    -   [run_copy][copier.main.Worker.run_copy] to copy a subproject.\n+    -   [run_recopy][copier.main.Worker.run_recopy] to recopy a subproject.\n+    -   [run_update][copier.main.Worker.run_update] to update a subproject.\n+\n+    Example:\n+        ```python\n+        with Worker(\n+            src_path=\"https://github.com/copier-org/autopretty.git\", \"output\"\n+        ) as worker:\n+            worker.run_copy()\n+        ```\n+\n+    Attributes:\n+        src_path:\n+            String that can be resolved to a template path, be it local or remote.\n+\n+            See [copier.vcs.get_repo][].\n+\n+            If it is `None`, then it means that you are\n+            [updating a project][updating-a-project], and the original\n+            `src_path` will be obtained from\n+            [the answers file][the-copier-answersyml-file].\n+\n+        dst_path:\n+            Destination path where to render the subproject.\n+\n+        answers_file:\n+            Indicates the path for [the answers file][the-copier-answersyml-file].\n+\n+            The path must be relative to `dst_path`.\n+\n+            If it is `None`, the default value will be obtained from\n+            [copier.template.Template.answers_relpath][].\n+\n+        vcs_ref:\n+            Specify the VCS tag/commit to use in the template.\n+\n+        data:\n+            Answers to the questionnaire defined in the template.\n+\n+        exclude:\n+            User-chosen additional [file exclusion patterns][exclude].\n+\n+        use_prereleases:\n+            Consider prereleases when detecting the *latest* one?\n+\n+            See [use_prereleases][].\n+\n+            Useless if specifying a [vcs_ref][].\n+\n+        skip_if_exists:\n+            User-chosen additional [file skip patterns][skip_if_exists].\n+\n+        cleanup_on_error:\n+            Delete `dst_path` if there's an error?\n+\n+            See [cleanup_on_error][].\n+\n+        defaults:\n+            When `True`, use default answers to questions, which might be null if not specified.\n+\n+            See [defaults][].\n+\n+        user_defaults:\n+            Specify user defaults that may override a template's defaults during question prompts.\n+\n+        overwrite:\n+            When `True`, Overwrite files that already exist, without asking.\n+\n+            See [overwrite][].\n+\n+        pretend:\n+            When `True`, produce no real rendering.\n+\n+            See [pretend][].\n+\n+        quiet:\n+            When `True`, disable all output.\n+\n+            See [quiet][].\n+\n+        conflict:\n+            One of \"inline\" (default), \"rej\".\n+\n+        context_lines:\n+            Lines of context to consider when solving conflicts in updates.\n+\n+            With more lines, context resolution is more accurate, but it will\n+            also produce more conflicts if your subproject has evolved.\n+\n+            With less lines, context resolution is less accurate, but it will\n+            respect better the evolution of your subproject.\n+\n+        unsafe:\n+            When `True`, allow usage of unsafe templates.\n+\n+            See [unsafe][]\n+\n+        skip_answered:\n+            When `True`, skip questions that have already been answered.\n+\n+        skip_tasks:\n+            When `True`, skip template tasks execution.\n+    \"\"\"\n+\n+    src_path: str | None = None\n+    dst_path: Path = Path()\n+    answers_file: RelativePath | None = None\n+    vcs_ref: str | None = None\n+    data: AnyByStrDict = field(default_factory=dict)\n+    settings: Settings = field(default_factory=Settings.from_file)\n+    exclude: Sequence[str] = ()\n+    use_prereleases: bool = False\n+    skip_if_exists: Sequence[str] = ()\n+    cleanup_on_error: bool = True\n+    defaults: bool = False\n+    user_defaults: AnyByStrDict = field(default_factory=dict)\n+    overwrite: bool = False\n+    pretend: bool = False\n+    quiet: bool = False\n+    conflict: Literal[\"inline\", \"rej\"] = \"inline\"\n+    context_lines: PositiveInt = 3\n+    unsafe: bool = False\n+    skip_answered: bool = False\n+    skip_tasks: bool = False\n+\n+    answers: AnswersMap = field(default_factory=AnswersMap, init=False)\n+    _cleanup_hooks: list[Callable[[], None]] = field(default_factory=list, init=False)\n+\n+    def __enter__(self) -> Worker:\n+        \"\"\"Allow using worker as a context manager.\"\"\"\n+        return self\n+\n+    @overload\n+    def __exit__(self, type: None, value: None, traceback: None) -> None: ...\n+\n+    @overload\n+    def __exit__(\n+        self, type: type[BaseException], value: BaseException, traceback: TracebackType\n+    ) -> None: ...\n+\n+    def __exit__(\n+        self,\n+        type: type[BaseException] | None,\n+        value: BaseException | None,\n+        traceback: TracebackType | None,\n+    ) -> None:\n+        \"\"\"Clean up garbage files after worker usage ends.\"\"\"\n+        if value is not None:\n+            # exception was raised from code inside context manager:\n+            # try to clean up, ignoring any exception, then re-raise\n+            with suppress(Exception):\n+                self._cleanup()\n+            raise value\n+        # otherwise clean up and let any exception bubble up\n+        self._cleanup()\n+\n+    def _cleanup(self) -> None:\n+        \"\"\"Execute all stored cleanup methods.\"\"\"\n+        for method in self._cleanup_hooks:\n+            method()\n+\n+    def _check_unsafe(self, mode: Literal[\"copy\", \"update\"]) -> None:\n+        \"\"\"Check whether a template uses unsafe features.\"\"\"\n+        if self.unsafe or self.settings.is_trusted(self.template.url):\n+            return\n+        features: set[str] = set()\n+        if self.template.jinja_extensions:\n+            features.add(\"jinja_extensions\")\n+        if self.template.tasks and not self.skip_tasks:\n+            features.add(\"tasks\")\n+        if mode == \"update\" and self.subproject.template:\n+            if self.subproject.template.jinja_extensions:\n+                features.add(\"jinja_extensions\")\n+            if self.subproject.template.tasks:\n+                features.add(\"tasks\")\n+            for stage in get_args(Literal[\"before\", \"after\"]):\n+                if self.template.migration_tasks(stage, self.subproject.template):\n+                    features.add(\"migrations\")\n+                    break\n+        if features:\n+            raise UnsafeTemplateError(sorted(features))\n+\n+    def _external_data(self) -> LazyDict:\n+        \"\"\"Load external data lazily.\n+\n+        Result keys are used for rendering, and values are the parsed contents\n+        of the YAML files specified in [external_data][].\n+\n+        Files will only be parsed lazily on 1st access. This helps avoiding\n+        circular dependencies when the file name also comes from a variable.\n+        \"\"\"\n+\n+        def _render(path: str) -> str:\n+            with Phase.use(Phase.UNDEFINED):\n+                return self._render_string(path)\n+\n+        # Given those values are lazily rendered on 1st access then cached\n+        # the phase value is irrelevant and could be misleading.\n+        # As a consequence it is explicitely set to \"undefined\".\n+        return LazyDict(\n+            **{\n+                name: lambda path=path: load_answersfile_data(\n+                    self.dst_path, _render(path), warn_on_missing=True\n+                )\n+                for name, path in self.template.external_data.items()\n+            }\n+        )\n+\n+    def _print_message(self, message: str) -> None:\n+        if message and not self.quiet:\n+            print(self._render_string(message), file=sys.stderr)\n+\n+    def _answers_to_remember(self) -> Mapping[str, Any]:\n+        \"\"\"Get only answers that will be remembered in the copier answers file.\"\"\"\n+        # All internal values must appear first\n+        answers: AnyByStrDict = {}\n+        commit = self.template.commit\n+        src = self.template.url\n+        for key, value in ((\"_commit\", commit), (\"_src_path\", src)):\n+            if value is not None:\n+                answers[key] = value\n+        # Other data goes next\n+        answers.update(\n+            (str(k), v)\n+            for (k, v) in self.answers.combined.items()\n+            if not k.startswith(\"_\")\n+            and k not in self.answers.hidden\n+            and k not in self.template.secret_questions\n+            and k in self.template.questions_data\n+            and isinstance(k, JSONSerializable)\n+            and isinstance(v, JSONSerializable)\n+        )\n+        return answers\n+\n+    def _execute_tasks(self, tasks: Sequence[Task]) -> None:\n+        \"\"\"Run the given tasks.\n+\n+        Arguments:\n+            tasks: The list of tasks to run.\n+        \"\"\"\n+        for i, task in enumerate(tasks):\n+            extra_context = {f\"_{k}\": v for k, v in task.extra_vars.items()}\n+\n+            if not cast_to_bool(self._render_value(task.condition, extra_context)):\n+                continue\n+\n+            task_cmd = task.cmd\n+            if isinstance(task_cmd, str):\n+                task_cmd = self._render_string(task_cmd, extra_context)\n+                use_shell = True\n+            else:\n+                task_cmd = [\n+                    self._render_string(str(part), extra_context) for part in task_cmd\n+                ]\n+                use_shell = False\n+\n+            if not self.quiet:\n+                print(\n+                    colors.info\n+                    | f\" > Running task {i + 1} of {len(tasks)}: {task_cmd}\",\n+                    file=sys.stderr,\n+                )\n+            if self.pretend:\n+                continue\n+\n+            working_directory = (\n+                # We can't use _render_path here, as that function has special handling for files in the template\n+                self.subproject.local_abspath\n+                / Path(self._render_string(str(task.working_directory), extra_context))\n+            ).absolute()\n+\n+            extra_env = {k.upper(): str(v) for k, v in task.extra_vars.items()}\n+            with local.cwd(working_directory), local.env(**extra_env):\n+                subprocess.run(task_cmd, shell=use_shell, check=True, env=local.env)\n+\n+    def _render_context(self) -> AnyByStrMutableMapping:\n+        \"\"\"Produce render context for Jinja.\"\"\"\n+        # Backwards compatibility\n+        # FIXME Remove it?\n+        conf = asdict(self)\n+        conf.pop(\"_cleanup_hooks\")\n+        conf.pop(\"answers\")\n+        conf.update(\n+            {\n+                \"answers_file\": self.answers_relpath,\n+                \"src_path\": self.template.local_abspath,\n+                \"vcs_ref_hash\": self.template.commit_hash,\n+                \"sep\": os.sep,\n+                \"os\": OS,\n+            }\n+        )\n+        return dict(\n+            **self.answers.combined,\n+            _copier_answers=self._answers_to_remember(),\n+            _copier_conf=conf,\n+            _folder_name=self.subproject.local_abspath.name,\n+            _copier_python=sys.executable,\n+            _copier_phase=Phase.current(),\n+        )\n+\n+    def _path_matcher(self, patterns: Iterable[str]) -> Callable[[Path], bool]:\n+        \"\"\"Produce a function that matches against specified patterns.\"\"\"\n+        # TODO Is normalization really needed?\n+        normalized_patterns = (normalize(\"NFD\", pattern) for pattern in patterns)\n+        spec = PathSpec.from_lines(\"gitwildmatch\", normalized_patterns)\n+        return spec.match_file\n+\n+    def _solve_render_conflict(self, dst_relpath: Path) -> bool:\n+        \"\"\"Properly solve render conflicts.\n+\n+        It can ask the user if running in interactive mode.\n+        \"\"\"\n+        assert not dst_relpath.is_absolute()\n+        printf(\n+            \"conflict\",\n+            dst_relpath,\n+            style=Style.DANGER,\n+            quiet=self.quiet,\n+            file_=sys.stderr,\n+        )\n+        if self.match_skip(dst_relpath):\n+            printf(\n+                \"skip\",\n+                dst_relpath,\n+                style=Style.OK,\n+                quiet=self.quiet,\n+                file_=sys.stderr,\n+            )\n+            return False\n+        if self.overwrite or dst_relpath == self.answers_relpath:\n+            printf(\n+                \"overwrite\",\n+                dst_relpath,\n+                style=Style.WARNING,\n+                quiet=self.quiet,\n+                file_=sys.stderr,\n+            )\n+            return True\n+        return bool(ask(f\" Overwrite {dst_relpath}?\", default=True))\n+\n+    def _render_allowed(\n+        self,\n+        dst_relpath: Path,\n+        is_dir: bool = False,\n+        is_symlink: bool = False,\n+        expected_contents: bytes | Path = b\"\",\n+    ) -> bool:\n+        \"\"\"Determine if a file or directory can be rendered.\n+\n+        Args:\n+            dst_relpath:\n+                Relative path to destination.\n+            is_dir:\n+                Indicate if the path must be treated as a directory or not.\n+            is_symlink:\n+                Indicate if the path must be treated as a symlink or not.\n+            expected_contents:\n+                Used to compare existing file contents with them. Allows to know if\n+                rendering is needed.\n+        \"\"\"\n+        assert not dst_relpath.is_absolute()\n+        assert not expected_contents or not is_dir, \"Dirs cannot have expected content\"\n+        dst_abspath = Path(self.subproject.local_abspath, dst_relpath)\n+        previous_is_symlink = dst_abspath.is_symlink()\n+        try:\n+            previous_content: bytes | Path\n+            if previous_is_symlink:\n+                previous_content = dst_abspath.readlink()\n+            else:\n+                previous_content = dst_abspath.read_bytes()\n+        except FileNotFoundError:\n+            printf(\n+                \"create\",\n+                dst_relpath,\n+                style=Style.OK,\n+                quiet=self.quiet,\n+                file_=sys.stderr,\n+            )\n+            return True\n+        except PermissionError as error:\n+            # HACK https://bugs.python.org/issue43095\n+            if not (error.errno == 13 and platform.system() == \"Windows\"):\n+                raise\n+        except IsADirectoryError:\n+            assert is_dir\n+        if is_dir or (\n+            previous_content == expected_contents and previous_is_symlink == is_symlink\n+        ):\n+            printf(\n+                \"identical\",\n+                dst_relpath,\n+                style=Style.IGNORE,\n+                quiet=self.quiet,\n+                file_=sys.stderr,\n+            )\n+            return is_dir\n+        return self._solve_render_conflict(dst_relpath)\n+\n+    def _ask(self) -> None:  # noqa: C901\n+        \"\"\"Ask the questions of the questionnaire and record their answers.\"\"\"\n+        self.answers = AnswersMap(\n+            user_defaults=self.user_defaults,\n+            init=self.data,\n+            last=self.subproject.last_answers,\n+            metadata=self.template.metadata,\n+            external=self._external_data(),\n+        )\n+\n+        for var_name, details in self.template.questions_data.items():\n+            question = Question(\n+                answers=self.answers,\n+                jinja_env=self.jinja_env,\n+                settings=self.settings,\n+                var_name=var_name,\n+                **details,\n+            )\n+            # Delete last answer if it cannot be parsed or validated, so a new\n+            # valid answer can be provided.\n+            if var_name in self.answers.last:\n+                try:\n+                    answer = question.parse_answer(self.answers.last[var_name])\n+                except Exception:\n+                    del self.answers.last[var_name]\n+                else:\n+                    if question.validate_answer(answer):\n+                        del self.answers.last[var_name]\n+            # Skip a question when the skip condition is met.\n+            if not question.get_when():\n+                # Omit its answer from the answers file.\n+                self.answers.hide(var_name)\n+                # Skip immediately to the next question when it has no default\n+                # value.\n+                if question.default is MISSING:\n+                    continue\n+            if var_name in self.answers.init:\n+                # Try to parse the answer value.\n+                answer = question.parse_answer(self.answers.init[var_name])\n+                # Try to validate the answer value if the question has a\n+                # validator.\n+                if err_msg := question.validate_answer(answer):\n+                    raise ValueError(\n+                        f\"Validation error for question '{var_name}': {err_msg}\"\n+                    )\n+                # At this point, the answer value is valid. Do not ask the\n+                # question again, but set answer as the user's answer instead.\n+                self.answers.user[var_name] = answer\n+                continue\n+            # Skip a question when the user already answered it.\n+            if self.skip_answered and var_name in self.answers.last:\n+                continue\n+\n+            # Display TUI and ask user interactively only without --defaults\n+            try:\n+                if self.defaults:\n+                    new_answer = question.get_default()\n+                    if new_answer is MISSING:\n+                        raise ValueError(f'Question \"{var_name}\" is required')\n+                else:\n+                    new_answer = unsafe_prompt(\n+                        [question.get_questionary_structure()],\n+                        answers={question.var_name: question.get_default()},\n+                    )[question.var_name]\n+            except KeyboardInterrupt as err:\n+                raise CopierAnswersInterrupt(\n+                    self.answers, question, self.template\n+                ) from err\n+            self.answers.user[var_name] = new_answer\n+\n+        # Reload external data, which may depend on answers\n+        self.answers.external = self._external_data()\n+\n+    @property\n+    def answers_relpath(self) -> Path:\n+        \"\"\"Obtain the proper relative path for the answers file.\n+\n+        It comes from:\n+\n+        1. User choice.\n+        2. Template default.\n+        3. Copier default.\n+        \"\"\"\n+        path = self.answers_file or self.template.answers_relpath\n+        template = self.jinja_env.from_string(str(path))\n+        return Path(\n+            template.render(_copier_phase=Phase.current(), **self.answers.combined)\n+        )\n+\n+    @cached_property\n+    def all_exclusions(self) -> Sequence[str]:\n+        \"\"\"Combine default, template and user-chosen exclusions.\"\"\"\n+        return self.template.exclude + tuple(self.exclude)\n+\n+    @cached_property\n+    def jinja_env(self) -> YieldEnvironment:\n+        \"\"\"Return a pre-configured Jinja environment.\n+\n+        Respects template settings.\n+        \"\"\"\n+        paths = [str(self.template.local_abspath)]\n+        loader = FileSystemLoader(paths)\n+        default_extensions = [\n+            \"jinja2_ansible_filters.AnsibleCoreFiltersExtension\",\n+            YieldExtension,\n+        ]\n+        extensions = default_extensions + list(self.template.jinja_extensions)\n+        try:\n+            env = YieldEnvironment(\n+                loader=loader, extensions=extensions, **self.template.envops\n+            )\n+        except ModuleNotFoundError as error:\n+            raise ExtensionNotFoundError(\n+                f\"Copier could not load some Jinja extensions:\\n{error}\\n\"\n+                \"Make sure to install these extensions alongside Copier itself.\\n\"\n+                \"See the docs at https://copier.readthedocs.io/en/latest/configuring/#jinja_extensions\"\n+            )\n+        # patch the `to_json` filter to support Pydantic dataclasses\n+        env.filters[\"to_json\"] = partial(\n+            env.filters[\"to_json\"], default=to_jsonable_python\n+        )\n+\n+        # Add a global function to join filesystem paths.\n+        separators = {\n+            \"posix\": \"/\",\n+            \"windows\": \"\\\\\",\n+            \"native\": os.path.sep,\n+        }\n+\n+        def _pathjoin(\n+            *path: str, mode: Literal[\"posix\", \"windows\", \"native\"] = \"posix\"\n+        ) -> str:\n+            return separators[mode].join(path)\n+\n+        env.globals[\"pathjoin\"] = _pathjoin\n+        return env\n+\n+    @cached_property\n+    def match_exclude(self) -> Callable[[Path], bool]:\n+        \"\"\"Get a callable to match paths against all exclusions.\"\"\"\n+        return self._path_matcher(self.all_exclusions)\n+\n+    @cached_property\n+    def match_skip(self) -> Callable[[Path], bool]:\n+        \"\"\"Get a callable to match paths against all skip-if-exists patterns.\"\"\"\n+        return self._path_matcher(\n+            map(\n+                self._render_string,\n+                tuple(chain(self.skip_if_exists, self.template.skip_if_exists)),\n+            )\n+        )\n+\n+    def _render_template(self) -> None:\n+        \"\"\"Render the template in the subproject root.\"\"\"\n+        follow_symlinks = not self.template.preserve_symlinks\n+        for src in scantree(str(self.template_copy_root), follow_symlinks):\n+            src_abspath = Path(src.path)\n+            src_relpath = Path(src_abspath).relative_to(self.template.local_abspath)\n+            dst_relpaths_ctxs = self._render_path(\n+                Path(src_abspath).relative_to(self.template_copy_root)\n+            )\n+            for dst_relpath, ctx in dst_relpaths_ctxs:\n+                if self.match_exclude(dst_relpath):\n+                    continue\n+                if src.is_symlink() and self.template.preserve_symlinks:\n+                    self._render_symlink(src_relpath, dst_relpath)\n+                elif src.is_dir(follow_symlinks=follow_symlinks):\n+                    self._render_folder(dst_relpath)\n+                else:\n+                    self._render_file(src_relpath, dst_relpath, extra_context=ctx or {})\n+\n+    def _render_file(\n+        self,\n+        src_relpath: Path,\n+        dst_relpath: Path,\n+        extra_context: AnyByStrDict | None = None,\n+    ) -> None:\n+        \"\"\"Render one file.\n+\n+        Args:\n+            src_relpath:\n+                File to be rendered. It must be a path relative to the template\n+                root.\n+            dst_relpath:\n+                File to be created. It must be a path relative to the subproject\n+                root.\n+            extra_context:\n+                Additional variables to use for rendering the template.\n+        \"\"\"\n+        # TODO Get from main.render_file()\n+        assert not src_relpath.is_absolute()\n+        assert not dst_relpath.is_absolute()\n+        src_abspath = self.template.local_abspath / src_relpath\n+        if src_relpath.name.endswith(self.template.templates_suffix):\n+            try:\n+                tpl = self.jinja_env.get_template(src_relpath.as_posix())\n+            except UnicodeDecodeError:\n+                if self.template.templates_suffix:\n+                    # suffix is not empty, re-raise\n+                    raise\n+                # suffix is empty, fallback to copy\n+                new_content = src_abspath.read_bytes()\n+            else:\n+                new_content = tpl.render(\n+                    **self._render_context(), **(extra_context or {})\n+                ).encode()\n+                if self.jinja_env.yield_name:\n+                    raise YieldTagInFileError(\n+                        f\"File {src_relpath} contains a yield tag, but it is not allowed.\"\n+                    )\n+        else:\n+            new_content = src_abspath.read_bytes()\n+        dst_abspath = self.subproject.local_abspath / dst_relpath\n+        src_mode = src_abspath.stat().st_mode\n+        if not self._render_allowed(dst_relpath, expected_contents=new_content):\n+            return\n+        if not self.pretend:\n+            dst_abspath.parent.mkdir(parents=True, exist_ok=True)\n+            if dst_abspath.is_symlink():\n+                # Writing to a symlink just writes to its target, so if we want to\n+                # replace a symlink with a file we have to unlink it first\n+                dst_abspath.unlink()\n+            dst_abspath.write_bytes(new_content)\n+            dst_abspath.chmod(src_mode)\n+\n+    def _render_symlink(self, src_relpath: Path, dst_relpath: Path) -> None:\n+        \"\"\"Render one symlink.\n+\n+        Args:\n+            src_relpath:\n+                Symlink to be rendered. It must be a path relative to the\n+                template root.\n+            dst_relpath:\n+                Symlink to be created. It must be a path relative to the\n+                subproject root.\n+        \"\"\"\n+        assert not src_relpath.is_absolute()\n+        assert not dst_relpath.is_absolute()\n+        if dst_relpath is None or self.match_exclude(dst_relpath):\n+            return\n+\n+        src_abspath = self.template.local_abspath / src_relpath\n+        src_target = src_abspath.readlink()\n+        if src_abspath.name.endswith(self.template.templates_suffix):\n+            dst_target = Path(self._render_string(str(src_target)))\n+        else:\n+            dst_target = src_target\n+\n+        if not self._render_allowed(\n+            dst_relpath,\n+            expected_contents=dst_target,\n+            is_symlink=True,\n+        ):\n+            return\n+\n+        if not self.pretend:\n+            dst_abspath = self.subproject.local_abspath / dst_relpath\n+            # symlink_to doesn't overwrite existing files, so delete it first\n+            if dst_abspath.is_symlink() or dst_abspath.exists():\n+                dst_abspath.unlink()\n+            dst_abspath.parent.mkdir(parents=True, exist_ok=True)\n+            dst_abspath.symlink_to(dst_target)\n+            if sys.platform == \"darwin\":\n+                # Only macOS supports permissions on symlinks.\n+                # Other platforms just copy the permission of the target\n+                src_mode = src_abspath.lstat().st_mode\n+                dst_abspath.lchmod(src_mode)\n+\n+    def _render_folder(self, dst_relpath: Path) -> None:\n+        \"\"\"Create one folder (without content).\n+\n+        Args:\n+            dst_relpath:\n+                Folder to be created. It must be a path relative to the\n+                subproject root.\n+        \"\"\"\n+        assert not dst_relpath.is_absolute()\n+        if not self.pretend and self._render_allowed(dst_relpath, is_dir=True):\n+            dst_abspath = self.subproject.local_abspath / dst_relpath\n+            dst_abspath.mkdir(parents=True, exist_ok=True)\n+\n+    def _adjust_rendered_part(self, rendered_part: str) -> str:\n+        \"\"\"Adjust the rendered part if necessary.\n+\n+        If `{{ _copier_conf.answers_file }}` becomes the full path,\n+        restore part to be just the end leaf.\n+\n+        Args:\n+            rendered_part:\n+                The rendered part of the path to adjust.\n+\n+        \"\"\"\n+        if str(self.answers_relpath) == rendered_part:\n+            return Path(rendered_part).name\n+        return rendered_part\n+\n+    def _render_parts(\n+        self,\n+        parts: tuple[str, ...],\n+        rendered_parts: tuple[str, ...] | None = None,\n+        extra_context: AnyByStrDict | None = None,\n+        is_template: bool = False,\n+    ) -> Iterable[tuple[Path, AnyByStrDict | None]]:\n+        \"\"\"Render a set of parts into path and context pairs.\n+\n+        If a yield tag is found in a part, it will recursively yield multiple path and context pairs.\n+        \"\"\"\n+        if rendered_parts is None:\n+            rendered_parts = tuple()\n+\n+        if not parts:\n+            rendered_path = Path(*rendered_parts)\n+\n+            templated_sibling = (\n+                self.template.local_abspath\n+                / f\"{rendered_path}{self.template.templates_suffix}\"\n+            )\n+            if is_template or not templated_sibling.exists():\n+                yield rendered_path, extra_context\n+\n+            return\n+\n+        part = parts[0]\n+        parts = parts[1:]\n+\n+        if not extra_context:\n+            extra_context = {}\n+\n+        # If the `part` has a yield tag, `self.jinja_env` will be set with the yield name and iterable\n+        rendered_part = self._render_string(part, extra_context=extra_context)\n+\n+        yield_name = self.jinja_env.yield_name\n+        if yield_name:\n+            for value in self.jinja_env.yield_iterable or ():\n+                new_context = {**extra_context, yield_name: value}\n+                rendered_part = self._render_string(part, extra_context=new_context)\n+                rendered_part = self._adjust_rendered_part(rendered_part)\n+\n+                # Skip if any part is rendered as an empty string\n+                if not rendered_part:\n+                    continue\n+\n+                yield from self._render_parts(\n+                    parts, rendered_parts + (rendered_part,), new_context, is_template\n+                )\n+\n+            return\n+\n+        # Skip if any part is rendered as an empty string\n+        if not rendered_part:\n+            return\n+\n+        rendered_part = self._adjust_rendered_part(rendered_part)\n+\n+        yield from self._render_parts(\n+            parts, rendered_parts + (rendered_part,), extra_context, is_template\n+        )\n+\n+    def _render_path(self, relpath: Path) -> Iterable[tuple[Path, AnyByStrDict | None]]:\n+        \"\"\"Render one relative path into multiple path and context pairs.\n+\n+        Args:\n+            relpath:\n+                The relative path to be rendered. Obviously, it can be templated.\n+        \"\"\"\n+        is_template = relpath.name.endswith(self.template.templates_suffix)\n+        templated_sibling = (\n+            self.template_copy_root / f\"{relpath}{self.template.templates_suffix}\"\n+        )\n+        # With an empty suffix, the templated sibling always exists.\n+        if templated_sibling.exists() and self.template.templates_suffix:\n+            return\n+        if self.template.templates_suffix and is_template:\n+            relpath = relpath.with_suffix(\"\")\n+\n+        yield from self._render_parts(relpath.parts, is_template=is_template)\n+\n+    def _render_string(\n+        self, string: str, extra_context: AnyByStrDict | None = None\n+    ) -> str:\n+        \"\"\"Render one templated string.\n+\n+        Args:\n+            string:\n+                The template source string.\n+\n+            extra_context:\n+                Additional variables to use for rendering the template.\n+        \"\"\"\n+        tpl = self.jinja_env.from_string(string)\n+        return tpl.render(**self._render_context(), **(extra_context or {}))\n+\n+    def _render_value(\n+        self, value: _T, extra_context: AnyByStrDict | None = None\n+    ) -> str | _T:\n+        \"\"\"Render a value, which may or may not be a templated string.\n+\n+        Args:\n+            value:\n+                The value to render.\n+\n+            extra_context:\n+                Additional variables to use for rendering the template.\n+        \"\"\"\n+        try:\n+            return self._render_string(value, extra_context=extra_context)  # type: ignore[arg-type]\n+        except TypeError:\n+            return value\n+\n+    @cached_property\n+    def subproject(self) -> Subproject:\n+        \"\"\"Get related subproject.\"\"\"\n+        result = Subproject(\n+            local_abspath=self.dst_path.absolute(),\n+            answers_relpath=self.answers_file or Path(\".copier-answers.yml\"),\n+        )\n+        self._cleanup_hooks.append(result._cleanup)\n+        return result\n+\n+    @cached_property\n+    def template(self) -> Template:\n+        \"\"\"Get related template.\"\"\"\n+        url = self.src_path\n+        if not url:\n+            if self.subproject.template is None:\n+                raise TypeError(\"Template not found\")\n+            url = str(self.subproject.template.url)\n+        result = Template(\n+            url=url, ref=self.vcs_ref, use_prereleases=self.use_prereleases\n+        )\n+        self._cleanup_hooks.append(result._cleanup)\n+        return result\n+\n+    @cached_property\n+    def template_copy_root(self) -> Path:\n+        \"\"\"Absolute path from where to start copying.\n+\n+        It points to the cloned template local abspath + the rendered subdir, if any.\n+        \"\"\"\n+        subdir = self._render_string(self.template.subdirectory) or \"\"\n+        return self.template.local_abspath / subdir\n+\n+    # Main operations\n+    def run_copy(self) -> None:\n+        \"\"\"Generate a subproject from zero, ignoring what was in the folder.\n+\n+        If `dst_path` was missing, it will be\n+        created. Otherwise, `src_path` be rendered\n+        directly into it, without worrying about evolving what was there\n+        already.\n+\n+        See [generating a project][generating-a-project].\n+        \"\"\"\n+        self._check_unsafe(\"copy\")\n+        self._print_message(self.template.message_before_copy)\n+        with Phase.use(Phase.PROMPT):\n+            self._ask()\n+        was_existing = self.subproject.local_abspath.exists()\n+        try:\n+            if not self.quiet:\n+                # TODO Unify printing tools\n+                print(\n+                    f\"\\nCopying from template version {self.template.version}\",\n+                    file=sys.stderr,\n+                )\n+            with Phase.use(Phase.RENDER):\n+                self._render_template()\n+            if not self.quiet:\n+                # TODO Unify printing tools\n+                print(\"\")  # padding space\n+            if not self.skip_tasks:\n+                with Phase.use(Phase.TASKS):\n+                    self._execute_tasks(self.template.tasks)\n+        except Exception:\n+            if not was_existing and self.cleanup_on_error:\n+                rmtree(self.subproject.local_abspath)\n+            raise\n+        self._print_message(self.template.message_after_copy)\n+        if not self.quiet:\n+            # TODO Unify printing tools\n+            print(\"\")  # padding space\n+\n+    def run_recopy(self) -> None:\n+        \"\"\"Update a subproject, keeping answers but discarding evolution.\"\"\"\n+        if self.subproject.template is None:\n+            raise UserMessageError(\n+                \"Cannot recopy because cannot obtain old template references \"\n+                f\"from `{self.subproject.answers_relpath}`.\"\n+            )\n+        with replace(self, src_path=self.subproject.template.url) as new_worker:\n+            new_worker.run_copy()\n+\n+    def run_update(self) -> None:\n+        \"\"\"Update a subproject that was already generated.\n+\n+        See [updating a project][updating-a-project].\n+        \"\"\"\n+        self._check_unsafe(\"update\")\n+        # Check all you need is there\n+        if self.subproject.vcs != \"git\":\n+            raise UserMessageError(\n+                \"Updating is only supported in git-tracked subprojects.\"\n+            )\n+        if self.subproject.is_dirty():\n+            raise UserMessageError(\n+                \"Destination repository is dirty; cannot continue. \"\n+                \"Please commit or stash your local changes and retry.\"\n+            )\n+        if self.subproject.template is None or self.subproject.template.ref is None:\n+            raise UserMessageError(\n+                \"Cannot update because cannot obtain old template references \"\n+                f\"from `{self.subproject.answers_relpath}`.\"\n+            )\n+        if self.template.commit is None:\n+            raise UserMessageError(\n+                \"Updating is only supported in git-tracked templates.\"\n+            )\n+        if not self.subproject.template.version:\n+            raise UserMessageError(\n+                \"Cannot update: version from last update not detected.\"\n+            )\n+        if not self.template.version:\n+            raise UserMessageError(\"Cannot update: version from template not detected.\")\n+        if self.subproject.template.version > self.template.version:\n+            raise UserMessageError(\n+                f\"You are downgrading from {self.subproject.template.version} to {self.template.version}. \"\n+                \"Downgrades are not supported.\"\n+            )\n+        if not self.overwrite:\n+            # Only git-tracked subprojects can be updated, so the user can\n+            # review the diff before committing; so we can safely avoid\n+            # asking for confirmation\n+            raise UserMessageError(\"Enable overwrite to update a subproject.\")\n+        self._print_message(self.template.message_before_update)\n+        if not self.quiet:\n+            # TODO Unify printing tools\n+            print(\n+                f\"Updating to template version {self.template.version}\", file=sys.stderr\n+            )\n+        self._apply_update()\n+        self._print_message(self.template.message_after_update)\n+\n+    def _apply_update(self) -> None:  # noqa: C901\n+        git = get_git()\n+        subproject_top = Path(\n+            git(\n+                \"-C\",\n+                self.subproject.local_abspath,\n+                \"rev-parse\",\n+                \"--show-toplevel\",\n+            ).strip()\n+        )\n+        subproject_subdir = self.subproject.local_abspath.relative_to(subproject_top)\n+\n+        with (\n+            TemporaryDirectory(\n+                prefix=f\"{__name__}.old_copy.\",\n+            ) as old_copy,\n+            TemporaryDirectory(\n+                prefix=f\"{__name__}.new_copy.\",\n+            ) as new_copy,\n+        ):\n+            # Copy old template into a temporary destination\n+            with replace(\n+                self,\n+                dst_path=old_copy / subproject_subdir,\n+                data=self.subproject.last_answers,\n+                defaults=True,\n+                quiet=True,\n+                src_path=self.subproject.template.url,  # type: ignore[union-attr]\n+                vcs_ref=self.subproject.template.commit,  # type: ignore[union-attr]\n+            ) as old_worker:\n+                old_worker.run_copy()\n+            # Run pre-migration tasks\n+            with Phase.use(Phase.MIGRATE):\n+                self._execute_tasks(\n+                    self.template.migration_tasks(\"before\", self.subproject.template)  # type: ignore[arg-type]\n+                )\n+            # Create a Git tree object from the current (possibly dirty) index\n+            # and keep the object reference.\n+            with local.cwd(subproject_top):\n+                subproject_head = git(\"write-tree\").strip()\n+            with local.cwd(old_copy):\n+                self._git_initialize_repo()\n+                # Configure borrowing Git objects from the real destination.\n+                set_git_alternates(subproject_top)\n+                # Save a list of files that were intentionally removed in the generated\n+                # project to avoid recreating them during the update.\n+                # Files listed in `skip_if_exists` should only be skipped if they exist.\n+                # They should even be recreated if deleted intentionally.\n+                files_removed = git(\n+                    \"diff-tree\",\n+                    \"-r\",\n+                    \"--diff-filter=D\",\n+                    \"--name-only\",\n+                    \"HEAD\",\n+                    subproject_head,\n+                ).splitlines()\n+                exclude_plus_removed = list(\n+                    set(self.exclude).union(\n+                        map(\n+                            escape_git_path,\n+                            map(\n+                                normalize_git_path,\n+                                (\n+                                    path\n+                                    for path in files_removed\n+                                    if not self.match_skip(path)\n+                                ),\n+                            ),\n+                        )\n+                    )\n+                )\n+            # Clear last answers cache to load possible answers migration, if skip_answered flag is not set\n+            if self.skip_answered is False:\n+                self.answers = AnswersMap(external=self._external_data())\n+                with suppress(AttributeError):\n+                    del self.subproject.last_answers\n+            # Do a normal update in final destination\n+            with replace(\n+                self,\n+                # Don't regenerate intentionally deleted paths\n+                exclude=exclude_plus_removed,\n+                # Files can change due to the historical diff, and those\n+                # changes are not detected in this process, so it's better to\n+                # say nothing than lie.\n+                # TODO\n+                quiet=True,\n+            ) as current_worker:\n+                current_worker.run_copy()\n+                self.answers = current_worker.answers\n+                self.answers.external = self._external_data()\n+            # Render with the same answers in an empty dir to avoid pollution\n+            with replace(\n+                self,\n+                dst_path=new_copy / subproject_subdir,\n+                data={\n+                    k: v\n+                    for k, v in self.answers.combined.items()\n+                    if k not in self.answers.hidden\n+                },\n+                defaults=True,\n+                quiet=True,\n+                src_path=self.subproject.template.url,  # type: ignore[union-attr]\n+                exclude=exclude_plus_removed,\n+            ) as new_worker:\n+                new_worker.run_copy()\n+            with local.cwd(new_copy):\n+                self._git_initialize_repo()\n+                new_copy_head = git(\"rev-parse\", \"HEAD\").strip()\n+            # Extract diff between temporary destination and real destination\n+            # with some special handling of newly added files in both the project\n+            # and the template.\n+            with local.cwd(old_copy):\n+                # Configure borrowing Git objects from the real destination and\n+                # temporary destination of the new template.\n+                set_git_alternates(subproject_top, Path(new_copy))\n+                # Create an empty file in the temporary destination when the\n+                # same file was added in *both* the project and the temporary\n+                # destination of the new template. With this minor change, the\n+                # diff between the temporary destination and the real\n+                # destination for such files will use the \"update file mode\"\n+                # instead of the \"new file mode\" which avoids deleting the file\n+                # content previously added in the project.\n+                diff_added_cmd = git[\n+                    \"diff-tree\", \"-r\", \"--diff-filter=A\", \"--name-only\"\n+                ]\n+                for filename in (\n+                    set(diff_added_cmd(\"HEAD\", subproject_head).splitlines())\n+                ) & set(diff_added_cmd(\"HEAD\", new_copy_head).splitlines()):\n+                    f = Path(filename)\n+                    f.parent.mkdir(parents=True, exist_ok=True)\n+                    f.touch((subproject_top / filename).stat().st_mode)\n+                    git(\"add\", \"--force\", filename)\n+                self._git_commit(\"add new empty files\")\n+                # Extract diff between temporary destination and real\n+                # destination\n+                diff_cmd = git[\n+                    \"diff-tree\",\n+                    f\"--unified={self.context_lines}\",\n+                    \"HEAD\",\n+                    subproject_head,\n+                ]\n+                try:\n+                    diff = diff_cmd(\"--inter-hunk-context=-1\")\n+                except ProcessExecutionError:\n+                    print(\n+                        colors.warn\n+                        | \"Make sure Git >= 2.24 is installed to improve updates.\",\n+                        file=sys.stderr,\n+                    )\n+                    diff = diff_cmd(\"--inter-hunk-context=0\")\n+            compared = dircmp(old_copy, new_copy)\n+            # Try to apply cached diff into final destination\n+            with local.cwd(subproject_top):\n+                apply_cmd = git[\"apply\", \"--reject\", \"--exclude\", self.answers_relpath]\n+                ignored_files = git[\"status\", \"--ignored\", \"--porcelain\"]()\n+                # returns \"!! file1\\n !! file2\\n\"\n+                # extra_exclude will contain: [\"file1\", file2\"]\n+                extra_exclude = [\n+                    filename.split(\"!! \").pop()\n+                    for filename in ignored_files.splitlines()\n+                ]\n+                for skip_pattern in chain(\n+                    self.skip_if_exists, self.template.skip_if_exists, extra_exclude\n+                ):\n+                    apply_cmd = apply_cmd[\"--exclude\", skip_pattern]\n+                (apply_cmd << diff)(retcode=None)\n+                if self.conflict == \"inline\":\n+                    conflicted = []\n+                    old_path = Path(old_copy)\n+                    new_path = Path(new_copy)\n+                    status = git(\"status\", \"--porcelain\").strip().splitlines()\n+                    for line in status:\n+                        # Filter merge rejections (part 1/2)\n+                        if not line.startswith(\"?? \"):\n+                            continue\n+                        # Remove \"?? \" prefix\n+                        fname = line[3:]\n+                        # Normalize name\n+                        fname = normalize_git_path(fname)\n+                        # Filter merge rejections (part 2/2)\n+                        if not fname.endswith(\".rej\"):\n+                            continue\n+                        # Remove \".rej\" suffix\n+                        fname = fname[:-4]\n+                        # Undo possible non-rejected chunks\n+                        git(\"checkout\", \"--\", fname)\n+                        # 3-way-merge the file directly\n+                        git(\n+                            \"merge-file\",\n+                            \"-L\",\n+                            \"before updating\",\n+                            \"-L\",\n+                            \"last update\",\n+                            \"-L\",\n+                            \"after updating\",\n+                            fname,\n+                            old_path / fname,\n+                            new_path / fname,\n+                            retcode=None,\n+                        )\n+                        # Remove rejection witness\n+                        Path(f\"{fname}.rej\").unlink()\n+                        # The 3-way merge might have resolved conflicts automatically,\n+                        # so we need to check if the file contains conflict markers\n+                        # before storing the file name for marking it as unmerged after the loop.\n+                        with Path(fname).open(encoding=\"utf-8\", errors=\"replace\") as conflicts_candidate:\n+                            if any(\n+                                line.rstrip()\n+                                in {\"<<<<<<< before updating\", \">>>>>>> after updating\"}\n+                                for line in conflicts_candidate\n+                            ):\n+                                conflicted.append(fname)\n+                    # We ran `git merge-file` outside of a regular merge operation,\n+                    # which means no merge conflict is recorded in the index.\n+                    # Only the usual stage 0 is recorded, with the hash of the current version.\n+                    # We therefore update the index with the missing stages:\n+                    # 1 = current (before updating), 2 = base (last update), 3 = other (after updating).\n+                    # See this SO post: https://stackoverflow.com/questions/79309642/\n+                    # and Git docs: https://git-scm.com/docs/git-update-index#_using_index_info.\n+                    if conflicted:\n+                        input_lines = []\n+                        for line in (\n+                            git(\"ls-files\", \"--stage\", *conflicted).strip().splitlines()\n+                        ):\n+                            perms_sha_mode, path = line.split(\"\\t\")\n+                            perms, sha, _ = perms_sha_mode.split()\n+                            input_lines.append(f\"0 {'0' * 40}\\t{path}\")\n+                            input_lines.append(f\"{perms} {sha} 1\\t{path}\")\n+                            with suppress(ProcessExecutionError):\n+                                # The following command will fail\n+                                # if the file did not exist in the previous version.\n+                                old_sha = git(\n+                                    \"hash-object\",\n+                                    \"-w\",\n+                                    old_path / normalize_git_path(path),\n+                                ).strip()\n+                                input_lines.append(f\"{perms} {old_sha} 2\\t{path}\")\n+                            with suppress(ProcessExecutionError):\n+                                # The following command will fail\n+                                # if the file was deleted in the latest version.\n+                                new_sha = git(\n+                                    \"hash-object\",\n+                                    \"-w\",\n+                                    new_path / normalize_git_path(path),\n+                                ).strip()\n+                                input_lines.append(f\"{perms} {new_sha} 3\\t{path}\")\n+                        (\n+                            git[\"update-index\", \"--index-info\"]\n+                            << \"\\n\".join(input_lines)\n+                        )()\n+            # Trigger recursive removal of deleted files in last template version\n+            _remove_old_files(subproject_top, compared)\n+\n+        # Run post-migration tasks\n+        with Phase.use(Phase.MIGRATE):\n+            self._execute_tasks(\n+                self.template.migration_tasks(\"after\", self.subproject.template)  # type: ignore[arg-type]\n+            )\n+\n+    def _git_initialize_repo(self) -> None:\n+        \"\"\"Initialize a git repository in the current directory.\"\"\"\n+        git = get_git()\n+        git(\"init\", retcode=None)\n+        git(\"add\", \".\")\n+        self._git_commit()\n+\n+    def _git_commit(self, message: str = \"dumb commit\") -> None:\n+        git = get_git()\n+        # 1st commit could fail if any pre-commit hook reformats code\n+        # 2nd commit uses --no-verify to disable pre-commit-like checks\n+        git(\n+            \"commit\",\n+            \"--allow-empty\",\n+            \"-am\",\n+            f\"{message} 1\",\n+            \"--no-gpg-sign\",\n+            retcode=None,\n+        )\n+        git(\n+            \"commit\",\n+            \"--allow-empty\",\n+            \"-am\",\n+            f\"{message} 2\",\n+            \"--no-gpg-sign\",\n+            \"--no-verify\",\n+        )\n+\n+\n+def run_copy(\n+    src_path: str,\n+    dst_path: StrOrPath = \".\",\n+    data: AnyByStrDict | None = None,\n+    **kwargs: Any,\n+) -> Worker:\n+    \"\"\"Copy a template to a destination, from zero.\n+\n+    This is a shortcut for [run_copy][copier.main.Worker.run_copy].\n+\n+    See [Worker][copier.main.Worker] fields to understand this function's args.\n+    \"\"\"\n+    if data is not None:\n+        kwargs[\"data\"] = data\n+    with Worker(src_path=src_path, dst_path=Path(dst_path), **kwargs) as worker:\n+        worker.run_copy()\n+    return worker\n+\n+\n+def run_recopy(\n+    dst_path: StrOrPath = \".\", data: AnyByStrDict | None = None, **kwargs: Any\n+) -> Worker:\n+    \"\"\"Update a subproject from its template, discarding subproject evolution.\n+\n+    This is a shortcut for [run_recopy][copier.main.Worker.run_recopy].\n+\n+    See [Worker][copier.main.Worker] fields to understand this function's args.\n+    \"\"\"\n+    if data is not None:\n+        kwargs[\"data\"] = data\n+    with Worker(dst_path=Path(dst_path), **kwargs) as worker:\n+        worker.run_recopy()\n+    return worker\n+\n+\n+def run_update(\n+    dst_path: StrOrPath = \".\",\n+    data: AnyByStrDict | None = None,\n+    **kwargs: Any,\n+) -> Worker:\n+    \"\"\"Update a subproject, from its template.\n+\n+    This is a shortcut for [run_update][copier.main.Worker.run_update].\n+\n+    See [Worker][copier.main.Worker] fields to understand this function's args.\n+    \"\"\"\n+    if data is not None:\n+        kwargs[\"data\"] = data\n+    with Worker(dst_path=Path(dst_path), **kwargs) as worker:\n+        worker.run_update()\n+    return worker\n+\n+\n+def _remove_old_files(prefix: Path, cmp: dircmp[str], rm_common: bool = False) -> None:\n+    \"\"\"Remove files and directories only found in \"old\" template.\n+\n+    This is an internal helper method used to process a comparison of 2\n+    directories, where the left one is considered the \"old\" one, and the\n+    right one is the \"new\" one.\n+\n+    Then, it will recursively try to remove anything that is only in the old\n+    directory.\n+\n+    Args:\n+        prefix:\n+            Where we start removing. It can be different from the directories\n+            being compared.\n+        cmp:\n+            The comparison result.\n+        rm_common:\n+            Should we remove common files and directories?\n+    \"\"\"\n+    # Gather files and dirs to remove\n+    to_rm = []\n+    subdirs = {}\n+    with suppress(NotADirectoryError, FileNotFoundError):\n+        to_rm = cmp.left_only\n+        if rm_common:\n+            to_rm += cmp.common_files + cmp.common_dirs\n+        subdirs = cmp.subdirs\n+    # Remove files found only in old template copy\n+    for name in to_rm:\n+        target = prefix / name\n+        if target.is_file():\n+            target.unlink()\n+        else:\n+            # Recurse in dirs totally removed in latest template\n+            _remove_old_files(target, dircmp(Path(cmp.left, name), target), True)\n+            # Remove subdir if it ends empty\n+            with suppress(OSError):\n+                target.rmdir()  # Raises if dir not empty\n+    # Recurse\n+    for key, value in subdirs.items():\n+        subdir = prefix / key\n+        _remove_old_files(subdir, value)\n+        # Remove subdir if it ends empty\n+        with suppress(OSError):\n+            subdir.rmdir()  # Raises if dir not empty\n"
 },
 "crc-org__crc-4576": {
  "model_patch": "diff --git a/cmd/crc/cmd/console.go b/cmd/crc/cmd/console.go\nindex bdd5c6a38..b315d0449 100644\n--- a/cmd/crc/cmd/console.go\n+++ b/cmd/crc/cmd/console.go\n@@ -64,6 +64,9 @@ type consoleResult struct {\n }\n \n func (s *consoleResult) prettyPrintTo(writer io.Writer) error {\n+\tif s.ClusterConfig != nil && s.ClusterConfig.ClusterType == preset.Microshift {\n+\t\treturn errors.New(\"the 'crc console' command is not supported for the MicroShift preset\")\n+\t}\n \tif s.Error != nil {\n \t\treturn s.Error\n \t}\n"
 },
 "elastic__beats-42172": {
  "model_patch": "diff --git a/.buildkite/x-pack/pipeline.xpack.packetbeat.yml b/.buildkite/x-pack/pipeline.xpack.packetbeat.yml\nindex 5b3625e0fc..0c73c48433 100644\n--- a/.buildkite/x-pack/pipeline.xpack.packetbeat.yml\n+++ b/.buildkite/x-pack/pipeline.xpack.packetbeat.yml\n@@ -207,7 +207,6 @@ steps:\n \n       - label: \":windows: x-pack/packetbeat: Win 2022 System Tests\"\n         key: \"mandatory-win-2022-system-tests\"\n-        skip: \"skipping due to elastic/beats#38142\"\n         command: |\n           source .buildkite/scripts/gcp_auth.sh\n           Set-Location -Path x-pack/packetbeat\n@@ -322,7 +321,6 @@ steps:\n \n       - label: \":windows: x-pack/packetbeat: Win 10 System Tests\"\n         key: \"extended-win-10-system-tests\"\n-        skip: \"skipping due to elastic/beats#38142\"\n         command: |\n           source .buildkite/scripts/gcp_auth.sh\n           Set-Location -Path x-pack/packetbeat\n"
 },
 "elastic__beats-43665": {
  "model_patch": "diff --git a/metricbeat/module/windows/service/service_status.go b/metricbeat/module/windows/service/service_status.go\nindex 571d324cb5..6525b4980e 100644\n--- a/metricbeat/module/windows/service/service_status.go\n+++ b/metricbeat/module/windows/service/service_status.go\n@@ -168,7 +168,8 @@ func GetServiceStates(handle Handle, state ServiceEnumState, protectedServices m\n \n \t\tservice, err := getServiceInformation(serviceTemp, servicesBuffer, handle, protectedServices)\n \t\tif err != nil {\n-\t\t\treturn nil, err\n+\t\t\tlogp.Error(fmt.Errorf(\"error while fetching information for service %s: %w\", service.ServiceName, err))\n+\t\t\tcontinue\n \t\t}\n \n \t\tservices = append(services, service)\n"
 },
 "gemrb__gemrb-2365": {
  "model_patch": "diff --git a/gemrb/core/System/VFS.cpp b/gemrb/core/System/VFS.cpp\nindex b4e524af5..fbb294c9a 100644\n--- a/gemrb/core/System/VFS.cpp\n+++ b/gemrb/core/System/VFS.cpp\n@@ -76,8 +76,10 @@ struct dirent {\n \t{\n \t\tauto cutOff = entryName.length();\n \t\tbuffer = std::move(entryName);\n-\t\tbuffer.resize(_MAX_PATH);\n-\t\tbuffer[cutOff] = 0;\n+\t\tif (buffer.length() < _MAX_PATH) {\n+\t\t\tbuffer.resize(_MAX_PATH);\n+\t\t}\n+\t\tbuffer[std::min(cutOff, (size_t) _MAX_PATH - 1)] = 0;\n \t\td_name = const_cast<char*>(buffer.data());\n \n \t\treturn *this;\n"
 },
 "gleam-lang__gleam-4850": {
  "model_patch": "diff --git a/compiler-core/src/requirement.rs b/compiler-core/src/requirement.rs\nindex 036ae08b6..869f8bc11 100644\n--- a/compiler-core/src/requirement.rs\n+++ b/compiler-core/src/requirement.rs\n@@ -122,7 +122,7 @@ impl<'de> Visitor<'de> for RequirementVisitor {\n     where\n         E: de::Error,\n     {\n-        Ok(FromStr::from_str(value).expect(\"expected string\"))\n+        FromStr::from_str(value).map_err(de::Error::custom)\n     }\n \n     fn visit_map<M>(self, visitor: M) -> Result<Self::Value, M::Error>\n@@ -165,4 +165,13 @@ mod tests {\n             Requirement::git(\"https://github.com/gleam-lang/otp.git\", \"4d34935\")\n         );\n     }\n+\n+    #[test]\n+    fn read_requirement_error() {\n+        let toml = r#\"\n+            runtime = \"node\"\n+        \"#;\n+        let result: Result<HashMap<String, Requirement>, toml::de::Error> = toml::from_str(toml);\n+        assert!(result.is_err());\n+    }\n }\n"
 },
 "gogf__gf-4386": {
  "model_patch": "diff --git a/cmd/gf/go.mod b/cmd/gf/go.mod\nindex 1adb3f4af..8641c16d1 100644\n--- a/cmd/gf/go.mod\n+++ b/cmd/gf/go.mod\n@@ -14,7 +14,7 @@ require (\n \tgithub.com/olekukonko/tablewriter v0.0.5\n \tgithub.com/schollz/progressbar/v3 v3.15.0\n \tgolang.org/x/mod v0.25.0\n-\tgolang.org/x/tools v0.34.0\n+\tgolang.org/x/tools v0.22.0\n )\n \n require (\n@@ -52,12 +52,12 @@ require (\n \tgo.opentelemetry.io/otel/metric v1.32.0 // indirect\n \tgo.opentelemetry.io/otel/sdk v1.32.0 // indirect\n \tgo.opentelemetry.io/otel/trace v1.32.0 // indirect\n-\tgolang.org/x/crypto v0.39.0 // indirect\n-\tgolang.org/x/net v0.41.0 // indirect\n+\tgolang.org/x/crypto v0.38.0 // indirect\n+\tgolang.org/x/net v0.40.0 // indirect\n \tgolang.org/x/sync v0.15.0 // indirect\n \tgolang.org/x/sys v0.33.0 // indirect\n \tgolang.org/x/term v0.32.0 // indirect\n-\tgolang.org/x/text v0.26.0 // indirect\n+\tgolang.org/x/text v0.25.0 // indirect\n \tgopkg.in/yaml.v3 v3.0.1 // indirect\n \tmodernc.org/libc v1.22.5 // indirect\n \tmodernc.org/mathutil v1.5.0 // indirect\ndiff --git a/cmd/gf/go.sum b/cmd/gf/go.sum\nindex d94dce82b..1873559de 100644\n--- a/cmd/gf/go.sum\n+++ b/cmd/gf/go.sum\n@@ -169,6 +169,7 @@ golang.org/x/crypto v0.0.0-20190308221718-c2843e01d9a2/go.mod h1:djNgcEr1/C05ACk\n golang.org/x/crypto v0.0.0-20191011191535-87dc89f01550/go.mod h1:yigFU9vqHzYiE8UmvKecakEJjdnWj3jj499lnFckfCI=\n golang.org/x/crypto v0.0.0-20200622213623-75b288015ac9/go.mod h1:LzIPMQfyMNhhGPhUkYOs5KpL4U8rLKemX1yGLhDgUto=\n golang.org/x/crypto v0.0.0-20210220033148-5ea612d1eb83/go.mod h1:jdWPYTVW3xRLrWPugEBEK3UY2ZEsg3UU495nc5E+M+I=\n+golang.org/x/crypto v0.38.0/go.mod h1:MvrbAqul58NNYPKnOra203SB9vpuZW0e+RRZV+Ggqjw=\n golang.org/x/crypto v0.39.0 h1:SHs+kF4LP+f+p14esP5jAoDpHU8Gu/v9lFRK6IT5imM=\n golang.org/x/crypto v0.39.0/go.mod h1:L+Xg3Wf6HoL4Bn4238Z6ft6KfEpN0tJGo53AAPC632U=\n golang.org/x/mod v0.2.0/go.mod h1:s0Qsj1ACt9ePp/hMypM3fl4fZqREWJwdYDEqhRiZZUA=\n@@ -179,6 +180,7 @@ golang.org/x/net v0.0.0-20190404232315-eb5bcb51f2a3/go.mod h1:t9HGtf8HONx5eT2rtn\n golang.org/x/net v0.0.0-20190620200207-3b0461eec859/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\n golang.org/x/net v0.0.0-20200226121028-0de0cce0169b/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=\n golang.org/x/net v0.0.0-20201021035429-f5854403a974/go.mod h1:sp8m0HH+o8qH0wwXwYZr8TS3Oi6o0r6Gce1SSxlDquU=\n+golang.org/x/net v0.40.0/go.mod h1:y0hY0exeL2Pku80/zKK7tpntoX23cqL3Oa6njdgRtds=\n golang.org/x/net v0.41.0 h1:vBTly1HeNPEn3wtREYfy4GZ/NECgw2Cnl+nK6Nz3uvw=\n golang.org/x/net v0.41.0/go.mod h1:B/K4NNqkfmg07DQYrbwvSluqCJOOXwUjeb/5lOisjbA=\n golang.org/x/sync v0.0.0-20190423024810-112230192c58/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=\n@@ -206,12 +208,15 @@ golang.org/x/term v0.32.0 h1:DR4lr0TjUs3epypdhTOkMmuF5CDFJ/8pOnbzMZPQ7bg=\n golang.org/x/term v0.32.0/go.mod h1:uZG1FhGx848Sqfsq4/DlJr3xGGsYMu/L5GW4abiaEPQ=\n golang.org/x/text v0.3.0/go.mod h1:NqM8EUOU14njkJ3fqMW+pc6Ldnwhi/IjpwHt7yyuwOQ=\n golang.org/x/text v0.3.3/go.mod h1:5Zoc/QRtKVWzQhOtBMvqHzDpF6irO9z98xDceosuGiQ=\n+golang.org/x/text v0.25.0/go.mod h1:WEdwpYrmk1qmdHvhkSTNPm3app7v4rsT8F2UD6+VHIA=\n golang.org/x/text v0.26.0 h1:P42AVeLghgTYr4+xUnTRKDMqpar+PtX7KWuNQL21L8M=\n golang.org/x/text v0.26.0/go.mod h1:QK15LZJUUQVJxhz7wXgxSy/CJaTFjd0G+YLonydOVQA=\n golang.org/x/tools v0.0.0-20180917221912-90fa682c2a6e/go.mod h1:n7NCudcB/nEzxVGmLbDWY5pfWTLqBcC2KZ6jyYvM4mQ=\n golang.org/x/tools v0.0.0-20191119224855-298f0cb1881e/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=\n golang.org/x/tools v0.0.0-20200619180055-7c47624df98f/go.mod h1:EkVYQZoAsY45+roYkvgYkIh4xh/qjgUK9TdY2XT94GE=\n golang.org/x/tools v0.0.0-20210106214847-113979e3529a/go.mod h1:emZCQorbCU4vsT4fOWvOPXz4eW1wZW4PmDk9uLelYpA=\n+golang.org/x/tools v0.22.0 h1:gqSGLZqv+AI9lIQzniJ0nZDRG5GBPsSi+DRNHWNz6yA=\n+golang.org/x/tools v0.22.0/go.mod h1:aCwcsjqvq7Yqt6TNyX7QMU2enbQ/Gt0bo6krSeEri+c=\n golang.org/x/tools v0.34.0 h1:qIpSLOxeCYGg9TrcJokLBG4KFA6d795g0xkBkiESGlo=\n golang.org/x/tools v0.34.0/go.mod h1:pAP9OwEaY1CAW3HOmg3hLZC5Z0CCmzjAF2UQMSqNARg=\n golang.org/x/xerrors v0.0.0-20190717185122-a985d3407aa7/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=\ndiff --git a/cmd/gf/go.work b/cmd/gf/go.work\nindex edf43a26c..6e1475f83 100644\n--- a/cmd/gf/go.work\n+++ b/cmd/gf/go.work\n@@ -1,18 +1,6 @@\n go 1.23.0\n \n-use ./\n-\n-// =====================================================================================================\n-// NOTE:\n-// Please update associated commands in ../../.set_version.sh if any of the follows replacements change.\n-// =====================================================================================================\n-\n-replace (\n-\tgithub.com/gogf/gf/contrib/drivers/clickhouse/v2 => ../../contrib/drivers/clickhouse\n-\tgithub.com/gogf/gf/contrib/drivers/mssql/v2 => ../../contrib/drivers/mssql\n-\tgithub.com/gogf/gf/contrib/drivers/mysql/v2 => ../../contrib/drivers/mysql\n-\tgithub.com/gogf/gf/contrib/drivers/oracle/v2 => ../../contrib/drivers/oracle\n-\tgithub.com/gogf/gf/contrib/drivers/pgsql/v2 => ../../contrib/drivers/pgsql\n-\tgithub.com/gogf/gf/contrib/drivers/sqlite/v2 => ../../contrib/drivers/sqlite\n-\tgithub.com/gogf/gf/v2 => ../../\n+use (\n+\t.\n+\t..\\..\n )\n"
 },
 "goreleaser__goreleaser-5631": {
  "model_patch": "diff --git a/pkg/archive/tar/tar.go b/pkg/archive/tar/tar.go\nindex 9738d5a9..c835d84e 100644\n--- a/pkg/archive/tar/tar.go\n+++ b/pkg/archive/tar/tar.go\n@@ -7,6 +7,7 @@ import (\n \t\"io\"\n \t\"io/fs\"\n \t\"os\"\n+\t\"runtime\"\n \n \t\"github.com/goreleaser/goreleaser/v2/pkg/config\"\n )\n@@ -80,6 +81,8 @@ func (a Archive) Add(f config.File) error {\n \t}\n \tif f.Info.Mode != 0 {\n \t\theader.Mode = int64(f.Info.Mode)\n+\t} else if runtime.GOOS == \"windows\" || info.Mode()&0o111 != 0 {\n+\t\theader.Mode = 0o755\n \t}\n \tif f.Info.Owner != \"\" {\n \t\theader.Uid = 0\ndiff --git a/pkg/archive/zip/zip.go b/pkg/archive/zip/zip.go\nindex 217b1dff..e07edaa3 100644\n--- a/pkg/archive/zip/zip.go\n+++ b/pkg/archive/zip/zip.go\n@@ -10,6 +10,7 @@ import (\n \t\"io/fs\"\n \t\"os\"\n \t\"path/filepath\"\n+\t\"runtime\"\n \n \t\"github.com/goreleaser/goreleaser/v2/pkg/config\"\n )\n@@ -100,6 +101,8 @@ func (a Archive) Add(f config.File) error {\n \t}\n \tif f.Info.Mode != 0 {\n \t\theader.SetMode(f.Info.Mode)\n+\t} else if runtime.GOOS == \"windows\" || info.Mode()&0o111 != 0 {\n+\t\theader.SetMode(0o755)\n \t}\n \tw, err := a.z.CreateHeader(header)\n \tif err != nil {\n"
 },
 "gravitational__teleport-53067": {
  "model_patch": "diff --git a/lib/config/configuration.go b/lib/config/configuration.go\nindex 6aeb06d2f3..14135837dd 100644\n--- a/lib/config/configuration.go\n+++ b/lib/config/configuration.go\n@@ -2074,6 +2074,8 @@ func applyWindowsDesktopConfig(fc *FileConfig, cfg *servicecfg.Config) error {\n \t\tFilters:         fc.WindowsDesktop.Discovery.Filters,\n \t\tLabelAttributes: fc.WindowsDesktop.Discovery.LabelAttributes,\n \t}\n+\tcfg.WindowsDesktop.DiscoveryRDPPort = fc.WindowsDesktop.DiscoveryRDPPort\n+\tcfg.WindowsDesktop.DiscoveryInterval = fc.WindowsDesktop.DiscoveryInterval\n \n \tvar err error\n \tcfg.WindowsDesktop.PublicAddrs, err = utils.AddrsFromStrings(fc.WindowsDesktop.PublicAddr, defaults.WindowsDesktopListenPort)\ndiff --git a/lib/config/fileconf.go b/lib/config/fileconf.go\nindex 44126398a2..45d216a097 100644\n--- a/lib/config/fileconf.go\n+++ b/lib/config/fileconf.go\n@@ -2491,6 +2491,10 @@ type WindowsDesktopService struct {\n \tKDCAddress string `yaml:\"kdc_address\"`\n \t// Discovery configures desktop discovery via LDAP.\n \tDiscovery LDAPDiscoveryConfig `yaml:\"discovery,omitempty\"`\n+\t// DiscoveryRDPPort is the RDP port to use for discovered desktops.\n+\tDiscoveryRDPPort int `yaml:\"discovery_rdp_port,omitempty\"`\n+\t// DiscoveryInterval is the interval at which to search for desktops.\n+\tDiscoveryInterval time.Duration `yaml:\"discovery_interval,omitempty\"`\n \t// ADHosts is a list of static, AD-connected Windows hosts. This gives users\n \t// a way to specify AD-connected hosts that won't be found by the filters\n \t// specified in `discovery` (or if `discovery` is omitted).\ndiff --git a/lib/service/servicecfg/windows.go b/lib/service/servicecfg/windows.go\nindex 988dc16b3e..f16a20746e 100644\n--- a/lib/service/servicecfg/windows.go\n+++ b/lib/service/servicecfg/windows.go\n@@ -22,6 +22,7 @@ import (\n \t\"crypto/x509\"\n \t\"maps\"\n \t\"regexp\"\n+\t\"time\"\n \n \t\"github.com/gravitational/teleport/lib/limiter\"\n \t\"github.com/gravitational/teleport/lib/services\"\n@@ -57,6 +58,12 @@ type WindowsDesktopConfig struct {\n \t// Discovery configures automatic desktop discovery via LDAP.\n \tDiscovery LDAPDiscoveryConfig\n \n+\t// DiscoveryRDPPort is the RDP port to use for discovered desktops.\n+\tDiscoveryRDPPort int\n+\n+\t// DiscoveryInterval is the interval at which to search for desktops.\n+\tDiscoveryInterval time.Duration\n+\n \t// StaticHosts is an optional list of static Windows hosts to expose through this\n \t// service.\n \tStaticHosts []WindowsHost\ndiff --git a/lib/srv/desktop/discovery.go b/lib/srv/desktop/discovery.go\nindex ee50a8de2a..37dac75c43 100644\n--- a/lib/srv/desktop/discovery.go\n+++ b/lib/srv/desktop/discovery.go\n@@ -121,9 +121,12 @@ func (s *WindowsService) startDesktopDiscovery() error {\n \t\t\ts.cfg.Logger.ErrorContext(s.closeCtx, \"desktop reconciliation failed\", \"error\", err)\n \t\t}\n \n-\t\t// TODO(zmb3): consider making the discovery period configurable\n-\t\t// (it's currently hard coded to 5 minutes in order to match DB access discovery behavior)\n-\t\tt := s.cfg.Clock.NewTicker(5 * time.Minute)\n+\t\t// reconciler/discovery interval for Windows Desktops\n+\t\tinterval := 5 * time.Minute\n+\t\tif s.cfg.DiscoveryInterval > 0 {\n+\t\t\tinterval = s.cfg.DiscoveryInterval\n+\t\t}\n+\t\tt := s.cfg.Clock.NewTicker(interval)\n \t\tdefer t.Stop()\n \t\tfor {\n \t\t\tselect {\n@@ -335,7 +338,11 @@ func (s *WindowsService) ldapEntryToWindowsDesktop(\n \t}\n \n \ts.cfg.Logger.DebugContext(ctx, \"resolved desktop host\", \"hostname\", hostname, \"addrs\", addrs)\n-\taddr, err := utils.ParseHostPortAddr(addrs[0], defaults.RDPListenPort)\n+\tport := defaults.RDPListenPort\n+\tif s.cfg.DiscoveryRDPPort > 0 {\n+\t\tport = s.cfg.DiscoveryRDPPort\n+\t}\n+\taddr, err := utils.ParseHostPortAddr(addrs[0], port)\n \tif err != nil {\n \t\treturn nil, trace.Wrap(err)\n \t}\ndiff --git a/lib/srv/desktop/windows_server.go b/lib/srv/desktop/windows_server.go\nindex 767c959510..9c7dba3583 100644\n--- a/lib/srv/desktop/windows_server.go\n+++ b/lib/srv/desktop/windows_server.go\n@@ -212,6 +212,10 @@ type WindowsServiceConfig struct {\n \t// DiscoveryLDAPAttributeLabels are optional LDAP attributes to convert\n \t// into Teleport labels.\n \tDiscoveryLDAPAttributeLabels []string\n+\t// DiscoveryRDPPort is the RDP port to use for discovered desktops.\n+\tDiscoveryRDPPort int\n+\t// DiscoveryInterval is the interval at which to search for desktops.\n+\tDiscoveryInterval time.Duration\n \t// Hostname of the Windows desktop service\n \tHostname string\n \t// ConnectedProxyGetter gets the proxies teleport is connected to.\n"
 },
 "gravitational__teleport-55616": {
  "model_patch": "diff --git a/lib/srv/desktop/discovery.go b/lib/srv/desktop/discovery.go\nindex 3ae2011128..0baa28cb13 100644\n--- a/lib/srv/desktop/discovery.go\n+++ b/lib/srv/desktop/discovery.go\n@@ -399,7 +399,9 @@ func (s *WindowsService) startDynamicReconciler(ctx context.Context) (*services.\n \t\t\treturn services.MatchResourceLabels(s.cfg.ResourceMatchers, desktop.GetAllLabels())\n \t\t},\n \t\tGetCurrentResources: func() map[string]types.WindowsDesktop {\n-\t\t\treturn currentResources\n+\t\t\ts.mu.Lock()\n+\t\t\tdefer s.mu.Unlock()\n+\t\t\treturn s.getResources(ctx, currentResources)\n \t\t},\n \t\tGetNewResources: func() map[string]types.WindowsDesktop {\n \t\t\treturn newResources\n@@ -430,7 +432,9 @@ func (s *WindowsService) startDynamicReconciler(ctx context.Context) (*services.\n \t\t\t\t\ts.cfg.Logger.WarnContext(ctx, \"Reconciliation failed, will retry\", \"error\", err)\n \t\t\t\t\tcontinue\n \t\t\t\t}\n+\t\t\t\ts.mu.Lock()\n \t\t\t\tcurrentResources = newResources\n+\t\t\t\ts.mu.Unlock()\n \t\t\tcase <-watcher.Done():\n \t\t\t\treturn\n \t\t\tcase <-ctx.Done():\n@@ -458,3 +462,20 @@ func (s *WindowsService) toWindowsDesktop(dynamicDesktop types.DynamicWindowsDes\n \t\t},\n \t})\n }\n+\n+func (s *WindowsService) getResources(ctx context.Context, reconciliationCache map[string]types.WindowsDesktop) map[string]types.WindowsDesktop {\n+\tdesktops, err := s.cfg.AccessPoint.GetWindowsDesktops(ctx, types.WindowsDesktopFilter{\n+\t\tHostID: s.cfg.Heartbeat.HostUUID,\n+\t})\n+\tif err != nil {\n+\t\ts.cfg.Logger.WarnContext(ctx, \"Failed to get current windows desktops, falling back to cache\", \"error\", err)\n+\t\treturn reconciliationCache\n+\t}\n+\tres := make(map[string]types.WindowsDesktop)\n+\tfor _, d := range desktops {\n+\t\tif services.MatchResourceLabels(s.cfg.ResourceMatchers, d.GetAllLabels()) {\n+\t\t\tres[d.GetName()] = d\n+\t\t}\n+\t}\n+\treturn res\n+}\n"
 },
 "hashicorp__packer-13388": {
  "model_patch": "diff --git a/provisioner/powershell/provisioner.go b/provisioner/powershell/provisioner.go\nindex 901707c22..a3820a4e2 100644\n--- a/provisioner/powershell/provisioner.go\n+++ b/provisioner/powershell/provisioner.go\n@@ -536,8 +536,7 @@ func (p *Provisioner) createFlattenedEnvVars(elevated bool) (flattened string) {\n \t\t// Escape chars special to PS in each env var value\n \t\tescapedEnvVarValue := psEscape.Replace(keyValue[1])\n \t\tif escapedEnvVarValue != keyValue[1] {\n-\t\t\tlog.Printf(\"Env var %s converted to %s after escaping chars special to PS\", keyValue[1],\n-\t\t\t\tescapedEnvVarValue)\n+\t\t\tlog.Printf(\"Env var %s converted after escaping chars special to PS\", keyValue[0])\n \t\t}\n \t\tenvVars[keyValue[0]] = escapedEnvVarValue\n \t}\n"
 },
 "JanDeDobbeleer__oh-my-posh-6330": {
  "model_patch": "diff --git a/src/segments/path.go b/src/segments/path.go\nindex a59ec51e..81311d99 100644\n--- a/src/segments/path.go\n+++ b/src/segments/path.go\n@@ -611,6 +611,10 @@ func (pt *Path) replaceMappedLocations(inputPath string) (string, string) {\n \n \trootN := pt.normalize(root)\n \trelativeN := pt.normalize(relative)\n+\tif strings.HasPrefix(inputPath, \"/mnt/\") {\n+\t\trootN = strings.ToLower(rootN)\n+\t\trelativeN = strings.ToLower(relativeN)\n+\t}\n \n \tescape := func(path string) string {\n \t\t// Escape chevron characters to avoid applying unexpected text styles.\n@@ -620,7 +624,11 @@ func (pt *Path) replaceMappedLocations(inputPath string) (string, string) {\n \tfor _, key := range keys {\n \t\tif strings.HasPrefix(key, regexPrefix) {\n \t\t\tinput := strings.ReplaceAll(inputPath, `\\`, `/`)\n-\t\t\tmatch, OK := regex.FindStringMatch(key[len(regexPrefix):], input, 1)\n+\t\t\tpattern := key[len(regexPrefix):]\n+\t\t\tif pt.env.GOOS() == runtime.WINDOWS || pt.env.GOOS() == runtime.DARWIN || strings.HasPrefix(input, \"/mnt/\") {\n+\t\t\t\tpattern = \"(?i)\" + pattern\n+\t\t\t}\n+\t\t\tmatch, OK := regex.FindStringMatch(pattern, input, 1)\n \t\t\tif !OK {\n \t\t\t\tcontinue\n \t\t\t}\n@@ -748,12 +756,12 @@ func (pt *Path) normalize(inputPath string) string {\n \t\tnormalized = pt.env.Home() + normalized[1:]\n \t}\n \n-\tnormalized = path.Clean(normalized)\n-\n \tif pt.env.GOOS() == runtime.WINDOWS || pt.env.GOOS() == runtime.DARWIN {\n \t\tnormalized = strings.ToLower(normalized)\n \t}\n \n+\tnormalized = path.Clean(normalized)\n+\n \treturn normalized\n }\n \n"
 },
 "jesseduffield__lazygit-4394": {
  "model_patch": "diff --git a/pkg/commands/git_commands/tag.go b/pkg/commands/git_commands/tag.go\nindex d2b01ba7e..3fa9d543d 100644\n--- a/pkg/commands/git_commands/tag.go\n+++ b/pkg/commands/git_commands/tag.go\n@@ -1,6 +1,9 @@\n package git_commands\n \n-import \"github.com/jesseduffield/gocui\"\n+import (\n+\t\"github.com/jesseduffield/gocui\"\n+\t\"github.com/jesseduffield/lazygit/pkg/commands/oscommands\"\n+)\n \n type TagCommands struct {\n \t*GitCommon\n@@ -12,24 +15,32 @@ func NewTagCommands(gitCommon *GitCommon) *TagCommands {\n \t}\n }\n \n-func (self *TagCommands) CreateLightweight(tagName string, ref string, force bool) error {\n+func (self *TagCommands) CreateLightweightCmdObj(tagName string, ref string, force bool) oscommands.ICmdObj {\n \tcmdArgs := NewGitCmd(\"tag\").\n \t\tArgIf(force, \"--force\").\n \t\tArg(\"--\", tagName).\n \t\tArgIf(len(ref) > 0, ref).\n \t\tToArgv()\n \n-\treturn self.cmd.New(cmdArgs).Run()\n+\treturn self.cmd.New(cmdArgs)\n }\n \n-func (self *TagCommands) CreateAnnotated(tagName, ref, msg string, force bool) error {\n+func (self *TagCommands) CreateLightweight(tagName string, ref string, force bool) error {\n+\treturn self.CreateLightweightCmdObj(tagName, ref, force).Run()\n+}\n+\n+func (self *TagCommands) CreateAnnotatedCmdObj(tagName, ref, msg string, force bool) oscommands.ICmdObj {\n \tcmdArgs := NewGitCmd(\"tag\").Arg(tagName).\n \t\tArgIf(force, \"--force\").\n \t\tArgIf(len(ref) > 0, ref).\n \t\tArg(\"-m\", msg).\n \t\tToArgv()\n \n-\treturn self.cmd.New(cmdArgs).Run()\n+\treturn self.cmd.New(cmdArgs)\n+}\n+\n+func (self *TagCommands) CreateAnnotated(tagName, ref, msg string, force bool) error {\n+\treturn self.CreateAnnotatedCmdObj(tagName, ref, msg, force).Run()\n }\n \n func (self *TagCommands) HasTag(tagName string) bool {\ndiff --git a/pkg/gui/controllers/helpers/tags_helper.go b/pkg/gui/controllers/helpers/tags_helper.go\nindex aa6ff7740..124fde904 100644\n--- a/pkg/gui/controllers/helpers/tags_helper.go\n+++ b/pkg/gui/controllers/helpers/tags_helper.go\n@@ -21,25 +21,17 @@ func NewTagsHelper(c *HelperCommon, commitsHelper *CommitsHelper) *TagsHelper {\n \n func (self *TagsHelper) OpenCreateTagPrompt(ref string, onCreate func()) error {\n \tdoCreateTag := func(tagName string, description string, force bool) error {\n-\t\treturn self.c.WithWaitingStatus(self.c.Tr.CreatingTag, func(gocui.Task) error {\n-\t\t\tif description != \"\" {\n-\t\t\t\tself.c.LogAction(self.c.Tr.Actions.CreateAnnotatedTag)\n-\t\t\t\tif err := self.c.Git().Tag.CreateAnnotated(tagName, ref, description, force); err != nil {\n-\t\t\t\t\treturn err\n-\t\t\t\t}\n-\t\t\t} else {\n-\t\t\t\tself.c.LogAction(self.c.Tr.Actions.CreateLightweightTag)\n-\t\t\t\tif err := self.c.Git().Tag.CreateLightweight(tagName, ref, force); err != nil {\n-\t\t\t\t\treturn err\n-\t\t\t\t}\n-\t\t\t}\n-\n-\t\t\tself.commitsHelper.OnCommitSuccess()\n-\n-\t\t\treturn self.c.Refresh(types.RefreshOptions{\n-\t\t\t\tMode: types.ASYNC, Scope: []types.RefreshableView{types.COMMITS, types.TAGS},\n+\t\tif description != \"\" {\n+\t\t\tcmdObj := self.c.Git().Tag.CreateAnnotatedCmdObj(tagName, ref, description, force)\n+\t\t\treturn self.c.Helpers().GPG.WithGpgHandling(cmdObj, self.c.Tr.CreatingTag, func() error {\n+\t\t\t\treturn self.afterTagCreate()\n \t\t\t})\n-\t\t})\n+\t\t} else {\n+\t\t\tcmdObj := self.c.Git().Tag.CreateLightweightCmdObj(tagName, ref, force)\n+\t\t\treturn self.c.Helpers().GPG.WithGpgHandling(cmdObj, self.c.Tr.CreatingTag, func() error {\n+\t\t\t\treturn self.afterTagCreate()\n+\t\t\t})\n+\t\t}\n \t}\n \n \tonConfirm := func(tagName string, description string) error {\n@@ -79,3 +71,11 @@ func (self *TagsHelper) OpenCreateTagPrompt(ref string, onCreate func()) error {\n \n \treturn nil\n }\n+\n+func (self *TagsHelper) afterTagCreate() error {\n+\tself.commitsHelper.OnCommitSuccess()\n+\n+\treturn self.c.Refresh(types.RefreshOptions{\n+\t\tMode: types.ASYNC, Scope: []types.RefreshableView{types.COMMITS, types.TAGS},\n+\t})\n+}\n"
 },
 "kubevirt__kubevirt-14681": {
  "model_patch": "diff --git a/pkg/virt-api/webhooks/validating-webhook/admitters/vmi-create-admitter.go b/pkg/virt-api/webhooks/validating-webhook/admitters/vmi-create-admitter.go\nindex 1a251490c4..e52e4856b4 100644\n--- a/pkg/virt-api/webhooks/validating-webhook/admitters/vmi-create-admitter.go\n+++ b/pkg/virt-api/webhooks/validating-webhook/admitters/vmi-create-admitter.go\n@@ -1448,18 +1448,29 @@ func validateFirmwareACPI(field *k8sfield.Path, spec *v1.VirtualMachineInstanceS\n \t}\n \n \tacpi := spec.Domain.Firmware.ACPI\n-\tfor _, volume := range spec.Volumes {\n-\t\tif acpi.SlicNameRef != volume.Name {\n+\tif acpi.SlicNameRef != \"\" {\n+\t\tcauses = append(causes, validateACPIVolume(field, acpi.SlicNameRef, \"slicNameRef\", spec.Volumes)...)\n+\t}\n+\tif acpi.MsdmNameRef != \"\" {\n+\t\tcauses = append(causes, validateACPIVolume(field, acpi.MsdmNameRef, \"msdmNameRef\", spec.Volumes)...)\n+\t}\n+\n+\treturn causes\n+}\n+\n+func validateACPIVolume(field *k8sfield.Path, volumeName string, childName string, volumes []v1.Volume) []metav1.StatusCause {\n+\tvar causes []metav1.StatusCause\n+\n+\tfor _, volume := range volumes {\n+\t\tif volumeName != volume.Name {\n \t\t\tcontinue\n \t\t}\n \n-\t\tswitch {\n-\t\tcase volume.Secret != nil:\n-\t\tdefault:\n+\t\tif volume.Secret == nil {\n \t\t\tcauses = append(causes, metav1.StatusCause{\n \t\t\t\tType:    metav1.CauseTypeFieldValueInvalid,\n \t\t\t\tMessage: fmt.Sprintf(\"%s refers to Volume of unsupported type.\", field.String()),\n-\t\t\t\tField:   field.Child(\"slicNameRef\").String(),\n+\t\t\t\tField:   field.Child(childName).String(),\n \t\t\t})\n \t\t}\n \t\treturn causes\n@@ -1468,9 +1479,8 @@ func validateFirmwareACPI(field *k8sfield.Path, spec *v1.VirtualMachineInstanceS\n \tcauses = append(causes, metav1.StatusCause{\n \t\tType:    metav1.CauseTypeFieldValueInvalid,\n \t\tMessage: fmt.Sprintf(\"%s does not have a matching Volume.\", field.String()),\n-\t\tField:   field.String(),\n+\t\tField:   field.Child(childName).String(),\n \t})\n-\n \treturn causes\n }\n \ndiff --git a/pkg/virt-launcher/virtwrap/api/deepcopy_generated.go b/pkg/virt-launcher/virtwrap/api/deepcopy_generated.go\nindex 4a5456de2d..70d861c38b 100644\n--- a/pkg/virt-launcher/virtwrap/api/deepcopy_generated.go\n+++ b/pkg/virt-launcher/virtwrap/api/deepcopy_generated.go\n@@ -2838,7 +2838,11 @@ func (in *OS) DeepCopy() *OS {\n // DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.\n func (in *OSACPI) DeepCopyInto(out *OSACPI) {\n \t*out = *in\n-\tout.Table = in.Table\n+\tif in.Tables != nil {\n+\t\tin, out := &in.Tables, &out.Tables\n+\t\t*out = make([]ACPITable, len(*in))\n+\t\tcopy(*out, *in)\n+\t}\n \treturn\n }\n \ndiff --git a/pkg/virt-launcher/virtwrap/api/schema.go b/pkg/virt-launcher/virtwrap/api/schema.go\nindex 09627b7140..5d6cb1f836 100644\n--- a/pkg/virt-launcher/virtwrap/api/schema.go\n+++ b/pkg/virt-launcher/virtwrap/api/schema.go\n@@ -998,7 +998,7 @@ type OSType struct {\n }\n \n type OSACPI struct {\n-\tTable ACPITable `xml:\"table,omitempty\"`\n+\tTables []ACPITable `xml:\"table,omitempty\"`\n }\n \n type ACPITable struct {\ndiff --git a/pkg/virt-launcher/virtwrap/converter/converter.go b/pkg/virt-launcher/virtwrap/converter/converter.go\nindex 3d4f4b8be0..444a92fc7d 100644\n--- a/pkg/virt-launcher/virtwrap/converter/converter.go\n+++ b/pkg/virt-launcher/virtwrap/converter/converter.go\n@@ -1232,17 +1232,36 @@ func Convert_v1_Firmware_To_related_apis(vmi *v1.VirtualMachineInstance, domain\n \t}\n \n \tif firmware.ACPI != nil {\n-\t\tpath, err := getSlicMountedPath(vmi.Spec.Volumes, firmware.ACPI.SlicNameRef)\n-\t\tif err != nil {\n-\t\t\tlog.Log.Object(vmi).Warningf(\"Failed to get supported path for Volume: %s\", firmware.ACPI.SlicNameRef)\n-\t\t\treturn err\n-\t\t}\n+\t\tif firmware.ACPI.SlicNameRef != \"\" {\n+\t\t\tpath, err := getSlicMountedPath(vmi.Spec.Volumes, firmware.ACPI.SlicNameRef)\n+\t\t\tif err != nil {\n+\t\t\t\tlog.Log.Object(vmi).Warningf(\"Failed to get supported path for Volume: %s\", firmware.ACPI.SlicNameRef)\n+\t\t\t\treturn err\n+\t\t\t}\n \n-\t\tdomain.Spec.OS.ACPI = &api.OSACPI{\n-\t\t\tTable: api.ACPITable{\n+\t\t\tif domain.Spec.OS.ACPI == nil {\n+\t\t\t\tdomain.Spec.OS.ACPI = &api.OSACPI{}\n+\t\t\t}\n+\t\t\tdomain.Spec.OS.ACPI.Tables = append(domain.Spec.OS.ACPI.Tables, api.ACPITable{\n \t\t\t\tType: \"slic\",\n \t\t\t\tPath: path,\n-\t\t\t},\n+\t\t\t})\n+\t\t}\n+\n+\t\tif firmware.ACPI.MsdmNameRef != \"\" {\n+\t\t\tpath, err := getMsdmMountedPath(vmi.Spec.Volumes, firmware.ACPI.MsdmNameRef)\n+\t\t\tif err != nil {\n+\t\t\t\tlog.Log.Object(vmi).Warningf(\"Failed to get supported path for Volume: %s\", firmware.ACPI.MsdmNameRef)\n+\t\t\t\treturn err\n+\t\t\t}\n+\n+\t\t\tif domain.Spec.OS.ACPI == nil {\n+\t\t\t\tdomain.Spec.OS.ACPI = &api.OSACPI{}\n+\t\t\t}\n+\t\t\tdomain.Spec.OS.ACPI.Tables = append(domain.Spec.OS.ACPI.Tables, api.ACPITable{\n+\t\t\t\tType: \"msdm\",\n+\t\t\t\tPath: path,\n+\t\t\t})\n \t\t}\n \t}\n \n@@ -1250,6 +1269,14 @@ func Convert_v1_Firmware_To_related_apis(vmi *v1.VirtualMachineInstance, domain\n }\n \n func getSlicMountedPath(volumes []v1.Volume, name string) (string, error) {\n+\treturn getACPIVolumeMountedPath(volumes, name, \"slic\")\n+}\n+\n+func getMsdmMountedPath(volumes []v1.Volume, name string) (string, error) {\n+\treturn getACPIVolumeMountedPath(volumes, name, \"msdm\")\n+}\n+\n+func getACPIVolumeMountedPath(volumes []v1.Volume, name string, acpiType string) (string, error) {\n \t// We need to know the the volume type referred by @name\n \tfor _, volume := range volumes {\n \t\tif volume.Name != name {\n@@ -1257,15 +1284,15 @@ func getSlicMountedPath(volumes []v1.Volume, name string) (string, error) {\n \t\t}\n \n \t\tif volume.Secret == nil {\n-\t\t\treturn \"\", fmt.Errorf(\"Firmware's slic volume type is unsupported\")\n+\t\t\treturn \"\", fmt.Errorf(\"Firmware's %s volume type is unsupported\", acpiType)\n \t\t}\n \n-\t\t// Return path to slic binary data\n+\t\t// Return path to binary data\n \t\tsourcePath := config.GetSecretSourcePath(name)\n-\t\treturn filepath.Join(sourcePath, \"slic.bin\"), nil\n+\t\treturn filepath.Join(sourcePath, fmt.Sprintf(\"%s.bin\", acpiType)), nil\n \t}\n \n-\treturn \"\", fmt.Errorf(\"Firmware's slic volume type not found\")\n+\treturn \"\", fmt.Errorf(\"Firmware's %s volume type not found\", acpiType)\n }\n \n func hasIOThreads(vmi *v1.VirtualMachineInstance) bool {\ndiff --git a/staging/src/kubevirt.io/api/core/v1/schema.go b/staging/src/kubevirt.io/api/core/v1/schema.go\nindex 12043dc9f8..7b9af7c5c9 100644\n--- a/staging/src/kubevirt.io/api/core/v1/schema.go\n+++ b/staging/src/kubevirt.io/api/core/v1/schema.go\n@@ -440,6 +440,10 @@ type ACPI struct {\n \t// be a binary blob that follows the ACPI SLIC standard, see:\n \t// https://learn.microsoft.com/en-us/previous-versions/windows/hardware/design/dn653305(v=vs.85)\n \tSlicNameRef string `json:\"slicNameRef,omitempty\"`\n+\t// MsdmNameRef should match the volume name of a secret object. The data in the secret should\n+\t// be a binary blob that follows the ACPI MSDM standard, see:\n+\t// https://learn.microsoft.com/en-us/previous-versions/windows/hardware/design/dn653305(v=vs.85)\n+\tMsdmNameRef string `json:\"msdmNameRef,omitempty\"`\n }\n \n type Devices struct {\n"
 },
 "libsdl-org__SDL-11761": {
  "model_patch": "diff --git a/CMakeLists.txt b/CMakeLists.txt\nindex 94b1c9bfe..95abcd5ba 100644\n--- a/CMakeLists.txt\n+++ b/CMakeLists.txt\n@@ -506,8 +506,12 @@ if(MSVC AND TARGET SDL3-shared)\n     set_property(SOURCE \"${asm_src}\" PROPERTY LANGUAGE \"ASM_MASM\")\n     target_sources(SDL3-shared PRIVATE \"${asm_src}\")\n   elseif(SDL_CPU_ARM32 OR SDL_CPU_ARM64)\n-    # FIXME: ARM assembler (armasm.exe/armasm64.exe) is NOT ASM_MASM, and does currently not work with CMake\n-    # (https://gitlab.kitware.com/cmake/cmake/-/issues/18912)\n+    if(SDL_CPU_ARM64)\n+      enable_language(ASM_ARM64)\n+      set(asm_src \"${SDL3_SOURCE_DIR}/src/stdlib/SDL_mslibc_arm64.s\")\n+      set_property(SOURCE \"${asm_src}\" PROPERTY LANGUAGE \"ASM_ARM64\")\n+      target_sources(SDL3-shared PRIVATE \"${asm_src}\")\n+    endif()\n   endif()\n endif()\n \ndiff --git a/src/stdlib/SDL_mslibc.c b/src/stdlib/SDL_mslibc.c\nindex a527bf75d..dd56e78e6 100644\n--- a/src/stdlib/SDL_mslibc.c\n+++ b/src/stdlib/SDL_mslibc.c\n@@ -731,8 +731,6 @@ void __declspec(naked) _alloca_probe_16(void)\n #ifdef _M_ARM64\n \n void __chkstk(void);\n-void __chkstk() {\n-}\n \n #endif\n \n"
 },
 "libsdl-org__SDL-12806": {
  "model_patch": "diff --git a/src/core/windows/SDL_gameinput.h b/src/core/windows/SDL_gameinput.h\nindex 0022c0bdd..f0c46b2be 100644\n--- a/src/core/windows/SDL_gameinput.h\n+++ b/src/core/windows/SDL_gameinput.h\n@@ -26,7 +26,11 @@\n #ifdef HAVE_GAMEINPUT_H\n \n #define COBJMACROS\n+#ifdef __cplusplus\n #include <gameinput.h>\n+#else\n+#include <GameInput.h>\n+#endif\n \n extern bool SDL_InitGameInput(IGameInput **ppGameInput);\n extern void SDL_QuitGameInput(void);\ndiff --git a/src/joystick/gdk/SDL_gameinputjoystick.c b/src/joystick/gdk/SDL_gameinputjoystick.c\nindex 6cf0a902b..a4cb0987e 100644\n--- a/src/joystick/gdk/SDL_gameinputjoystick.c\n+++ b/src/joystick/gdk/SDL_gameinputjoystick.c\n@@ -447,7 +447,7 @@ static bool GAMEINPUT_JoystickOpen(SDL_Joystick *joystick, int device_index)\n                 ++joystick->nbuttons;\n             }\n \n-#if 1 // The C macro in GameInput.h version 10.0.26100 refers to a focus policy which I guess has been removed from the final API?\n+#if 0 // The C macro in GameInput.h version 10.0.26100 refers to a focus policy which I guess has been removed from the final API?\n #undef IGameInput_RegisterSystemButtonCallback\n #define IGameInput_RegisterSystemButtonCallback(This, device, buttonFilter, context, callbackFunc, callbackToken) ((This)->lpVtbl->RegisterSystemButtonCallback(This, device, buttonFilter, context, callbackFunc, callbackToken))\n #endif\ndiff --git a/src/video/windows/SDL_windowsgameinput.c b/src/video/windows/SDL_windowsgameinput.c\nindex 183733a38..a9fa47cad 100644\n--- a/src/video/windows/SDL_windowsgameinput.c\n+++ b/src/video/windows/SDL_windowsgameinput.c\n@@ -569,11 +569,6 @@ void WIN_QuitGameInput(SDL_VideoDevice *_this)\n             GAMEINPUT_InternalRemoveByIndex(data, 0);\n         }\n \n-        IGameInput_Release(data->pGameInput);\n-        data->pGameInput = NULL;\n-    }\n-\n-    if (data->pGameInput) {\n         SDL_QuitGameInput();\n         data->pGameInput = NULL;\n     }\n"
 },
 "lima-vm__lima-3280": {
  "model_patch": "diff --git a/pkg/hostagent/requirements.go b/pkg/hostagent/requirements.go\nindex 00908655..171950fe 100644\n--- a/pkg/hostagent/requirements.go\n+++ b/pkg/hostagent/requirements.go\n@@ -88,7 +88,7 @@ func prefixExportParam(script string) (string, error) {\n \t// strings will be interpolated into single-quoted strings, so protect any existing single quotes\n \tinterpreter = strings.ReplaceAll(interpreter, \"'\", `'\"'\"'`)\n \texportParam = strings.ReplaceAll(exportParam, \"'\", `'\"'\"'`)\n-\treturn fmt.Sprintf(\"#!/bin/bash -c \\\"$(printf '%s%s')\\\"\\n%s\", exportParam, interpreter, script), nil\n+\treturn fmt.Sprintf(\"#!/bin/bash -c \\\"$(printf '%%s%%s' '%s' '%s')\\\"\\n%s\", exportParam, interpreter, script), nil\n }\n \n func (a *HostAgent) waitForRequirement(r requirement) error {\n"
 },
 "lima-vm__lima-3300": {
  "model_patch": "diff --git a/cmd/limactl/copy.go b/cmd/limactl/copy.go\nindex d6333399..2fd35eb6 100644\n--- a/cmd/limactl/copy.go\n+++ b/cmd/limactl/copy.go\n@@ -80,12 +80,31 @@ func copyAction(cmd *cobra.Command, args []string) error {\n \t// this assumes that ssh and scp come from the same place, but scp has no -V\n \tlegacySSH := sshutil.DetectOpenSSHVersion(\"ssh\").LessThan(*semver.New(\"8.0.0\"))\n \tfor _, arg := range args {\n-\t\tpath := strings.Split(arg, \":\")\n-\t\tswitch len(path) {\n-\t\tcase 1:\n+\t\tvar instName, remainingPath string\n+\t\tcolonIndex := strings.Index(arg, \":\")\n+\t\tif colonIndex > 0 {\n+\t\t\tif colonIndex == 1 && ((arg[0] >= 'a' && arg[0] <= 'z') || (arg[0] >= 'A' && arg[0] <= 'Z')) {\n+\t\t\t\tnextColonIndex := strings.Index(arg[colonIndex+1:], \":\")\n+\t\t\t\tif nextColonIndex != -1 {\n+\t\t\t\t\tactualColonIndex := colonIndex + 1 + nextColonIndex\n+\t\t\t\t\tinstName = arg[:actualColonIndex]\n+\t\t\t\t\tremainingPath = arg[actualColonIndex+1:]\n+\t\t\t\t} else {\n+\t\t\t\t\tinstName = \"\"\n+\t\t\t\t\tremainingPath = arg\n+\t\t\t\t}\n+\t\t\t} else {\n+\t\t\t\tinstName = arg[:colonIndex]\n+\t\t\t\tremainingPath = arg[colonIndex+1:]\n+\t\t\t}\n+\t\t} else {\n+\t\t\tinstName = \"\"\n+\t\t\tremainingPath = arg\n+\t\t}\n+\n+\t\tif instName == \"\" {\n \t\t\tscpArgs = append(scpArgs, arg)\n-\t\tcase 2:\n-\t\t\tinstName := path[0]\n+\t\t} else {\n \t\t\tinst, err := store.Inspect(instName)\n \t\t\tif err != nil {\n \t\t\t\tif errors.Is(err, os.ErrNotExist) {\n@@ -98,13 +117,11 @@ func copyAction(cmd *cobra.Command, args []string) error {\n \t\t\t}\n \t\t\tif legacySSH {\n \t\t\t\tscpFlags = append(scpFlags, \"-P\", fmt.Sprintf(\"%d\", inst.SSHLocalPort))\n-\t\t\t\tscpArgs = append(scpArgs, fmt.Sprintf(\"%s@127.0.0.1:%s\", *inst.Config.User.Name, path[1]))\n+\t\t\t\tscpArgs = append(scpArgs, fmt.Sprintf(\"%s@127.0.0.1:%s\", *inst.Config.User.Name, remainingPath))\n \t\t\t} else {\n-\t\t\t\tscpArgs = append(scpArgs, fmt.Sprintf(\"scp://%s@127.0.0.1:%d/%s\", *inst.Config.User.Name, inst.SSHLocalPort, path[1]))\n+\t\t\t\tscpArgs = append(scpArgs, fmt.Sprintf(\"scp://%s@127.0.0.1:%d/%s\", *inst.Config.User.Name, inst.SSHLocalPort, remainingPath))\n \t\t\t}\n \t\t\tinstances[instName] = inst\n-\t\tdefault:\n-\t\t\treturn fmt.Errorf(\"path %q contains multiple colons\", arg)\n \t\t}\n \t}\n \tif legacySSH && len(instances) > 1 {\ndiff --git a/cmd/limactl/shell.go b/cmd/limactl/shell.go\nindex 3dee0bac..a8a790d6 100644\n--- a/cmd/limactl/shell.go\n+++ b/cmd/limactl/shell.go\n@@ -8,11 +8,13 @@ import (\n \t\"fmt\"\n \t\"os\"\n \t\"os/exec\"\n+\t\"runtime\"\n \t\"strconv\"\n \t\"strings\"\n \n \t\"al.essio.dev/pkg/shellescape\"\n \t\"github.com/coreos/go-semver/semver\"\n+\t\"github.com/lima-vm/lima/pkg/ioutilx\"\n \t\"github.com/lima-vm/lima/pkg/sshutil\"\n \t\"github.com/lima-vm/lima/pkg/store\"\n \t\"github.com/mattn/go-isatty\"\n@@ -88,11 +90,17 @@ func shellAction(cmd *cobra.Command, args []string) error {\n \t\treturn err\n \t}\n \tif workDir != \"\" {\n+\t\tif runtime.GOOS == \"windows\" {\n+\t\t\tworkDir = ioutilx.CanonicalWindowsPath(workDir)\n+\t\t}\n \t\tchangeDirCmd = fmt.Sprintf(\"cd %s || exit 1\", shellescape.Quote(workDir))\n \t\t// FIXME: check whether y.Mounts contains the home, not just len > 0\n \t} else if len(inst.Config.Mounts) > 0 {\n \t\thostCurrentDir, err := os.Getwd()\n \t\tif err == nil {\n+\t\t\tif runtime.GOOS == \"windows\" {\n+\t\t\t\thostCurrentDir = ioutilx.CanonicalWindowsPath(hostCurrentDir)\n+\t\t\t}\n \t\t\tchangeDirCmd = fmt.Sprintf(\"cd %s\", shellescape.Quote(hostCurrentDir))\n \t\t} else {\n \t\t\tchangeDirCmd = \"false\"\n@@ -100,6 +108,9 @@ func shellAction(cmd *cobra.Command, args []string) error {\n \t\t}\n \t\thostHomeDir, err := os.UserHomeDir()\n \t\tif err == nil {\n+\t\t\tif runtime.GOOS == \"windows\" {\n+\t\t\t\thostHomeDir = ioutilx.CanonicalWindowsPath(hostHomeDir)\n+\t\t\t}\n \t\t\tchangeDirCmd = fmt.Sprintf(\"%s || cd %s\", changeDirCmd, shellescape.Quote(hostHomeDir))\n \t\t} else {\n \t\t\tlogrus.WithError(err).Warn(\"failed to get the home directory\")\ndiff --git a/pkg/sshutil/sshutil.go b/pkg/sshutil/sshutil.go\nindex 891279d2..561f4cee 100644\n--- a/pkg/sshutil/sshutil.go\n+++ b/pkg/sshutil/sshutil.go\n@@ -98,8 +98,12 @@ func DefaultPubKeys(loadDotSSH bool) ([]PubKey, error) {\n \t\t}\n \t\tif err := lockutil.WithDirLock(configDir, func() error {\n \t\t\t// no passphrase, no user@host comment\n+\t\t\tprivateKeyPath := filepath.Join(configDir, filenames.UserPrivateKey)\n+\t\t\tif runtime.GOOS == \"windows\" {\n+\t\t\t\tprivateKeyPath = ioutilx.CanonicalWindowsPath(privateKeyPath)\n+\t\t\t}\n \t\t\tkeygenCmd := exec.Command(\"ssh-keygen\", \"-t\", \"ed25519\", \"-q\", \"-N\", \"\",\n-\t\t\t\t\"-C\", \"lima\", \"-f\", filepath.Join(configDir, filenames.UserPrivateKey))\n+\t\t\t\t\"-C\", \"lima\", \"-f\", privateKeyPath)\n \t\t\tlogrus.Debugf(\"executing %v\", keygenCmd.Args)\n \t\t\tif out, err := keygenCmd.CombinedOutput(); err != nil {\n \t\t\t\treturn fmt.Errorf(\"failed to run %v: %q: %w\", keygenCmd.Args, string(out), err)\n"
 },
 "lima-vm__lima-3351": {
  "model_patch": "diff --git a/hack/test-templates.sh b/hack/test-templates.sh\nindex cf9d7f9f..a0de2c77 100755\n--- a/hack/test-templates.sh\n+++ b/hack/test-templates.sh\n@@ -92,7 +92,8 @@ case \"$NAME\" in\n \t;;\n \"wsl2\")\n \t# TODO https://github.com/lima-vm/lima/issues/3267\n-\tCHECKS[\"systemd\"]=\n+\t# CHECKS[\"systemd\"]=\n+\tCHECKS[\"systemd-strict\"]=\n \t# TODO https://github.com/lima-vm/lima/issues/3268\n \tCHECKS[\"proxy-settings\"]=\n \tCHECKS[\"port-forwards\"]=\ndiff --git a/templates/experimental/wsl2.yaml b/templates/experimental/wsl2.yaml\nindex b95e3d8c..218b9826 100644\n--- a/templates/experimental/wsl2.yaml\n+++ b/templates/experimental/wsl2.yaml\n@@ -15,3 +15,12 @@ mountType: wsl2\n containerd:\n   system: true\n   user: false\n+\n+provision:\n+- mode: system\n+  script: |\n+    #!/bin/sh\n+    # Mask auditd and audit-rules as they fail in WSL2\n+    systemctl mask auditd.service audit-rules.service\n+    # Also mask NetworkManager-wait-online.service which fails in mirrored networking mode\n+    systemctl mask NetworkManager-wait-online.service\n"
 },
 "lobehub__lobe-chat-8459": {
  "model_patch": ""
 },
 "microsoft__ebpf-for-windows-4117": {
  "model_patch": "diff --git a/tests/stress/ebpf_stress_tests.cpp b/tests/stress/ebpf_stress_tests.cpp\nindex 45285e62..3acc84b0 100644\n--- a/tests/stress/ebpf_stress_tests.cpp\n+++ b/tests/stress/ebpf_stress_tests.cpp\n@@ -116,7 +116,7 @@ main(int argc, char* argv[])\n         Opt(_test_program_list_arg,\n             \"program names\")[\"-tp\"][\"--test-programs\"](\"Comma separated JIT compiled program names\") |\n         Opt(_test_threads_count_arg, \"thread count\")[\"-tt\"][\"--test-threads\"](\"Count of threads per test\") |\n-        Opt(_test_duration_arg, \"test duration\")[\"-td\"][\"--test-duration\"](\"Test duration (per-test) in seconds\") |\n+        Opt(_test_duration_arg, \"test duration\")[\"-td\"][\"--test-duration\"](\"Test duration (per-test) in minutes\") |\n         Opt(_test_verbose_output_arg,\n             \"verbosity flag\")[\"-vo\"][\"--verbose-output\"](\"Verbosity flag (1 to enable, 0 to disable(default))\") |\n         Opt(_extension_restart_arg, \"restart extension/provider flag\")[\"-er\"][\"--extension-restart\"](\ndiff --git a/tests/stress/km/stress_tests_km.cpp b/tests/stress/km/stress_tests_km.cpp\nindex 53a9e00e..00b00ba5 100644\n--- a/tests/stress/km/stress_tests_km.cpp\n+++ b/tests/stress/km/stress_tests_km.cpp\n@@ -81,7 +81,8 @@ enum class service_state_type : uint32_t\n };\n \n static bool\n-_set_extension_state(SC_HANDLE service_handle, service_state_type service_state, uint32_t timeout)\n+_set_extension_state(\n+101|    const std::string& extension_name, SC_HANDLE service_handle, service_state_type service_state, uint32_t timeout)\n {\n     std::string ss = (service_state == service_state_type::STOP ? \"STOP\" : \"START\");\n     using sc = std::chrono::steady_clock;\n@@ -149,7 +150,8 @@ _set_extension_state(SC_HANDLE service_handle, service_state_type service_state,\n         // We ignore the return status here as this API returns an error if the driver is actually starting or is\n         // already running which is a no-op for us. This can happen if the driver goes into a 'start pending/running'\n         // state _after_ the QueryServiceStatusEx above returns.\n-        (void)system(\"net start NetEbpfExt >NUL 2>&1\");\n+        std::string command = \"net start \" + extension_name + \" >NUL 2>&1\";\n+161|        (void)system(command.c_str());\n         LOG_VERBOSE(\"Issued extension START\");\n \n         // Sleep for a bit to let the SCM process our command.\n"
 },
 "microsoft__STL-5444": {
  "model_patch": "diff --git a/stl/inc/regex b/stl/inc/regex\nindex b63f7e83..5e8febc8 100644\n--- a/stl/inc/regex\n+++ b/stl/inc/regex\n@@ -318,6 +318,10 @@ public:\n \n             _Getctype()->tolower(_Temp.data(), _Temp.data() + _Temp.size());\n             _Res = _Getcoll()->transform(_Temp.data(), _Temp.data() + _Temp.size());\n+\n+            if (!_Res.empty()) {\n+                _Res.erase(_STD min(_Res.size(), static_cast<size_t>(2)));\n+            }\n         }\n         return _Res;\n     }\n"
 },
 "microsoft__vscode-239695": {
  "model_patch": "diff --git a/src/vs/workbench/contrib/terminal/common/scripts/shellIntegration.ps1 b/src/vs/workbench/contrib/terminal/common/scripts/shellIntegration.ps1\nindex 4c5a7059a51..daea565bf94 100644\n--- a/src/vs/workbench/contrib/terminal/common/scripts/shellIntegration.ps1\n+++ b/src/vs/workbench/contrib/terminal/common/scripts/shellIntegration.ps1\n@@ -363,7 +363,7 @@ function Send-Completions {\n \t\t\t$prefixCursorDelta = $cursorIndex - $newCursorIndex\n \t\t\t$cursorIndex = $newCursorIndex\n \t\t}\n-\t\telseif ($lastWord -notmatch '[/\\\\$]') {\n+\t\telseif ($lastWord -notmatch '[/\\\\$]' -and $lastWord -ne \"\") {\n \t\t\tif ($lastWhitespaceIndex -ne -1 -and $lastWhitespaceIndex -lt $cursorIndex) {\n \t\t\t\t$newCursorIndex = $lastWhitespaceIndex + 1\n \t\t\t\t$completionPrefix = $completionPrefix.Substring(0, $newCursorIndex)\n"
 },
 "moby__moby-49938": {
  "model_patch": ""
 },
 "nats-io__nats-server-6803": {
  "model_patch": "diff --git a/server/certstore/certstore_windows.go b/server/certstore/certstore_windows.go\nindex d47adb6ee..562894f11 100644\n--- a/server/certstore/certstore_windows.go\n+++ b/server/certstore/certstore_windows.go\n@@ -593,16 +593,16 @@ func winSignECDSA(kh uintptr, digest []byte) ([]byte, error) {\n \t\treturn nil, ErrStoreECDSASigningError\n \t}\n \n-\treturn winPackECDSASigValue(bytes.NewReader(buf[:size]), len(digest))\n+\treturn winPackECDSASigValue(bytes.NewReader(buf[:size]), int(size))\n }\n \n-func winPackECDSASigValue(r io.Reader, digestLength int) ([]byte, error) {\n-\tsigR := make([]byte, digestLength)\n+func winPackECDSASigValue(r io.Reader, signatureLength int) ([]byte, error) {\n+\tsigR := make([]byte, signatureLength/2)\n \tif _, err := io.ReadFull(r, sigR); err != nil {\n \t\treturn nil, ErrStoreECDSASigningError\n \t}\n \n-\tsigS := make([]byte, digestLength)\n+\tsigS := make([]byte, signatureLength/2)\n \tif _, err := io.ReadFull(r, sigS); err != nil {\n \t\treturn nil, ErrStoreECDSASigningError\n \t}\n"
 },
 "podman-desktop__podman-desktop-13439": {
  "model_patch": "diff --git a/.github/workflows/e2e-main.yaml b/.github/workflows/e2e-main.yaml\nindex b1851fe12c5..f1bf43b0810 100644\n--- a/.github/workflows/e2e-main.yaml\n+++ b/.github/workflows/e2e-main.yaml\n@@ -175,8 +175,7 @@ jobs:\n             !./tests/**/traces/raw\n \n   win-update-e2e-test:\n-    name: win update e2e tests - ${{ matrix.installation }}\n-    runs-on: windows-2025\n+    name: win update e2e tests - ${{ matrix.installation }} - ${{ matrix.os }}\n     # disable on forks as secrets are not available\n     if: github.event.repository.fork == false\n     permissions:\n@@ -187,9 +186,11 @@ jobs:\n     strategy:\n       fail-fast: false\n       matrix:\n+        os: [windows-2025, windows-arm-11]\n         installation: ['vanilla', 'custom-extensions']\n         exclude:\n         - installation: ${{ (github.event.inputs.update_with_extensions && github.event.inputs.update_with_extensions == 'true') && 'N/A' || 'custom-extensions' }}\n+    runs-on: ${{ matrix.os }}\n     steps:\n       - uses: actions/checkout@08c6903cd8c0fde910a37f88322edcfb5dd907a8 # v5.0.0\n         with:\n@@ -221,7 +222,13 @@ jobs:\n           ELECTRON_ENABLE_INSPECT: true\n         run: |\n           pnpm compile:current --win nsis\n-          $path=('./dist/win-unpacked/Podman Desktop.exe' | resolve-path).ProviderPath\n+          $runnerArch=$env:RUNNER_ARCH\n+          $unpackedPath = \"dist/win-unpacked\"\n+          if ($runnerArch -eq 'ARM64') {\n+            $unpackedPath = \"dist/win-arm64-unpacked\"\n+          }\n+          echo (\"PD_DIST_PATH=\" + $unpackedPath) >> $env:GITHUB_ENV\n+          $path=(\"./$unpackedPath/Podman Desktop.exe\" | resolve-path).ProviderPath\n           echo $path\n           echo (\"PODMAN_DESKTOP_BINARY=\" + $path) >> $env:GITHUB_ENV\n \ndiff --git a/.github/workflows/pr-check.yaml b/.github/workflows/pr-check.yaml\nindex b3817d2b3f5..fe153a398ef 100644\n--- a/.github/workflows/pr-check.yaml\n+++ b/.github/workflows/pr-check.yaml\n@@ -429,12 +429,12 @@ jobs:\n           fi\n \n   win-update-e2e-test:\n-    name: win update e2e tests\n+    name: win update e2e tests - ${{ matrix.os }}\n     needs: detect_pnpm_changes\n     strategy:\n       fail-fast: false\n-      matrix: \n-        os: [windows-2025]\n+      matrix:\n+        os: [windows-2025, windows-arm-11]\n     if: contains(github.event.pull_request.labels.*.name, 'area/update') || needs.detect_pnpm_changes.outputs.pnpm_lock_changed == 'true'\n     runs-on: ${{ matrix.os }}\n     timeout-minutes: 60\n"
 },
 "rustls__rustls-2586": {
  "model_patch": ""
 },
 "tailscale__tailscale-14669": {
  "model_patch": "diff --git a/cmd/tailscaled/tailscaled_windows.go b/cmd/tailscaled/tailscaled_windows.go\nindex 786c5d833..2cd70c5c7 100644\n--- a/cmd/tailscaled/tailscaled_windows.go\n+++ b/cmd/tailscaled/tailscaled_windows.go\n@@ -143,6 +143,10 @@ func runWindowsService(pol *logpolicy.Policy) error {\n \t\tdefer syslog.Close()\n \t}\n \n+\tif isWindowsService() {\n+\t\treturn svc.Run(serviceName, &ipnService{Policy: pol})\n+\t}\n+\n \tsyslogf(\"Service entering svc.Run\")\n \tdefer syslogf(\"Service exiting svc.Run\")\n \treturn svc.Run(serviceName, &ipnService{Policy: pol})\n"
 },
 "zarf-dev__zarf-3788": {
  "model_patch": "diff --git a/src/cmd/dev.go b/src/cmd/dev.go\nindex 8989c4b2..cc18a59d 100644\n--- a/src/cmd/dev.go\n+++ b/src/cmd/dev.go\n@@ -17,7 +17,6 @@ import (\n \t\"github.com/AlecAivazis/survey/v2\"\n \t\"github.com/defenseunicorns/pkg/helpers/v2\"\n \tgoyaml \"github.com/goccy/go-yaml\"\n-\t\"github.com/mholt/archiver/v3\"\n \t\"github.com/pterm/pterm\"\n \t\"github.com/sergi/go-diff/diffmatchpatch\"\n \t\"github.com/spf13/cobra\"\n@@ -27,6 +26,7 @@ import (\n \t\"github.com/zarf-dev/zarf/src/config/lang\"\n \t\"github.com/zarf-dev/zarf/src/internal/packager2\"\n \tlayout2 \"github.com/zarf-dev/zarf/src/internal/packager2/layout\"\n+\t\"github.com/zarf-dev/zarf/src/pkg/archive\"\n \t\"github.com/zarf-dev/zarf/src/pkg/layout\"\n \t\"github.com/zarf-dev/zarf/src/pkg/lint\"\n \t\"github.com/zarf-dev/zarf/src/pkg/logger\"\n@@ -536,7 +536,7 @@ func (o *devSha256SumOptions) run(cmd *cobra.Command, args []string) (err error)\n \t\textractedFile := filepath.Join(tmp, o.extractPath)\n \n \t\t// TODO(mkcp): See https://github.com/zarf-dev/zarf/issues/3051\n-\t\terr = archiver.Extract(fileName, o.extractPath, tmp)\n+\t\terr = archive.Extract(cmd.Context(), fileName, o.extractPath, tmp)\n \t\tif err != nil {\n \t\t\treturn errors.Join(hashErr, err)\n \t\t}\ndiff --git a/src/internal/packager2/create.go b/src/internal/packager2/create.go\nindex e9f163bb..3af60306 100644\n--- a/src/internal/packager2/create.go\n+++ b/src/internal/packager2/create.go\n@@ -72,7 +72,7 @@ func Create(ctx context.Context, packagePath string, opt CreateOptions) error {\n \t}\n \n \tif opt.SBOMOut != \"\" {\n-\t\t_, err := pkgLayout.GetSBOM(opt.SBOMOut)\n+\t\t_, err := pkgLayout.GetSBOM(ctx, opt.SBOMOut)\n \t\t// Don't fail package create if the package doesn't have an sbom\n \t\tvar noSBOMErr *layout2.NoSBOMAvailableError\n \t\tif errors.As(err, &noSBOMErr) {\ndiff --git a/src/internal/packager2/inspect.go b/src/internal/packager2/inspect.go\nindex cf4e6cb9..ebdf0c04 100644\n--- a/src/internal/packager2/inspect.go\n+++ b/src/internal/packager2/inspect.go\n@@ -311,7 +311,7 @@ func InspectPackageSboms(ctx context.Context, source string, opts InspectPackage\n \tdefer func() {\n \t\terr = errors.Join(err, pkgLayout.Cleanup())\n \t}()\n-\toutputPath, err := pkgLayout.GetSBOM(opts.OutputDir)\n+\toutputPath, err := pkgLayout.GetSBOM(ctx, opts.OutputDir)\n \tif err != nil {\n \t\treturn InspectPackageSbomsResult{}, fmt.Errorf(\"could not get SBOM: %w\", err)\n \t}\ndiff --git a/src/internal/packager2/layout/create.go b/src/internal/packager2/layout/create.go\nindex 20fb10be..94f60f5f 100644\n--- a/src/internal/packager2/layout/create.go\n+++ b/src/internal/packager2/layout/create.go\n@@ -23,7 +23,6 @@ import (\n \n \t\"github.com/defenseunicorns/pkg/helpers/v2\"\n \tgoyaml \"github.com/goccy/go-yaml\"\n-\t\"github.com/mholt/archiver/v3\"\n \t\"github.com/sigstore/cosign/v2/cmd/cosign/cli/options\"\n \t\"github.com/sigstore/cosign/v2/cmd/cosign/cli/sign\"\n \n@@ -36,6 +35,7 @@ import (\n \t\"github.com/zarf-dev/zarf/src/internal/packager/kustomize\"\n \tactions2 \"github.com/zarf-dev/zarf/src/internal/packager2/actions\"\n \t\"github.com/zarf-dev/zarf/src/internal/packager2/filters\"\n+\t\"github.com/zarf-dev/zarf/src/pkg/archive\"\n \t\"github.com/zarf-dev/zarf/src/pkg/interactive\"\n \t\"github.com/zarf-dev/zarf/src/pkg/lint\"\n \t\"github.com/zarf-dev/zarf/src/pkg/logger\"\n@@ -407,7 +407,7 @@ func assemblePackageComponent(ctx context.Context, component v1alpha1.ZarfCompon\n \t\t\t\t\treturn fmt.Errorf(lang.ErrDownloading, file.Source, err.Error())\n \t\t\t\t}\n \t\t\t\t// TODO(mkcp): See https://github.com/zarf-dev/zarf/issues/3051\n-\t\t\t\terr = archiver.Extract(compressedFile, file.ExtractPath, destinationDir)\n+\t\t\t\terr = archive.Extract(ctx, compressedFile, file.ExtractPath, destinationDir)\n \t\t\t\tif err != nil {\n \t\t\t\t\treturn fmt.Errorf(lang.ErrFileExtract, file.ExtractPath, compressedFileName, err.Error())\n \t\t\t\t}\n@@ -419,7 +419,7 @@ func assemblePackageComponent(ctx context.Context, component v1alpha1.ZarfCompon\n \t\t} else {\n \t\t\tif file.ExtractPath != \"\" {\n \t\t\t\t// TODO(mkcp): See https://github.com/zarf-dev/zarf/issues/3051\n-\t\t\t\tif err := archiver.Extract(filepath.Join(packagePath, file.Source), file.ExtractPath, destinationDir); err != nil {\n+\t\t\t\tif err := archive.Extract(ctx, filepath.Join(packagePath, file.Source), file.ExtractPath, destinationDir); err != nil {\n \t\t\t\t\treturn fmt.Errorf(lang.ErrFileExtract, file.ExtractPath, file.Source, err.Error())\n \t\t\t\t}\n \t\t\t} else {\n@@ -612,7 +612,7 @@ func assembleSkeletonComponent(component v1alpha1.ZarfComponent, packagePath, bu\n \n \t\tif file.ExtractPath != \"\" {\n \t\t\t// TODO(mkcp): See https://github.com/zarf-dev/zarf/issues/3051\n-\t\t\tif err := archiver.Extract(filepath.Join(packagePath, file.Source), file.ExtractPath, destinationDir); err != nil {\n+\t\t\tif err := archive.Extract(ctx, filepath.Join(packagePath, file.Source), file.ExtractPath, destinationDir); err != nil {\n \t\t\t\treturn fmt.Errorf(lang.ErrFileExtract, file.ExtractPath, file.Source, err.Error())\n \t\t\t}\n \ndiff --git a/src/internal/packager2/layout/create_test.go b/src/internal/packager2/layout/create_test.go\nindex e0e859c8..fa7bdde8 100644\n--- a/src/internal/packager2/layout/create_test.go\n+++ b/src/internal/packager2/layout/create_test.go\n@@ -351,7 +351,7 @@ func TestGetSBOM(t *testing.T) {\n \t// Ensure the SBOM does not exist\n \trequire.NoFileExists(t, filepath.Join(pkgLayout.dirPath, SBOMTar))\n \t// Ensure Zarf errors correctly\n-\t_, err = pkgLayout.GetSBOM(tmpdir)\n+\t_, err = pkgLayout.GetSBOM(context.TODO(), tmpdir)\n \tvar noSBOMErr *NoSBOMAvailableError\n \trequire.ErrorAs(t, err, &noSBOMErr)\n }\ndiff --git a/src/internal/packager2/layout/import.go b/src/internal/packager2/layout/import.go\nindex dcf9fa2a..081332e7 100644\n--- a/src/internal/packager2/layout/import.go\n+++ b/src/internal/packager2/layout/import.go\n@@ -17,7 +17,7 @@ import (\n \n \t\"github.com/defenseunicorns/pkg/helpers/v2\"\n \t\"github.com/defenseunicorns/pkg/oci\"\n-\t\"github.com/mholt/archiver/v3\"\n+\t\"github.com/zarf-dev/zarf/src/pkg/archive\"\n \tocispec \"github.com/opencontainers/image-spec/specs-go/v1\"\n \tocistore \"oras.land/oras-go/v2/content/oci\"\n \ndiff --git a/src/internal/packager2/layout/package.go b/src/internal/packager2/layout/package.go\nindex ead18f00..f91ac270 100644\n--- a/src/internal/packager2/layout/package.go\n+++ b/src/internal/packager2/layout/package.go\n@@ -17,13 +17,13 @@ import (\n \t\"strings\"\n \n \t\"github.com/defenseunicorns/pkg/helpers/v2\"\n-\t\"github.com/mholt/archiver/v3\"\n \t\"github.com/sigstore/cosign/v2/cmd/cosign/cli/options\"\n \t\"github.com/sigstore/cosign/v2/cmd/cosign/cli/verify\"\n \n \t\"github.com/zarf-dev/zarf/src/api/v1alpha1\"\n \t\"github.com/zarf-dev/zarf/src/config\"\n \t\"github.com/zarf-dev/zarf/src/internal/packager2/filters\"\n+\t\"github.com/zarf-dev/zarf/src/pkg/archive\"\n \t\"github.com/zarf-dev/zarf/src/pkg/logger\"\n \t\"github.com/zarf-dev/zarf/src/pkg/packager/sources\"\n \t\"github.com/zarf-dev/zarf/src/pkg/utils\"\n@@ -141,13 +141,13 @@ func (e *NoSBOMAvailableError) Error() string {\n }\n \n // GetSBOM outputs the SBOM data from the package to the give destination path.\n-func (p *PackageLayout) GetSBOM(destPath string) (string, error) {\n+func (p *PackageLayout) GetSBOM(ctx context.Context, destPath string) (string, error) {\n \tif !p.Pkg.IsSBOMAble() {\n \t\treturn \"\", &NoSBOMAvailableError{pkgName: p.Pkg.Metadata.Name}\n \t}\n \tpath := filepath.Join(destPath, p.Pkg.Metadata.Name)\n \t// TODO(mkcp): See https://github.com/zarf-dev/zarf/issues/3051\n-\terr := archiver.Extract(filepath.Join(p.dirPath, SBOMTar), \"\", path)\n+\terr := archive.Extract(ctx, filepath.Join(p.dirPath, SBOMTar), \"\", path)\n \tif err != nil {\n \t\treturn \"\", err\n \t}\ndiff --git a/src/internal/packager2/layout/package_test.go b/src/internal/packager2/layout/package_test.go\nindex 584c8092..38f3dfbf 100644\n--- a/src/internal/packager2/layout/package_test.go\n+++ b/src/internal/packager2/layout/package_test.go\n@@ -40,7 +40,7 @@ func TestPackageLayout(t *testing.T) {\n \trequire.ErrorContains(t, err, \"component test could not access a files directory\")\n \n \ttmpDir = t.TempDir()\n-\tsbomPath, err := pkgLayout.GetSBOM(tmpDir)\n+\tsbomPath, err := pkgLayout.GetSBOM(context.TODO(), tmpDir)\n \trequire.NoError(t, err)\n \trequire.FileExists(t, filepath.Join(sbomPath, \"compare.html\"))\n \ndiff --git a/src/pkg/archive/archive.go b/src/pkg/archive/archive.go\nindex 6aa0b1a1..91e1515b 100644\n--- a/src/pkg/archive/archive.go\n+++ b/src/pkg/archive/archive.go\n@@ -13,7 +13,6 @@ import (\n \t\"path/filepath\"\n \t\"strings\"\n \n-\t\"github.com/mholt/archiver/v3\"\n \t\"github.com/mholt/archives\"\n \t\"github.com/zarf-dev/zarf/src/config/lang\"\n \t\"github.com/zarf-dev/zarf/src/pkg/layout\"\n@@ -25,9 +24,32 @@ const rwxPerm = 0o755\n type CompressOpts struct{}\n \n // Compress takes any number of source files and archives them into a tarball at dest path.\n-// TODO(mkcp): Migrate to mholt/archives, see CVE-2024-0406\n-func Compress(_ context.Context, sources []string, dest string, _ CompressOpts) error {\n-\treturn archiver.Archive(sources, dest)\n+func Compress(ctx context.Context, sources []string, dest string, _ CompressOpts) error {\n+\tif ctx == nil {\n+\t\tctx = context.Background()\n+\t}\n+\tfiles, err := archives.FilesFromDisk(ctx, nil, nil)\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\t// Filter files to only those in sources\n+\t// Actually FilesFromDisk with a map might be easier.\n+\tsourceMap := make(map[string]string)\n+\tfor _, s := range sources {\n+\t\tsourceMap[s] = \"\"\n+\t}\n+\tfiles, err = archives.FilesFromDisk(ctx, nil, sourceMap)\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\tout, err := os.Create(dest)\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\tdefer out.Close()\n+\n+\tformat := archives.Tar{}\n+\treturn format.Archive(ctx, out, files)\n }\n \n // DecompressOpts provides optional parameters for Decompress\n@@ -158,3 +180,84 @@ func unarchive(ctx context.Context, src, dst string) error {\n \t}\n \treturn nil\n }\n+\n+// Extract extracts a single file or directory from an archive.\n+func Extract(ctx context.Context, src, targetInArchive, dst string) error {\n+\tif ctx == nil {\n+\t\tctx = context.Background()\n+\t}\n+\t// Open the archive file\n+\tfile, err := os.Open(src)\n+\tif err != nil {\n+\t\treturn fmt.Errorf(\"unable to open archive %q: %w\", src, err)\n+\t}\n+\tdefer func() {\n+\t\terr = errors.Join(err, file.Close())\n+\t}()\n+\n+\t// Identify format & get an input stream\n+\tformat, input, err := archives.Identify(ctx, src, file)\n+\tif err != nil {\n+\t\treturn fmt.Errorf(\"unable to identify archive %q: %w\", src, err)\n+\t}\n+\n+\t// Assert that it supports extraction\n+\textractor, ok := format.(archives.Extractor)\n+\tif !ok {\n+\t\treturn fmt.Errorf(\"unsupported format for extraction: %T\", format)\n+\t}\n+\n+\t// Define how each entry is written to disk\n+\thandler := func(_ context.Context, f archives.FileInfo) error {\n+\t\tif f.NameInArchive != targetInArchive && !strings.HasPrefix(f.NameInArchive, targetInArchive+\"/\") {\n+\t\t\treturn nil\n+\t\t}\n+\n+\t\trelPath, err := filepath.Rel(targetInArchive, f.NameInArchive)\n+\t\tif err != nil {\n+\t\t\treturn err\n+\t\t}\n+\t\ttarget := filepath.Join(dst, relPath)\n+\n+\t\tswitch {\n+\t\tcase f.IsDir():\n+\t\t\t// directory\n+\t\t\treturn os.MkdirAll(target, f.Mode())\n+\n+\t\tcase f.LinkTarget != \"\":\n+\t\t\t// symlink\n+\t\t\tlinkDest := filepath.Join(dst, f.LinkTarget)\n+\t\t\treturn os.Symlink(linkDest, target)\n+\n+\t\tdefault:\n+\t\t\t// regular file\n+\t\t\tif err := os.MkdirAll(filepath.Dir(target), rwxPerm); err != nil {\n+\t\t\t\treturn err\n+\t\t\t}\n+\t\t\tout, err := os.OpenFile(target, os.O_CREATE|os.O_WRONLY, f.Mode())\n+\t\t\tif err != nil {\n+\t\t\t\treturn err\n+\t\t\t}\n+\t\t\tdefer func() {\n+\t\t\t\terr = errors.Join(err, out.Close())\n+\t\t\t}()\n+\n+\t\t\tin, err := f.Open()\n+\t\t\tif err != nil {\n+\t\t\t\treturn err\n+\t\t\t}\n+\t\t\tdefer func() {\n+\t\t\t\terr = errors.Join(err, in.Close())\n+\t\t\t}()\n+\n+\t\t\t_, err = io.Copy(out, in)\n+\t\t\treturn err\n+\t\t}\n+\t}\n+\n+\t// Perform extraction\n+\tif err := extractor.Extract(ctx, input, handler); err != nil {\n+\t\treturn fmt.Errorf(\"unable to extract %q from %q: %w\", targetInArchive, src, err)\n+\t}\n+\treturn nil\n+}\ndiff --git a/src/pkg/layout/sbom.go b/src/pkg/layout/sbom.go\nindex fcfb300b..c1437a0e 100644\n--- a/src/pkg/layout/sbom.go\n+++ b/src/pkg/layout/sbom.go\n@@ -11,7 +11,7 @@ import (\n \t\"path/filepath\"\n \n \t\"github.com/defenseunicorns/pkg/helpers/v2\"\n-\t\"github.com/mholt/archiver/v3\"\n+\t\"github.com/zarf-dev/zarf/src/pkg/archive\"\n )\n \n // ComponentSBOM contains paths for a component's SBOM.\ndiff --git a/src/test/e2e/06_create_sbom_test.go b/src/test/e2e/06_create_sbom_test.go\nindex 3b8c0969..29c76370 100644\n--- a/src/test/e2e/06_create_sbom_test.go\n+++ b/src/test/e2e/06_create_sbom_test.go\n@@ -35,7 +35,7 @@ func TestCreateSBOM(t *testing.T) {\n \n \tpkgLayout, err := layout2.LoadFromTar(context.Background(), tarPath, layout2.PackageLayoutOptions{})\n \trequire.NoError(t, err)\n-\tgetSbomPath, err := pkgLayout.GetSBOM(t.TempDir())\n+\tgetSbomPath, err := pkgLayout.GetSBOM(context.TODO(), t.TempDir())\n \trequire.NoError(t, err)\n \tfor _, expectedFile := range expectedFiles {\n \t\trequire.FileExists(t, filepath.Join(getSbomPath, expectedFile))\n"
 },
 "zarf-dev__zarf-3799": {
  "model_patch": "diff --git a/src/pkg/packager/actions/actions.go b/src/pkg/packager/actions/actions.go\nindex cffcb8f7..6b54defd 100644\n--- a/src/pkg/packager/actions/actions.go\n+++ b/src/pkg/packager/actions/actions.go\n@@ -225,15 +225,18 @@ func actionCmdMutation(ctx context.Context, cmd string, shellPref v1alpha1.Shell\n \t\t// http://web.cs.ucla.edu/~miryung/teaching/EE461L-Spring2012/labs/posix.html for more details.\n \t\tcmd = regexp.MustCompile(`^touch `).ReplaceAllString(cmd, `New-Item `)\n \n-\t\t// Convert any ${ZARF_VAR_*} or $ZARF_VAR_* to ${env:ZARF_VAR_*} or $env:ZARF_VAR_* respectively (also TF_VAR_*).\n+\t\t// Convert any ${ZARF_VAR_*}, $ZARF_VAR_*, ${ZARF_CONST_*}, or $ZARF_CONST_* to $Env:ZARF_VAR_* or $Env:ZARF_CONST_* respectively (also TF_VAR_*).\n \t\t// https://regex101.com/r/xk1rkw/1\n-\t\tenvVarRegex := regexp.MustCompile(`(?P<envIndicator>\\${?(?P<varName>(ZARF|TF)_VAR_([a-zA-Z0-9_-])+)}?)`)\n-\t\tget, err := helpers.MatchRegex(envVarRegex, cmd)\n-\t\tif err == nil {\n-\t\t\tnewCmd := strings.ReplaceAll(cmd, get(\"envIndicator\"), fmt.Sprintf(\"$Env:%s\", get(\"varName\")))\n-\t\t\tlogger.From(ctx).Debug(\"converted command\", \"cmd\", cmd, \"newCmd\", newCmd)\n-\t\t\tcmd = newCmd\n-\t\t}\n+\t\tenvVarRegex := regexp.MustCompile(`(\\${?(?P<varName>(ZARF|TF)_(VAR|CONST)_([a-zA-Z0-9_-])+)}?)`)\n+\t\tcmd = envVarRegex.ReplaceAllStringFunc(cmd, func(match string) string {\n+\t\t\tsubmatches := envVarRegex.FindStringSubmatch(match)\n+\t\t\tfor i, name := range envVarRegex.SubexpNames() {\n+\t\t\t\tif name == \"varName\" {\n+\t\t\t\t\treturn fmt.Sprintf(\"$Env:%s\", submatches[i])\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\treturn match\n+\t\t})\n \t}\n \n \treturn cmd, nil\n"
 }
}